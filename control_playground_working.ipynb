{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c497ef5-9126-48b6-b285-17df0293ed03",
   "metadata": {},
   "source": [
    "# Control\n",
    "\n",
    "I'm using this notebook to:\n",
    "\n",
    "1. Explore what it takes to add a new control dataset.\n",
    "2. Make notes that we can use for discussion and knowledge-share.\n",
    "\n",
    "My plan, for now, is to copy into this notebook the bare minimum lines needed to run a training/evaluation step using a ControlTask.\n",
    "\n",
    "Every time I hit a road block, I'll make note of the issue, try my best to get around it (not necessarily in the correct way), and rinse/repeat.\n",
    "\n",
    "If you're scanning for places where I have questions, look for formatting ***? like this ?***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8db616d4-a211-4ba0-b506-eabb353815fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.5.2 (SDL 2.28.2, Python 3.10.11)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "# If you edit code while this notebook is running,\n",
    "# you might need to reload the module for the changes\n",
    "# to take effect. You could also resolve the issue\n",
    "# by restarting the kernel. But reloading the module\n",
    "# is quicker and less destructive. I'm adding the \n",
    "# reload at the top so you can just run this cell\n",
    "# after making any breaking edits to the code.\n",
    "import importlib\n",
    "import gato.tasks.control_task\n",
    "importlib.reload(gato.tasks.control_task)\n",
    "\n",
    "import minari\n",
    "import gymnasium as gym\n",
    "from gato.tasks.control_task import ControlTask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818a1f01-4a17-4b4d-8dae-37895acd0f18",
   "metadata": {},
   "source": [
    "## BabyAI Minari dataset\n",
    "\n",
    "I grabbed a random dataset from the ones created with [the bot.py script in Santiago's baby-ai-dataset repo](https://github.com/snat-s/baby-ai-dataset/blob/master/scripts/bot.py#L64)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9eb40a3-7ba7-45fc-901c-5c3650467ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'd4rl_halfcheetah-expert-v2'\n",
    "dataset = minari.load_dataset(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48e2d91e-6129-4b31-9948-c23816ecc559",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<minari.dataset.minari_dataset.MinariDataset at 0x7f40dc0f0130>,\n",
       " 'HalfCheetah-v3')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset, dataset.spec.env_spec.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8bb39ed5-1830-43b9-bf39-76b3930c072a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eihli/miniconda3/envs/neko/lib/python3.10/site-packages/gymnasium/envs/mujoco/mujoco_env.py:211: DeprecationWarning: \u001b[33mWARN: This version of the mujoco environments depends on the mujoco-py bindings, which are no longer maintained and may stop working. Please upgrade to the v4 versions of the environments (which depend on the mujoco python bindings instead), unless you are trying to precisely replicate previous works).\u001b[0m\n",
      "  logger.deprecation(\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(dataset.spec.env_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1786d1ed-44f4-412c-83fc-3535c2455ef7",
   "metadata": {},
   "source": [
    "# train.py\n",
    "\n",
    "## Training arguments\n",
    "\n",
    "I'm using [Namespace](https://docs.python.org/3/library/argparse.html#argparse.Namespace) to hack up an `args` object so that I don't have to go through the hassle of `parser = ArgumentParser; parser.add_argument(...); args = parser.parse(['foo', 'bar', 'baz', ...])`.\n",
    "\n",
    "Browsing the code, the only `args` attribute that I see `ControlTask` access is `args.patch_size`, so that's the only one I'm bothering to add."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6510562-6b7e-4598-b5fa-464d18b6b975",
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a94b4b9-fc85-40bc-ba7e-bdd0690a5af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(patch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dcdf66fd-3f1a-440a-893a-60492878b545",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_len = 512\n",
    "training_prompg_len_proportion=0.5\n",
    "share_prompt_episodes=True\n",
    "top_5_prompting=None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06ba044-04d3-46c6-8714-48ec277b4c67",
   "metadata": {},
   "source": [
    "# ControlTask\n",
    "\n",
    "`train.py` creates a `ControlTask`.\n",
    "\n",
    "## Supported observation spaces\n",
    "\n",
    "The first error I encounter is that the BabyAI environment has a Dict observation space.\n",
    "\n",
    "***? What would it take to support a Dict (or any other) observation space ?***\n",
    "\n",
    "Looking at how this is used, I see it conditionally:\n",
    "\n",
    "- If it's a Box that has a shape of length 2 or 3\n",
    "    - Adds image transforms\n",
    "- Otherwise\n",
    "    - Sets `obs_str` to `'continuous_obs'`, which eventually makes its way into `input_dict`, which eventually makes its way to `predict_control`, which doesn't get checked in `predict_control` but eventually makes its way to `tokenize_input_dicts`, and _that's_ where it gets checked.\n",
    "        - It tokenizes the batch with the `continuous_obs_tokenizer`. \n",
    "\n",
    "(Is this a safe condition? Might there be non-image con)\n",
    "\n",
    "### tokens_per_space\n",
    "\n",
    "This is another place where the type of the observation space is checked.\n",
    "\n",
    "Box: `space.shape[0]`\n",
    "Discrete: `1`\n",
    "Dict: ?\n",
    "\n",
    "***? What should this? How is it used? ?***\n",
    "\n",
    "`tokens_per_space` gets assigned to `action_tokens` [here](https://github.com/eihli/NEKO/blob/b66b48b88117307a442c43a7f4d8701706670144/gato/tasks/control_task.py#L74) and then used to calculate `tokens_per_timestamp` and is eventually used to create/manipulate the shape of the `input_dict` in [ControlTask.evaluate](https://github.com/eihli/NEKO/blob/b66b48b88117307a442c43a7f4d8701706670144/gato/tasks/control_task.py#L138).\n",
    "\n",
    "### Hacking my way past errors.\n",
    "\n",
    "I hard coded some arbitrary values to get past the assertions.\n",
    "\n",
    "For example, I added an `isinstance(space, Dict): return 1` to `tokens_per_space` [here](https://github.com/eihli/NEKO/blob/b66b48b88117307a442c43a7f4d8701706670144/gato/tasks/control_task.py#L23).\n",
    "\n",
    "I'm sure it's going to blow up due to a dict size mismatch or something. I just want to get to that point so I can see that error and maybe understand what the value _should_ be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7f4423d-1c28-4f32-9192-21218f90cb15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(-inf, inf, (17,), float64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d28676e-6225-44f0-ae05-ef16596903b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "control = ControlTask('playground_env_name', env, dataset, context_len, args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707ce9cf-c93d-4a7b-879b-4ba8e713c3a9",
   "metadata": {},
   "source": [
    "# Back to train.py now that we have a ControlTask\n",
    "\n",
    "Now we're going to need a lot more args."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f37f081-43eb-4070-90b6-7417b44a2145",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gato.policy.gato_policy import GatoPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d99217f7-1f7d-43f9-b6c0-feda8569e7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.device = 'cpu'\n",
    "args.embed_dim = 128\n",
    "args.layers = 4\n",
    "args.heads = 8\n",
    "args.dropout = 0.1\n",
    "args.mu = 100\n",
    "args.M = 256\n",
    "args.resid_mid_channels = 128\n",
    "args.continuous_tokens = 1024\n",
    "args.discrete_tokens = 1024\n",
    "args.sequence_length = 1024\n",
    "args.disable_patch_pos_encoding = False\n",
    "args.disable_inner_pos_encoding = False\n",
    "args.activation_fn = 'gelu'\n",
    "args.pretrained_lm = None\n",
    "args.flash = False\n",
    "args.tokenizer_model_name = 'gpt2'\n",
    "args.pad_seq = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b4e33b6-020e-47ac-be9b-f80b837b7dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GatoPolicy(\n",
    "    device=args.device,\n",
    "    embed_dim=args.embed_dim,\n",
    "    layers=args.layers,\n",
    "    heads=args.heads,\n",
    "    dropout=args.dropout,\n",
    "    mu=args.mu,\n",
    "    M=args.M,\n",
    "    patch_size=args.patch_size,\n",
    "    resid_mid_channels=args.resid_mid_channels,\n",
    "    continuous_tokens=args.continuous_tokens,\n",
    "    discrete_tokens=args.discrete_tokens,\n",
    "    context_len=args.sequence_length,\n",
    "    use_patch_pos_encoding=not args.disable_patch_pos_encoding,\n",
    "    use_pos_encoding=not args.disable_inner_pos_encoding,\n",
    "    activation_fn=args.activation_fn,\n",
    "    pretrained_lm=args.pretrained_lm,\n",
    "    flash=args.flash,\n",
    "    tokenizer_model_name=args.tokenizer_model_name,\n",
    "    pad_seq=args.pad_seq,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c8c4bce-b9d1-4f74-9ed6-da0b8332670f",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.embed_dim = model.embed_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "145e04aa-f36d-48a2-a725-f63344c3f12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator, DistributedDataParallelKwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0a8cf724-4c8d-45b6-a1bd-1c75a8766b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.cpu = True\n",
    "args.mixed_precision = 'no'\n",
    "args.gradient_accumulation_steps = 1\n",
    "args.eval_mode = 'deterministic'\n",
    "args.save_dir = 'models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3a784988-8036-4554-a2f0-39755c25c545",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddp_kwargs = DistributedDataParallelKwargs(find_unused_parameters=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "401644de-7fd8-41c7-a56e-007a0caa592d",
   "metadata": {},
   "outputs": [],
   "source": [
    "accelerator = Accelerator(\n",
    "    cpu=args.cpu,\n",
    "    mixed_precision=args.mixed_precision,\n",
    "    split_batches=True,\n",
    "    gradient_accumulation_steps=args.gradient_accumulation_steps,\n",
    "    kwargs_handlers=[ddp_kwargs]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "53a234b9-8c02-4390-bc6f-c5f1fb501c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = accelerator.prepare(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2346e1de-55fe-4d1d-b37b-f64ded900e8b",
   "metadata": {},
   "source": [
    "# Trainer\n",
    "\n",
    "***? What if we wanted to skip the trainer? Could I just run `evaluate` on the ControlTask myself ?***\n",
    "\n",
    "`trainer.train` runs `train_iteration`.\n",
    "\n",
    "`train_iteration` runs `model.train()` (where `model` is an `nn.Module` ([docs](https://pytorch.org/docs/stable/generated/torch.nn.Module.html)) and we don't extend its `train` method).\n",
    "\n",
    "`GatoPolicy.forward` gets something called a \"final_representation\", which is the result of passing some token embeddings and a mask to the GPT2Model.\n",
    "\n",
    "I'm getting a bit lost at this point. I'd love to have someone explain what's going on around this part of the code.\n",
    "\n",
    "`forward` calls `tokenize_input_dicts(inputs)` [here](https://github.com/eihli/NEKO/blob/b66b48b88117307a442c43a7f4d8701706670144/gato/policy/gato_policy.py#L156). If your tracking `inputs`, which is probably an important variable to track`, then this line is probably important.\n",
    "\n",
    "Continuing anyways...\n",
    "\n",
    "We eventually call `predict_tokens` and return the `logits` (and conditionally the `loss`). [predict_token](https://github.com/eihli/NEKO/blob/b9facb61e7d48bf5f9fef9f4ec73b85b531e4aaf/gato/policy/gato_policy.py#L123) is an `nn.Linear`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "83a5a577-6f5f-4f04-81ac-e1258fb84ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from gato.training.schedulers import get_linear_warmup_cosine_decay_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "43cfd72d-8c28-4a45-a33b-7d1c1d8a20c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.learning_rate = 1e-4\n",
    "args.beta_1 = 0.9\n",
    "args.beta_2 = 0.95\n",
    "args.adam_eps = 1e-8\n",
    "args.weight_decay = 0.1\n",
    "args.warmup_steps = 10\n",
    "args.training_steps = 20\n",
    "args.init_lr = 1e-7\n",
    "args.min_factor = 10.0\n",
    "args.disable_cosine_decay = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d772d5e4-2cfd-4571-a917-3e8553dd96a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=args.learning_rate,\n",
    "    betas=(args.beta_1, args.beta_2),\n",
    "    eps=args.adam_eps,\n",
    "    weight_decay=args.weight_decay,\n",
    ")\n",
    "\n",
    "# Setup scheduler\n",
    "scheduler = get_linear_warmup_cosine_decay_scheduler(\n",
    "    optimizer,\n",
    "    args.warmup_steps,\n",
    "    args.training_steps,\n",
    "    base_lr=args.learning_rate,\n",
    "    init_lr=args.init_lr,\n",
    "    min_lr=args.learning_rate / args.min_factor,\n",
    "    cosine_decay=not args.disable_cosine_decay\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e3d2f03f-7fea-49d6-be07-b133a7b6b168",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer, scheduler = accelerator.prepare(optimizer, scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1990fd98-76a5-4641-a1c4-1b85e965e98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gato.training.trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "799775c0-805f-4de4-9e0d-5799ea4f3841",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eafbe84d-786d-4dc6-90e5-380f08b773bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = [control]\n",
    "exp_name = f'neko-lab-{datetime.now().isoformat()}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "abcc0f90-4de9-4458-ab4d-ce6ad502449b",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.log_eval_freq = 10  # must be less than half of training_steps\n",
    "args.save_model = False\n",
    "args.save_mode = 'last'\n",
    "args.text_prop = 0.5\n",
    "args.batch_size = 512\n",
    "args.prompt_ep_proportion = 0.25\n",
    "args.prompt_len_proportion = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a74339dc-85f7-4a9e-85f1-3e016f010643",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    optimizer,\n",
    "    accelerator,\n",
    "    scheduler,\n",
    "    tasks,\n",
    "    exp_name,\n",
    "    args\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d19bf11-829a-4419-a46b-02073cc09c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4820ee44-49d7-4911-bee5-d6b5aaf3f8b7",
   "metadata": {},
   "source": [
    "# Pause...\n",
    "\n",
    "I just realized it would probably benefit me to explore what an existing _working_ dataset looks like. I'm going to take a break from this notebook to go do that. Maybe I'll do it below. Maybe I'll do it in a new notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d28794-9429-4926-9cf1-604be5f33bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4b8c9d-4645-476c-8974-0c03fcf0b0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.training_steps, args.log_eval_freq, trainer.args.training_steps // trainer.args.log_eval_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8554074c-4fbd-463f-97d6-365888f57d17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
