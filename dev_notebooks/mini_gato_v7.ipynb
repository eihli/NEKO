{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "949b7cdb-1309-4a42-af31-70ddbbaa978f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "from dataclasses import dataclass, fields\n",
    "from functools import partial\n",
    "from itertools import cycle\n",
    "import pdb\n",
    "import random\n",
    "from einops import rearrange\n",
    "import numpy as np\n",
    "import minari\n",
    "from minigrid.core import constants as mgc\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "import torchvision.transforms.v2 as transforms\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import GPT2Tokenizer, GPT2Config, GPT2Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b901d325-a96d-463d-88f6-31702432d715",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed7f7ec7-1ea5-4fd0-8210-cbb96e80330f",
   "metadata": {},
   "outputs": [],
   "source": [
    "minigrid_dataset = minari.load_dataset('D4RL/minigrid/fourrooms-v0', download=True)\n",
    "env  = minigrid_dataset.recover_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7a51868-93a6-4a29-b669-b7dca65d3985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note on shapes:\n",
    "# You're probably familiar with the old (B, T, C) shape â€“ batch, timestep, channel.\n",
    "# I'd like to introduce a new dimension: sequence. It fits between timestep and channel.\n",
    "# `stack` here concats together sequences. A timestep is a list of sequence.\n",
    "# IMPORTANT! Shape must always be (T, S, ...) for the below code to work.\n",
    "# Stack pads along S and concats along T.\n",
    "@dataclass\n",
    "class TokenData:\n",
    "    tokens: torch.Tensor\n",
    "    targets: torch.Tensor\n",
    "    attention_mask: torch.Tensor\n",
    "    embedding: torch.Tensor = torch.tensor([])  # Optional at first.\n",
    "\n",
    "    def combine(self, other):\n",
    "        \"\"\"Concats attributes of self to attributes of other.\"\"\"\n",
    "        # Requires padding to already be handled.\n",
    "        # Requires shapes to be (T', T, C, ...)\n",
    "        # Where T' is episode timestep and T is the usual timestep.\n",
    "        return type(self)(\n",
    "            tokens=torch.concat([self.tokens, other.tokens]),\n",
    "            targets=torch.concat([self.targets, other.targets]),\n",
    "            attention_mask=torch.concat([self.attention_mask, other.attention_mask]),\n",
    "        )\n",
    "\n",
    "    def embed(self, embedder):\n",
    "        raise Exception('TODO: Override')\n",
    "\n",
    "    def to(self, device):\n",
    "        return type(self)(\n",
    "            tokens=self.tokens.to(device),\n",
    "            targets=self.targets.to(device),\n",
    "            attention_mask=self.attention_mask.to(device),\n",
    "        )\n",
    "        \n",
    "    @property\n",
    "    def size(self):\n",
    "        \"\"\"The number of tokens this will consume of the context window\"\"\"\n",
    "        return self.tokens.size(0) * self.tokens.size(1)\n",
    "\n",
    "class TextTokenData(TokenData):\n",
    "    def embed(self, embedder):\n",
    "        return type(self)(\n",
    "            tokens=self.tokens,\n",
    "            targets=self.targets,\n",
    "            attention_mask=self.attention_mask,\n",
    "            embedding=embedder.text(self.tokens),\n",
    "        ) \n",
    "\n",
    "class ImageTokenData(TokenData):\n",
    "    def embed(self, embedder):\n",
    "        return type(self)(\n",
    "            tokens=self.tokens,\n",
    "            targets=self.targets,\n",
    "            attention_mask=self.attention_mask,\n",
    "            embedding=embedder.image(self.tokens),\n",
    "        ) \n",
    "\n",
    "class DiscreteTokenData(TokenData):\n",
    "    def embed(self, embedder):\n",
    "        return type(self)(\n",
    "            tokens=self.tokens,\n",
    "            targets=self.targets,\n",
    "            attention_mask=self.attention_mask,\n",
    "            embedding=embedder.discrete(self.tokens),\n",
    "        ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7503a9c7-97ae-47fe-8d43-c92b19671e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class EpisodeData:\n",
    "    def __getitem__(self, i):\n",
    "        # Iterate over fields\n",
    "        return type(self)(**{\n",
    "            field.name: type(getattr(self, field.name))(\n",
    "                tokens=getattr(self, field.name).tokens[i:i+1],\n",
    "                targets=getattr(self, field.name).targets[i:i+1],\n",
    "                attention_mask=getattr(self, field.name).attention_mask[i:i+1],\n",
    "            )\n",
    "            for field in fields(self)\n",
    "        })\n",
    "\n",
    "    def combine(self, other):\n",
    "        return type(self)(**{\n",
    "            field.name: getattr(self, field.name).combine(getattr(other, field.name))\n",
    "            for field in fields(self)\n",
    "        })\n",
    "\n",
    "    @property\n",
    "    def size(self):\n",
    "        return sum(getattr(self, field.name).size for field in fields(self))\n",
    "\n",
    "    @property\n",
    "    def num_timesteps(self):\n",
    "        return next(getattr(self, field.name) for field in fields(self)).tokens.size(0)\n",
    "\n",
    "    def embed(self, embedder):\n",
    "        return type(self)(**{\n",
    "            field.name: getattr(self, field.name).embed(embedder)\n",
    "            for field in fields(self)\n",
    "        })\n",
    "\n",
    "    def to(self, device):\n",
    "        return type(self)(**{\n",
    "            field.name: getattr(self, field.name).to(device)\n",
    "            for field in fields(self)\n",
    "        })\n",
    "\n",
    "    def sequence(self, embeddings):\n",
    "        raise Exception('Override me')\n",
    "\n",
    "@dataclass\n",
    "class FourRoomsTimestep(EpisodeData):\n",
    "    mission: TextTokenData  # torch.Size((length of episode subsequence, length of _max_ (pad) mission text tokens))\n",
    "    image: ImageTokenData\n",
    "    direction: DiscreteTokenData\n",
    "    actions: DiscreteTokenData\n",
    "\n",
    "    def sequence(self, sequence_length):\n",
    "        xs = torch.concat([self.mission.embedding, self.image.embedding, self.direction.embedding, self.actions.embedding], dim=1)\n",
    "        ys = torch.concat([self.mission.targets, self.image.targets, self.direction.targets, self.actions.targets], dim=1)\n",
    "        ms = torch.concat([self.mission.attention_mask, self.image.attention_mask, self.direction.attention_mask, self.actions.attention_mask], dim=1)\n",
    "        T, S, C = xs.shape\n",
    "        xs, ys, ms = xs.reshape(T*S, C), ys.reshape(T*S), ms.reshape(T*S)\n",
    "        padding_len = sequence_length - T*S\n",
    "        xs = F.pad(xs, (0, 0, 0, padding_len), value=0)\n",
    "        ys, ms = [F.pad(x, (0, padding_len), value=0) for x in [ys, ms]]\n",
    "        return xs, ys, ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19bee90e-e4f9-44a3-ac01-29f7cb30a64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self, text_gen_tokenizer, text_obs_tokenizer):\n",
    "        self.text_gen_tokenizer = text_gen_tokenizer\n",
    "        self.text_obs_tokenizer = text_obs_tokenizer\n",
    "\n",
    "    def text_gen(self, data, **kwargs):\n",
    "        tokenized =  self.text_gen_tokenizer(data, **kwargs)\n",
    "        return TextTokenData(**{\n",
    "            \"tokens\": tokenized[\"input_ids\"][:, :-1],\n",
    "            \"targets\": tokenized[\"input_ids\"][:, 1:].to(torch.long),\n",
    "            \"attention_mask\": tokenized[\"attention_mask\"][:, :-1],\n",
    "        })\n",
    "\n",
    "    def text_obs(self, data, **kwargs):\n",
    "        tokenized =  self.text_obs_tokenizer(data, **kwargs)\n",
    "        return TextTokenData(**{\n",
    "            \"tokens\": tokenized[\"input_ids\"],\n",
    "            \"targets\": tokenized[\"input_ids\"].to(torch.long),\n",
    "            \"attention_mask\": torch.zeros_like(tokenized[\"attention_mask\"]),\n",
    "        })\n",
    "\n",
    "    def image(self, data):\n",
    "        if len(data.shape) == 3:\n",
    "          data = data.unsqueeze(0)\n",
    "        patches = images_to_patches(data, patch_size=16)\n",
    "        # Hardcoding as a reminder to do something smarter\n",
    "        SQUARE_ROOT_OF_PATCH_SIZE = 3.464\n",
    "        xs = (\n",
    "            apply_along_dimension(\n",
    "                normalize_to_between_minus_one_plus_one, 2, patches\n",
    "            )\n",
    "            / SQUARE_ROOT_OF_PATCH_SIZE\n",
    "        )\n",
    "        # We don't predict images, but we need ys\n",
    "        # becaues these image ys will be in our\n",
    "        # concatenated ys of text/image/action/etc...\n",
    "        ys = torch.zeros(xs.shape[:2])\n",
    "        ms = torch.zeros(xs.shape[:2])  # Same story as above.\n",
    "        return ImageTokenData(tokens=xs, targets=ys, attention_mask=ms)\n",
    "\n",
    "    def discrete_obs(self, data):\n",
    "        if len(data.shape) == 0:\n",
    "            data = data.unsqueeze(0)\n",
    "        if len(data.shape) == 1:\n",
    "            data = data.unsqueeze(1)\n",
    "        xs = data\n",
    "        ys = torch.zeros(xs.shape[:2])\n",
    "        ms = torch.zeros(xs.shape[:2])\n",
    "        return DiscreteTokenData(tokens=xs, targets=ys, attention_mask=ms)\n",
    "\n",
    "    def discrete_act(self, data):\n",
    "        if len(data.shape) == 0:\n",
    "            data = data.unsqueeze(0)\n",
    "        if len(data.shape) == 1:\n",
    "            data = data.unsqueeze(1)\n",
    "        xs = torch.concat([torch.full((data.size(0), 1), 1023), data], dim=1)[:, :-1]  # Instead of '|' being the separator, like Gato...\n",
    "        ys = data\n",
    "        ms = torch.ones(*ys.shape)\n",
    "        return DiscreteTokenData(tokens=xs, targets=ys, attention_mask=ms)\n",
    "\n",
    "    def continuous(self, data):\n",
    "        raise Exception('TODO: Tokenizer.continuous')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "484e577b-f0df-4533-b3ce-eb6b779638a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 5])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.concat([torch.arange(3), torch.tensor([5])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3649c8cc-5946-4046-92c5-32c1f210ad65",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformDataset(Dataset):\n",
    "    def __init__(self, dataset, transform):\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.transform(self.dataset[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bae1599c-4eec-453b-af2d-6cffe906c216",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQUENCE_LENGTH = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98e0883f-5c10-40c1-b766-9a0ad9a9839f",
   "metadata": {},
   "outputs": [],
   "source": [
    "__text_tokenizer = GPT2Tokenizer.from_pretrained(\"openai-community/gpt2\", clean_up_tokenization_spaces=True)\n",
    "__text_tokenizer.pad_token = __text_tokenizer.eos_token\n",
    "_text_gen_tokenizer = partial(\n",
    "    __text_tokenizer,\n",
    "    max_length=SEQUENCE_LENGTH+1,\n",
    "    truncation=True,\n",
    "    padding=\"max_length\",\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "_text_obs_tokenizer = partial(\n",
    "    __text_tokenizer,\n",
    "    max_length=SEQUENCE_LENGTH,\n",
    "    truncation=True,\n",
    "    padding=\"longest\",\n",
    "    return_tensors=\"pt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fff59177-d1b5-4fa5-8354-f59756841803",
   "metadata": {},
   "outputs": [],
   "source": [
    "def images_to_patches(images, patch_size=16):\n",
    "    return rearrange(images, 'b c (h s1) (w s2) -> b (h w) (c s1 s2)', s1=patch_size, s2=patch_size)\n",
    "def normalize_to_between_minus_one_plus_one(t: torch.Tensor):\n",
    "    min_val, max_val = t.min(), t.max()\n",
    "    if min_val == max_val:\n",
    "        return torch.zeros_like(t)\n",
    "    normalized = 2 * (t - min_val) / (max_val - min_val) - 1\n",
    "    return normalized\n",
    "# There's a small deviation in the NEKO codebase from the paper.\n",
    "# The paper normalizes _per patch_. The NEKO codebase currently normalizes _per image_.\n",
    "# https://github.com/eihli/NEKO/blob/master/gato/policy/embeddings.py#L38\n",
    "# This notebook normalizeds per patch. That's what this utility helps.\n",
    "def apply_along_dimension(func, dim, tensor):\n",
    "    tensor = tensor.transpose(0, dim)\n",
    "    shape = tensor.shape\n",
    "    tensor = tensor.reshape(shape[0], -1)\n",
    "    result = torch.stack([func(tensor[:, i]) for i in range(tensor.size(1))], dim=1)\n",
    "    result = result.reshape(shape).transpose(0, dim)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "831e1f5d-4bf5-4336-bf54-029a7506c11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lookup table\n",
    "lut = np.zeros((256, 3), dtype=np.uint8)\n",
    "for idx, color_name in mgc.IDX_TO_COLOR.items():\n",
    "    lut[idx] = mgc.COLORS[color_name]\n",
    "\n",
    "def minigrid_to_rgb(episode):\n",
    "    \"\"\"Convert discrete \"image\" observations into actual images.\n",
    "    I'm expecting this will improve our image modality while not losing\n",
    "    much. The downside is we can fit less in our context window. Note:\n",
    "    We might need to overlay the color/type image (index 1) with the\n",
    "    state image (index 2), if we really don't want to lose any info.\"\"\"\n",
    "    # Apply lookup to second channel\n",
    "    image = lut[episode.observations['image'][:, :, :, 1]]\n",
    "    # Convert to PyTorch tensor and permute\n",
    "    image = torch.from_numpy(image).permute(0, 3, 1, 2)\n",
    "    return image\n",
    "\n",
    "image_transform = transforms.Compose([\n",
    "    # No particular reason to use `transforms.Compose` here since we're only doing one transform. But it's nice to know about.\n",
    "    transforms.RandomResizedCrop((192, 192), (0.5, 1.0)),\n",
    "    transforms.ToDtype(torch.float32, scale=True),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "def minigrid_tokenizer(tokenizer, episode):\n",
    "    num_timesteps = len(episode.actions)\n",
    "    image = image_transform(minigrid_to_rgb(episode)[:num_timesteps])\n",
    "    image = tokenizer.image(image[:num_timesteps])\n",
    "    mission = tokenizer.text_obs(episode.observations['mission'][:num_timesteps], padding=False)\n",
    "    direction = tokenizer.discrete_obs(torch.from_numpy(episode.observations['direction'])[:num_timesteps])\n",
    "    actions = tokenizer.discrete_act(torch.from_numpy(episode.actions))\n",
    "    return FourRoomsTimestep(mission=mission, image=image, direction=direction, actions=actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c9b01e0-e4db-469a-9f64-cc969e27b99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(_text_gen_tokenizer, _text_obs_tokenizer)\n",
    "minigrid_tokenize = partial(minigrid_tokenizer, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7f88bdb1-9ef9-47f4-b80a-09ca2241bcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "minigrid_dataset_xf = TransformDataset(minigrid_dataset, minigrid_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d7bc9e5e-092b-4837-8a8a-af70ab3c9ed9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([19, 1])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minigrid_dataset_xf[0].actions.tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8d9b70ea-3f9f-4cf8-88a6-a04c6218ef33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([19, 3])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minigrid_dataset_xf[0].mission.targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f9e6367a-96f1-4799-ab03-ab87f728de2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "38d670a2-e681-44f0-a108-ca3190c2e8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def minigrid_collate_fn(batch):\n",
    "    result = []\n",
    "    for sample in batch:\n",
    "        i = random.randint(0, sample.num_timesteps - 1)\n",
    "\n",
    "        # Starting at that index, we'll continue adding observations to our context window until\n",
    "        # we run out of space.\n",
    "        step = sample[i]\n",
    "        while i < len(sample.actions.tokens) and step.size + step[0].size < SEQUENCE_LENGTH:\n",
    "            i += 1\n",
    "            step = step.combine(sample[i])\n",
    "        result.append(step)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1086a274-b36a-4d25-a23e-fb91aa2d5afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "minigrid_dataloader = DataLoader(minigrid_dataset_xf, batch_size=BATCH_SIZE, collate_fn=minigrid_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "27b348fa-bdee-40c3-90e4-e32e4e19f4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "minigrid_batch = next(iter(minigrid_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a79021-f818-4d34-9891-b5c2136408cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "95e806b1-9d64-4bbf-8604-ce70793e2198",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# From section 2.2 of the Gato paper:\n",
    "#\n",
    "#    Tokens belonging to image patches for any time-step are embedded using a\n",
    "#    single ResNet (He et al., 2016a) block to obtain a vector per patch. For\n",
    "#    image patch token embeddings, we also add a learnable within-image position\n",
    "#    encoding vector.\n",
    "class ResNetV2Block(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, num_groups=24):\n",
    "        super(ResNetV2Block, self).__init__()\n",
    "        self.gn1 = nn.GroupNorm(1, in_channels)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size=3,\n",
    "            stride=stride,\n",
    "            padding=1,\n",
    "            bias=False,\n",
    "        )\n",
    "        self.gn2 = nn.GroupNorm(num_groups, out_channels)\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            out_channels, in_channels, kernel_size=3, stride=1, padding=1, bias=False\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, CHW = x.shape\n",
    "        # TODO: Remove these hardcoded values.\n",
    "        out = rearrange(x, 'b t (c h w) -> (b t) c h w', c=3, h=16)\n",
    "        out = self.gn1(out)\n",
    "        out = self.gelu(out)\n",
    "        out = self.conv1(out)\n",
    "        out = self.gn2(out)\n",
    "        out = self.gelu(out)\n",
    "        out = self.conv2(out)\n",
    "        return x + rearrange(out, '(b t) c h w -> b t (c h w)', b=B, t=T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1cb5e0d9-a958-4360-b900-73915d986790",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Embedder:\n",
    "    text: Callable\n",
    "    image: Callable\n",
    "    discrete: Callable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9fc4b833-e1d8-45df-9d0b-d7c828b5c7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class MiniGatoConfig:\n",
    "    embedding_dim: int\n",
    "    sequence_length: int\n",
    "    vocab_size: int \n",
    "    transformer_config: GPT2Config\n",
    "    transformer: GPT2Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "36ac07b5-7677-4b7a-9ee0-c22ed5c21714",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_default_config() -> MiniGatoConfig:\n",
    "    transformer_config = GPT2Config()\n",
    "    return MiniGatoConfig(\n",
    "        embedding_dim=768,\n",
    "        sequence_length=1024,\n",
    "        vocab_size=__text_tokenizer.vocab_size,\n",
    "        transformer_config=transformer_config,\n",
    "        transformer=GPT2Model(transformer_config),\n",
    "    )\n",
    "default_config = init_default_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "84775ff6-30a1-4786-a9c5-bcee084f58c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniGato(nn.Module):\n",
    "    def __init__(self, config: MiniGatoConfig=default_config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.sequence_length = self.config.sequence_length\n",
    "        text_embedding = nn.Embedding(self.config.vocab_size, self.config.embedding_dim)\n",
    "        image_embedding = ResNetV2Block(3, self.config.embedding_dim)\n",
    "        discrete_embedding = nn.Embedding(1024, self.config.embedding_dim)\n",
    "        self.embedder = Embedder(text=text_embedding, image=image_embedding, discrete=discrete_embedding)\n",
    "        self.transformer = self.config.transformer\n",
    "        self.lm_head = nn.Linear(self.transformer.config.hidden_size, self.config.vocab_size)     \n",
    "\n",
    "    def forward(self, batch):\n",
    "        batch = [\n",
    "            sample.embed(self.embedder).sequence(self.sequence_length) for sample in batch\n",
    "        ]\n",
    "        xs, ys, ms = map(torch.stack, zip(*batch))\n",
    "        xs, ys, ms = [x.to(device) for x in [xs, ys, ms]]\n",
    "        out = self.transformer(inputs_embeds=xs)\n",
    "        predicted = self.lm_head(out.last_hidden_state)\n",
    "        return predicted, ys, ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2e40fcf0-be16-4f91-89cf-17e8e1d4fa47",
   "metadata": {},
   "outputs": [],
   "source": [
    "minigrid_dataloader = DataLoader(minigrid_dataset_xf, batch_size=BATCH_SIZE, collate_fn=minigrid_collate_fn)\n",
    "minigrid_iterator = iter(minigrid_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bdf45b06-7b45-4379-90c7-ccd72f8e67e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "minigrid_batch = next(minigrid_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9e420165-26b3-4144-b11b-432223186e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infinite_dataloader(fn):\n",
    "    it = iter(fn())\n",
    "    while True:\n",
    "        try:\n",
    "            yield next(it)\n",
    "        except StopIteration:\n",
    "            it = iter(fn())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6c746035-3724-4d09-a2fd-3f49d1b4d295",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = [\n",
    "    infinite_dataloader(partial(DataLoader, minigrid_dataset_xf, batch_size=BATCH_SIZE, collate_fn=minigrid_collate_fn, num_workers=4))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "101caa7d-1002-4dc0-851c-b771020928b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loss\n",
    "##\n",
    "## See section 2.3 of the Gato paper.\n",
    "##\n",
    "##   Let b index a training batch of sequences B. We define a masking function m\n",
    "##   such that m(b, l) = 1 if the token at index l is either from text or from\n",
    "##   the logged action of an agent, and 0 otherwise. The training loss for a\n",
    "##   batch B can then be written as...\n",
    "def cross_entropy(predicted, target, mask):\n",
    "    # See: https://youtu.be/kCc8FmEb1nY?list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ&t=1553\n",
    "    B, T, C = predicted.shape\n",
    "    predicted = predicted.view(B * T, C)\n",
    "    target = target.view(-1).to(torch.long)\n",
    "    losses = F.cross_entropy(predicted, target, reduction=\"none\")\n",
    "    losses = losses * mask.squeeze(-1).view(-1)\n",
    "    loss = losses.sum() / mask.sum()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "35d28ab7-f0d6-4a22-8065-ee253676c11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniGatoTrainer:\n",
    "    def __init__(self, model, optimizer, dataloaders):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.dataloaders = dataloaders\n",
    "        self.dl_it = cycle(dataloaders)\n",
    "        self.losses = []\n",
    "\n",
    "    def train(self, iterations=50):\n",
    "        self.model.train()\n",
    "        for i in tqdm(range(iterations)):\n",
    "            dl = next(self.dl_it)\n",
    "            batch = next(dl)\n",
    "            predicted, targets, attention_mask = self.model(batch)\n",
    "            loss = cross_entropy(predicted, targets, attention_mask)\n",
    "            self.losses.append(loss.item())\n",
    "            loss.backward()\n",
    "            self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3be9730a-e374-48a7-a674-875e966fba24",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = init_default_config()\n",
    "model = MiniGato(config).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters())\n",
    "trainer = MiniGatoTrainer(\n",
    "    model,\n",
    "    optimizer,\n",
    "    dataloaders,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6215fe53-e442-41ed-bac5-947582c7eddd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bce1fb296032457292d4d29a30b5dcf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15be08ee472a445383c0d19fab74e370",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd58f38c48134bb8bf0c6bd64183ca1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "751a7ed08d5d4d08b72609f6cf01de80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b8b6f8714794c608748dc722fdfeab0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()\n",
    "trainer.train()\n",
    "trainer.train()\n",
    "trainer.train()\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "341cc2f6-e034-4d70-bf86-179bebc32923",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10.339223861694336,\n",
       " 1.6069600582122803,\n",
       " 2.229372024536133,\n",
       " 1.258705735206604,\n",
       " 2.7163641452789307,\n",
       " 1.777280330657959,\n",
       " 2.38631534576416,\n",
       " 1.599935531616211,\n",
       " 1.9081206321716309,\n",
       " 1.6184971332550049,\n",
       " 1.4483606815338135,\n",
       " 1.645542860031128,\n",
       " 0.7315148711204529,\n",
       " 1.316531777381897,\n",
       " 1.6763911247253418,\n",
       " 0.5046938061714172,\n",
       " 0.47034740447998047,\n",
       " 0.7731776237487793,\n",
       " 1.2030525207519531,\n",
       " 2.016554594039917,\n",
       " 2.9713335037231445,\n",
       " 3.7278053760528564,\n",
       " 4.147859573364258,\n",
       " 4.27758264541626,\n",
       " 4.491320610046387,\n",
       " 4.8445353507995605,\n",
       " 4.7534050941467285,\n",
       " 4.044671058654785,\n",
       " 2.968477487564087,\n",
       " 1.7000150680541992,\n",
       " 0.8968724608421326,\n",
       " 0.7794124484062195,\n",
       " 1.0121744871139526,\n",
       " 0.5032528042793274,\n",
       " 2.1747968196868896,\n",
       " 3.135807514190674,\n",
       " 0.7833988070487976,\n",
       " 1.7329763174057007,\n",
       " 2.0020411014556885,\n",
       " 2.197782278060913,\n",
       " 3.306472063064575,\n",
       " 3.6065943241119385,\n",
       " 3.0668156147003174,\n",
       " 3.968012809753418,\n",
       " 2.7870094776153564,\n",
       " 5.604249000549316,\n",
       " 9.405218124389648,\n",
       " 7.041199684143066,\n",
       " 10.381896018981934,\n",
       " 4.1935200691223145,\n",
       " 8.90310001373291,\n",
       " 5.96910285949707,\n",
       " 12.167180061340332,\n",
       " 8.352463722229004,\n",
       " 8.601020812988281,\n",
       " 5.6523871421813965,\n",
       " 1.9354031085968018,\n",
       " 5.351975917816162,\n",
       " 8.510807991027832,\n",
       " 3.3856375217437744,\n",
       " 6.358719825744629,\n",
       " 8.265571594238281,\n",
       " 9.66893196105957,\n",
       " 6.778173446655273,\n",
       " 4.353558540344238,\n",
       " 10.619606018066406,\n",
       " 10.711928367614746,\n",
       " 9.565101623535156,\n",
       " 6.435332298278809,\n",
       " 7.695609092712402,\n",
       " 12.116175651550293,\n",
       " 14.416638374328613,\n",
       " 9.05405330657959,\n",
       " 5.116767883300781,\n",
       " 11.357747077941895,\n",
       " 9.00313949584961,\n",
       " 8.500839233398438,\n",
       " 0.0,\n",
       " 6.484931468963623,\n",
       " 8.048171043395996,\n",
       " 7.011363506317139,\n",
       " 8.123392105102539,\n",
       " 9.307361602783203,\n",
       " 4.346917629241943,\n",
       " 4.3173017501831055,\n",
       " 7.997313022613525,\n",
       " 4.560544967651367,\n",
       " 5.560678005218506,\n",
       " 5.53632116317749,\n",
       " 3.6661031246185303]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06eed7b-9bd8-4870-a3a2-2a4138eae6c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15942866-1fac-44c0-a0e3-8b0a1eafade1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba529169-b327-4b7e-a262-5325f4d7a08f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
