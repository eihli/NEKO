{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "949b7cdb-1309-4a42-af31-70ddbbaa978f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "from dataclasses import dataclass, fields\n",
    "from functools import partial\n",
    "from itertools import cycle\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pdb\n",
    "import random\n",
    "import re\n",
    "import tempfile\n",
    "from einops import rearrange\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import minari\n",
    "from minigrid.core import constants as mgc\n",
    "import requests\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "import torchvision.transforms.v2 as transforms\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import GPT2Tokenizer, GPT2Config, GPT2Model\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b901d325-a96d-463d-88f6-31702432d715",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed7f7ec7-1ea5-4fd0-8210-cbb96e80330f",
   "metadata": {},
   "outputs": [],
   "source": [
    "minigrid_dataset = minari.load_dataset('D4RL/minigrid/fourrooms-v0', download=True)\n",
    "env  = minigrid_dataset.recover_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7a51868-93a6-4a29-b669-b7dca65d3985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note on shapes:\n",
    "# You're probably familiar with the old (B, T, C, ...) shape â€“ batch, timestep, channel.\n",
    "@dataclass\n",
    "class TokenData:\n",
    "    tokens: torch.Tensor\n",
    "    targets: torch.Tensor\n",
    "    attention_mask: torch.Tensor\n",
    "    embedding: torch.Tensor = torch.tensor([])  # Optional at first.\n",
    "\n",
    "    def combine(self, other):\n",
    "        \"\"\"Concats attributes of self to attributes of other.\"\"\"\n",
    "        # Requires padding to already be handled.\n",
    "        # Requires shapes to be (T', T, [C, ...])\n",
    "        # Where T' is episode timestep and T is the usual timestep.\n",
    "        return type(self)(\n",
    "            tokens=torch.concat([self.tokens, other.tokens]),\n",
    "            targets=torch.concat([self.targets, other.targets]),\n",
    "            attention_mask=torch.concat([self.attention_mask, other.attention_mask]),\n",
    "            embedding=torch.concat([self.embedding, other.embedding]),\n",
    "            \n",
    "        )\n",
    "\n",
    "    def embed(self, embedder):\n",
    "        raise NotImplementedError('See subclass')\n",
    "\n",
    "    def to(self, device):\n",
    "        return type(self)(\n",
    "            tokens=self.tokens.to(device),\n",
    "            targets=self.targets.to(device),\n",
    "            attention_mask=self.attention_mask.to(device),\n",
    "            embedding=self.embedding.to(device),\n",
    "        )\n",
    "        \n",
    "    @property\n",
    "    def size(self):\n",
    "        \"\"\"The number of tokens this will consume of the context window\"\"\"\n",
    "        return self.tokens.size(0) * self.tokens.size(1)\n",
    "\n",
    "class TextTokenData(TokenData):\n",
    "    def embed(self, embedder):\n",
    "        return type(self)(\n",
    "            tokens=self.tokens,\n",
    "            targets=self.targets,\n",
    "            attention_mask=self.attention_mask,\n",
    "            embedding=embedder.text(self.tokens),\n",
    "        ) \n",
    "\n",
    "class ImageTokenData(TokenData):\n",
    "    def embed(self, embedder):\n",
    "        return type(self)(\n",
    "            tokens=self.tokens,\n",
    "            targets=self.targets,\n",
    "            attention_mask=self.attention_mask,\n",
    "            embedding=embedder.image(self.tokens),\n",
    "        )\n",
    "\n",
    "class DiscreteTokenData(TokenData):\n",
    "    def embed(self, embedder):\n",
    "        return type(self)(\n",
    "            tokens=self.tokens,\n",
    "            targets=self.targets,\n",
    "            attention_mask=self.attention_mask,\n",
    "            embedding=embedder.discrete(self.tokens),\n",
    "        ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7503a9c7-97ae-47fe-8d43-c92b19671e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class EpisodeData:\n",
    "    def __getitem__(self, i):\n",
    "        # Iterate over fields\n",
    "        return type(self)(**{\n",
    "            field.name: type(getattr(self, field.name))(\n",
    "                tokens=getattr(self, field.name).tokens[[i]],\n",
    "                targets=getattr(self, field.name).targets[[i]],\n",
    "                attention_mask=getattr(self, field.name).attention_mask[[i]],\n",
    "            )\n",
    "            for field in fields(self)\n",
    "        })\n",
    "\n",
    "    def combine(self, other):\n",
    "        return type(self)(**{\n",
    "            field.name: getattr(self, field.name).combine(getattr(other, field.name))\n",
    "            for field in fields(self)\n",
    "        })\n",
    "\n",
    "    @property\n",
    "    def size(self):\n",
    "        return sum(getattr(self, field.name).size for field in fields(self))\n",
    "\n",
    "    @property\n",
    "    def num_timesteps(self):\n",
    "        return next(getattr(self, field.name) for field in fields(self)).tokens.size(0)\n",
    "\n",
    "    def embed(self, embedder):\n",
    "        return type(self)(**{\n",
    "            field.name: getattr(self, field.name).embed(embedder)\n",
    "            for field in fields(self)\n",
    "        })\n",
    "\n",
    "    def to(self, device):\n",
    "        return type(self)(**{\n",
    "            field.name: getattr(self, field.name).to(device)\n",
    "            for field in fields(self)\n",
    "        })\n",
    "\n",
    "    def sequence(self, embeddings):\n",
    "        raise Exception('Override me')\n",
    "\n",
    "@dataclass\n",
    "class FourRoomsTimestep(EpisodeData):\n",
    "    mission: TextTokenData  # torch.Size((length of episode subsequence, length of _max_ (pad) mission text tokens))\n",
    "    image: ImageTokenData\n",
    "    direction: DiscreteTokenData\n",
    "    actions: DiscreteTokenData\n",
    "\n",
    "    def sequence(self, sequence_length):\n",
    "        xs = torch.concat([self.mission.embedding, self.image.embedding, self.direction.embedding, self.actions.embedding], dim=1)\n",
    "        ys = torch.concat([self.mission.targets, self.image.targets, self.direction.targets, self.actions.targets], dim=1)\n",
    "        ms = torch.concat([self.mission.attention_mask, self.image.attention_mask, self.direction.attention_mask, self.actions.attention_mask], dim=1)\n",
    "        T, S, C = xs.shape\n",
    "        xs, ys, ms = xs.reshape(T*S, C), ys.reshape(T*S), ms.reshape(T*S)\n",
    "        padding_len = sequence_length - T*S\n",
    "        xs = F.pad(xs, (0, 0, 0, padding_len), value=0)\n",
    "        ys, ms = [F.pad(x, (0, padding_len), value=0) for x in [ys, ms]]\n",
    "        return xs, ys, ms\n",
    "\n",
    "@dataclass\n",
    "class TextTimestep(EpisodeData):\n",
    "    text: TextTokenData\n",
    "    \n",
    "    def sequence(self, _):\n",
    "        # TODO: HERE!!!!!!!!!!!!!!!!\n",
    "        return self.text.embedding.squeeze(0), self.text.targets.squeeze(0), self.text.attention_mask.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9fac1cbd-1dfd-454e-b5d5-9f1be3bc7dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_embedding = nn.Embedding(__text_tokenizer.vocab_size, 768)\n",
    "# image_embedding = ResNetV2Block(3, 768)\n",
    "# discrete_embedding = nn.Embedding(1024, 768)\n",
    "# embedder = Embedder(text=text_embedding, image=image_embedding, discrete=discrete_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "648b8643-d58e-41d4-9034-222e182c63d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample = minigrid_dataset[0]\n",
    "# tokenized = minigrid_tokenize(sample)\n",
    "# embedded = tokenized.embed(embedder)\n",
    "# xs = torch.concat([embedded.mission.embedding, embedded.image.embedding, embedded.direction.embedding, embedded.actions.embedding], dim=1)\n",
    "# xs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "312f9ba5-3195-4562-9f04-3ba50d4ba393",
   "metadata": {},
   "outputs": [],
   "source": [
    "def acquire_shakespeare_dataset():\n",
    "    temp_dir = tempfile.gettempdir()\n",
    "    shakespeare_filepath = Path(temp_dir)/\"shakespeare.txt\"\n",
    "    if not os.path.exists(shakespeare_filepath):\n",
    "        data_url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
    "        with open(shakespeare_filepath, 'w', encoding='utf-8') as f:\n",
    "            f.write(requests.get(data_url).text)\n",
    "    \n",
    "    with open(shakespeare_filepath, 'r', encoding='utf-8') as f:\n",
    "        data = f.read()\n",
    "\n",
    "    # Split the dataset into each character's lines.\n",
    "    # Continue taking lines until you have at least 250 words in the sample.\n",
    "    # Add that sample to the dataset.\n",
    "    characters_lines = re.split(r\"\\n\\s*\\n\", data.strip())\n",
    "    MIN_WORDS_PER_BATCH = 250\n",
    "    sample = [characters_lines[0]]\n",
    "    num_words_in_sample = len(characters_lines[0].split())\n",
    "    text_dataset = []\n",
    "    i = 1\n",
    "    while i < len(characters_lines):\n",
    "        if num_words_in_sample > MIN_WORDS_PER_BATCH:\n",
    "            text_dataset.append(\"\\n\\n\".join(sample))\n",
    "            num_words_in_sample -= len(sample[0].split())\n",
    "            sample = sample[1:]\n",
    "        sample += [characters_lines[i]]\n",
    "        num_words_in_sample += len(characters_lines[i].split())\n",
    "        i += 1\n",
    "\n",
    "    return text_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11d13aef-f1de-49a9-80af-c229b24cd55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "shakespeare_dataset = acquire_shakespeare_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19bee90e-e4f9-44a3-ac01-29f7cb30a64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self, text_gen_tokenizer, text_obs_tokenizer):\n",
    "        self.text_gen_tokenizer = text_gen_tokenizer\n",
    "        self.text_obs_tokenizer = text_obs_tokenizer\n",
    "\n",
    "    @property\n",
    "    def bos_token(self):\n",
    "        return self.text_gen_tokenizer.func.bos_token\n",
    "\n",
    "    @property\n",
    "    def eos_token(self):\n",
    "        return self.text_gen_tokenizer.func.eos_token\n",
    "\n",
    "    def text_gen(self, data, **kwargs):\n",
    "        tokenized =  self.text_gen_tokenizer(data, **kwargs)\n",
    "        return TextTokenData(**{\n",
    "            \"tokens\": tokenized[\"input_ids\"][:, :-1],\n",
    "            \"targets\": tokenized[\"input_ids\"][:, 1:],\n",
    "            \"attention_mask\": tokenized[\"attention_mask\"][:, :-1],\n",
    "        })\n",
    "\n",
    "    def text_obs(self, data, **kwargs):\n",
    "        tokenized =  self.text_obs_tokenizer(data, **kwargs)\n",
    "        return TextTokenData(**{\n",
    "            \"tokens\": tokenized[\"input_ids\"],\n",
    "            \"targets\": tokenized[\"input_ids\"].to(torch.long),\n",
    "            \"attention_mask\": torch.zeros_like(tokenized[\"attention_mask\"]),\n",
    "        })\n",
    "\n",
    "    def image(self, data):\n",
    "        if len(data.shape) == 3:\n",
    "          data = data.unsqueeze(0)\n",
    "        patches = images_to_patches(data, patch_size=16)\n",
    "        # Hardcoding as a reminder to do something smarter\n",
    "        SQUARE_ROOT_OF_PATCH_SIZE = 3.464\n",
    "        xs = (\n",
    "            apply_along_dimension(\n",
    "                normalize_to_between_minus_one_plus_one, 2, patches\n",
    "            )\n",
    "            / SQUARE_ROOT_OF_PATCH_SIZE\n",
    "        )\n",
    "        # We don't predict images, but we need ys\n",
    "        # becaues these image ys will be in our\n",
    "        # concatenated ys of text/image/action/etc...\n",
    "        ys = torch.zeros(xs.shape[:2])\n",
    "        ms = torch.zeros(xs.shape[:2])  # Same story as above.\n",
    "        return ImageTokenData(tokens=xs, targets=ys, attention_mask=ms)\n",
    "\n",
    "    def discrete_obs(self, data):\n",
    "        if len(data.shape) == 0:\n",
    "            data = data.unsqueeze(0)\n",
    "        if len(data.shape) == 1:\n",
    "            data = data.unsqueeze(1)\n",
    "        xs = data\n",
    "        ys = torch.zeros(xs.shape[:2])\n",
    "        ms = torch.zeros(xs.shape[:2])\n",
    "        return DiscreteTokenData(tokens=xs, targets=ys, attention_mask=ms)\n",
    "\n",
    "    def discrete_act(self, data):\n",
    "        if len(data.shape) == 0:\n",
    "            data = data.unsqueeze(0)\n",
    "        if len(data.shape) == 1:\n",
    "            data = data.unsqueeze(1)\n",
    "        xs = torch.concat([\n",
    "            torch.full((data.size(0), 1), 1023),\n",
    "            data,\n",
    "        ], dim=1)  # Instead of '|' being the separator, like Gato...\n",
    "        ys = torch.concat([\n",
    "            data,\n",
    "            torch.full((data.size(0), 1), 1023),            \n",
    "        ], dim=1)\n",
    "        ms = torch.ones(*ys.shape)\n",
    "        return DiscreteTokenData(tokens=xs, targets=ys, attention_mask=ms)\n",
    "\n",
    "    def continuous(self, data):\n",
    "        raise Exception('TODO: Tokenizer.continuous')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3649c8cc-5946-4046-92c5-32c1f210ad65",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformDataset(Dataset):\n",
    "    def __init__(self, dataset, transform):\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.transform(self.dataset[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bae1599c-4eec-453b-af2d-6cffe906c216",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQUENCE_LENGTH = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "98e0883f-5c10-40c1-b766-9a0ad9a9839f",
   "metadata": {},
   "outputs": [],
   "source": [
    "__text_tokenizer = GPT2Tokenizer.from_pretrained(\"openai-community/gpt2\", clean_up_tokenization_spaces=True)\n",
    "__text_tokenizer.pad_token = __text_tokenizer.eos_token\n",
    "_text_gen_tokenizer = partial(\n",
    "    __text_tokenizer,\n",
    "    max_length=SEQUENCE_LENGTH+1,\n",
    "    truncation=True,\n",
    "    padding=\"max_length\",\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "_text_obs_tokenizer = partial(\n",
    "    __text_tokenizer,\n",
    "    max_length=SEQUENCE_LENGTH,\n",
    "    truncation=True,\n",
    "    padding=\"longest\",\n",
    "    return_tensors=\"pt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fff59177-d1b5-4fa5-8354-f59756841803",
   "metadata": {},
   "outputs": [],
   "source": [
    "def images_to_patches(images, patch_size=16):\n",
    "    return rearrange(images, 'b c (h s1) (w s2) -> b (h w) (c s1 s2)', s1=patch_size, s2=patch_size)\n",
    "def normalize_to_between_minus_one_plus_one(t: torch.Tensor):\n",
    "    min_val, max_val = t.min(), t.max()\n",
    "    if min_val == max_val:\n",
    "        return torch.zeros_like(t)\n",
    "    normalized = 2 * (t - min_val) / (max_val - min_val) - 1\n",
    "    return normalized\n",
    "# There's a small deviation in the NEKO codebase from the paper.\n",
    "# The paper normalizes _per patch_. The NEKO codebase currently normalizes _per image_.\n",
    "# https://github.com/eihli/NEKO/blob/master/gato/policy/embeddings.py#L38\n",
    "# This notebook normalizeds per patch. That's what this utility helps.\n",
    "def apply_along_dimension(func, dim, tensor):\n",
    "    tensor = tensor.transpose(0, dim)\n",
    "    shape = tensor.shape\n",
    "    tensor = tensor.reshape(shape[0], -1)\n",
    "    result = torch.stack([func(tensor[:, i]) for i in range(tensor.size(1))], dim=1)\n",
    "    result = result.reshape(shape).transpose(0, dim)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "831e1f5d-4bf5-4336-bf54-029a7506c11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lookup table\n",
    "lut = np.zeros((256, 3), dtype=np.uint8)\n",
    "for idx, color_name in mgc.IDX_TO_COLOR.items():\n",
    "    lut[idx] = mgc.COLORS[color_name]\n",
    "\n",
    "def minigrid_to_rgb(episode):\n",
    "    \"\"\"Convert discrete \"image\" observations into actual images.\n",
    "    I'm expecting this will improve our image modality while not losing\n",
    "    much. The downside is we can fit less in our context window. Note:\n",
    "    We might need to overlay the color/type image (index 1) with the\n",
    "    state image (index 2), if we really don't want to lose any info.\"\"\"\n",
    "    # Apply lookup to second channel\n",
    "    image = lut[episode.observations['image'][:, :, :, 1]]\n",
    "    # Convert to PyTorch tensor and permute\n",
    "    image = torch.from_numpy(image).permute(0, 3, 1, 2)\n",
    "    return image\n",
    "\n",
    "image_transform = transforms.Compose([\n",
    "    # No particular reason to use `transforms.Compose` here since we're only doing one transform. But it's nice to know about.\n",
    "    transforms.RandomResizedCrop((192, 192), (0.5, 1.0)),\n",
    "    transforms.ToDtype(torch.float32, scale=True),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "def minigrid_tokenizer(tokenizer, episode):\n",
    "    num_timesteps = len(episode.actions)\n",
    "    image = image_transform(minigrid_to_rgb(episode)[:num_timesteps])\n",
    "    image = tokenizer.image(image[:num_timesteps])\n",
    "    mission = tokenizer.text_obs(episode.observations['mission'][:num_timesteps], padding=False)\n",
    "    direction = tokenizer.discrete_obs(torch.from_numpy(episode.observations['direction'])[:num_timesteps])\n",
    "    actions = tokenizer.discrete_act(torch.from_numpy(episode.actions))\n",
    "    return FourRoomsTimestep(mission=mission, image=image, direction=direction, actions=actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa77431b-0e4a-4c04-80c4-5d7f6dda53f4",
   "metadata": {},
   "source": [
    "### Collation on the GPU:\n",
    "\n",
    "https://pytorch.org/docs/stable/data.html#torch.utils.data.distributed.DistributedSampler\n",
    "\n",
    "```\n",
    ">>> sampler = DistributedSampler(dataset) if is_distributed else None\n",
    ">>> loader = DataLoader(dataset, shuffle=(sampler is None),\n",
    "...                     sampler=sampler)\n",
    ">>> for epoch in range(start_epoch, n_epochs):\n",
    "...     if is_distributed:\n",
    "...         sampler.set_epoch(epoch)\n",
    "...     train(loader)\n",
    "```\n",
    "\n",
    "We can only collate after we embed. Different modalities won't have the same dimensions until after embedding. Do we want to embed in the collate_fn?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "38d670a2-e681-44f0-a108-ca3190c2e8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def minigrid_collate_fn(batch):\n",
    "    result = []\n",
    "    for sample in batch:\n",
    "        i = random.randint(0, sample.num_timesteps - 1)\n",
    "        i = 0  # TODO: Remove FIX REMOVE!!!\n",
    "\n",
    "        # Starting at that index, we'll continue adding observations to our context window until\n",
    "        # we run out of space.\n",
    "        step = sample[i]\n",
    "        i += 1\n",
    "        while i < len(sample.actions.tokens) and step.size + step[0].size < SEQUENCE_LENGTH:\n",
    "            step = step.combine(sample[i])\n",
    "            i += 1\n",
    "        result.append(step)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "27917b2f-aacf-4655-985a-5384d0f4bb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_tokenizer(tokenizer, text):\n",
    "    return TextTimestep(text=tokenizer.text_gen(tokenizer.bos_token + text + tokenizer.eos_token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4c9b01e0-e4db-469a-9f64-cc969e27b99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(_text_gen_tokenizer, _text_obs_tokenizer)\n",
    "minigrid_tokenize = partial(minigrid_tokenizer, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bd9f8582-3d47-4dcd-9ea5-6fac980b61b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tokenize = partial(text_tokenizer, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f9e6367a-96f1-4799-ab03-ab87f728de2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7f88bdb1-9ef9-47f4-b80a-09ca2241bcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "shakespeare_dataset_xf = TransformDataset(shakespeare_dataset, text_tokenize)\n",
    "minigrid_dataset_xf = TransformDataset(minigrid_dataset, minigrid_tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab782085-0c55-4eee-a85b-1016f0ba8da2",
   "metadata": {},
   "source": [
    "**TODO**: Text is returning TextTokenData and minigrid is returning FourroomsTimestep. I don't like that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "619eed52-4e50-4f40-afaf-8f7e86ea0efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "shakespeare_dataloader = DataLoader(shakespeare_dataset_xf, batch_size=BATCH_SIZE, collate_fn=lambda x: x)\n",
    "shakespeare_batch = next(iter(shakespeare_dataloader))\n",
    "shakespeare_batch;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3ead6b39-d219-44ad-a336-6b9fe2315ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "minigrid_dataloader = DataLoader(minigrid_dataset_xf, batch_size=BATCH_SIZE, collate_fn=minigrid_collate_fn)\n",
    "minigrid_batch = next(iter(minigrid_dataloader))\n",
    "minigrid_batch;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "95e806b1-9d64-4bbf-8604-ce70793e2198",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# From section 2.2 of the Gato paper:\n",
    "#\n",
    "#    Tokens belonging to image patches for any time-step are embedded using a\n",
    "#    single ResNet (He et al., 2016a) block to obtain a vector per patch. For\n",
    "#    image patch token embeddings, we also add a learnable within-image position\n",
    "#    encoding vector.\n",
    "class ResNetV2Block(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, num_groups=24):\n",
    "        super(ResNetV2Block, self).__init__()\n",
    "        self.gn1 = nn.GroupNorm(1, in_channels)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size=3,\n",
    "            stride=stride,\n",
    "            padding=1,\n",
    "            bias=False,\n",
    "        )\n",
    "        self.gn2 = nn.GroupNorm(num_groups, out_channels)\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            out_channels, in_channels, kernel_size=3, stride=1, padding=1, bias=False\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, CHW = x.shape\n",
    "        # TODO: Remove these hardcoded values.\n",
    "        out = rearrange(x, 'b t (c h w) -> (b t) c h w', c=3, h=16)\n",
    "        out = self.gn1(out)\n",
    "        out = self.gelu(out)\n",
    "        out = self.conv1(out)\n",
    "        out = self.gn2(out)\n",
    "        out = self.gelu(out)\n",
    "        out = self.conv2(out)\n",
    "        return x + rearrange(out, '(b t) c h w -> b t (c h w)', b=B, t=T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1cb5e0d9-a958-4360-b900-73915d986790",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Embedder:\n",
    "    text: Callable\n",
    "    image: Callable\n",
    "    discrete: Callable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9fc4b833-e1d8-45df-9d0b-d7c828b5c7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class MiniGatoConfig:\n",
    "    embedding_dim: int\n",
    "    sequence_length: int\n",
    "    vocab_size: int \n",
    "    transformer_config: GPT2Config\n",
    "    transformer: GPT2Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "36ac07b5-7677-4b7a-9ee0-c22ed5c21714",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_default_config() -> MiniGatoConfig:\n",
    "    transformer_config = GPT2Config()\n",
    "    return MiniGatoConfig(\n",
    "        embedding_dim=768,\n",
    "        sequence_length=1024,\n",
    "        vocab_size=__text_tokenizer.vocab_size,\n",
    "        transformer_config=transformer_config,\n",
    "        transformer=GPT2Model(transformer_config),\n",
    "    )\n",
    "default_config = init_default_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "84775ff6-30a1-4786-a9c5-bcee084f58c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniGato(nn.Module):\n",
    "    def __init__(self, config: MiniGatoConfig=default_config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.sequence_length = self.config.sequence_length\n",
    "        text_embedding = nn.Embedding(self.config.vocab_size, self.config.embedding_dim)\n",
    "        image_embedding = ResNetV2Block(3, self.config.embedding_dim)\n",
    "        discrete_embedding = nn.Embedding(1024, self.config.embedding_dim)\n",
    "        self.embedder = Embedder(text=text_embedding, image=image_embedding, discrete=discrete_embedding)\n",
    "        self.transformer = self.config.transformer\n",
    "        self.lm_head = nn.Linear(self.transformer.config.hidden_size, self.config.vocab_size)     \n",
    "\n",
    "    def forward(self, batch):\n",
    "        batch = [\n",
    "            sample.embed(self.embedder).sequence(self.sequence_length) for sample in batch\n",
    "        ]\n",
    "        xs, ys, ms = map(torch.stack, zip(*batch))\n",
    "        xs, ys, ms = [x.to(device) for x in [xs, ys, ms]]\n",
    "        out = self.transformer(inputs_embeds=xs)\n",
    "        predicted = self.lm_head(out.last_hidden_state)\n",
    "        return predicted, ys, ms     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2e40fcf0-be16-4f91-89cf-17e8e1d4fa47",
   "metadata": {},
   "outputs": [],
   "source": [
    "minigrid_dataloader = DataLoader(minigrid_dataset_xf, batch_size=BATCH_SIZE, collate_fn=minigrid_collate_fn, num_workers=4)\n",
    "minigrid_iterator = iter(minigrid_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bdf45b06-7b45-4379-90c7-ccd72f8e67e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "minigrid_batch = next(minigrid_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9e420165-26b3-4144-b11b-432223186e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infinite_dataloader(fn):\n",
    "    it = iter(fn())\n",
    "    while True:\n",
    "        try:\n",
    "            yield next(it)\n",
    "        except StopIteration:\n",
    "            it = iter(fn())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "101caa7d-1002-4dc0-851c-b771020928b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loss\n",
    "##\n",
    "## See section 2.3 of the Gato paper.\n",
    "##\n",
    "##   Let b index a training batch of sequences B. We define a masking function m\n",
    "##   such that m(b, l) = 1 if the token at index l is either from text or from\n",
    "##   the logged action of an agent, and 0 otherwise. The training loss for a\n",
    "##   batch B can then be written as...\n",
    "def cross_entropy(predicted, target, mask):\n",
    "    # See: https://youtu.be/kCc8FmEb1nY?list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ&t=1553\n",
    "    B, T, C = predicted.shape\n",
    "    predicted = predicted.view(B * T, C)\n",
    "    target = target.view(-1).to(torch.long)\n",
    "    losses = F.cross_entropy(predicted, target, reduction=\"none\")\n",
    "    losses = losses * mask.squeeze(-1).view(-1)\n",
    "    loss = losses.sum() / (mask.sum() + 1e-8)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "35d28ab7-f0d6-4a22-8065-ee253676c11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniGatoTrainer:\n",
    "    def __init__(self, model, optimizer, dataloaders, scheduler=None):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.dataloaders = dataloaders\n",
    "        self.scheduler = scheduler\n",
    "        self.dl_it = cycle(dataloaders)\n",
    "        self.losses = []\n",
    "\n",
    "    def train(self, iterations=10):\n",
    "        self.model.train()\n",
    "        for i in tqdm(range(iterations)):\n",
    "            dl = next(self.dl_it)\n",
    "            batch = next(dl)\n",
    "            optimizer.zero_grad()\n",
    "            predicted, targets, attention_mask = self.model(batch)\n",
    "            loss = cross_entropy(predicted, targets, attention_mask)\n",
    "            self.losses.append(loss.item())\n",
    "            loss.backward()\n",
    "            if self.scheduler:\n",
    "                self.scheduler.step()\n",
    "            self.optimizer.step()\n",
    "\n",
    "    def save_checkpoint(self, name):\n",
    "        checkpoint_dir = os.path.expanduser(\"~/.cache/models/checkpoints/\")\n",
    "        if not checkpoint_dir.exists():\n",
    "            os.mkdir(checkpoint_dir, exists_ok=True, parents=True)\n",
    "        state = self.model.module.state_dict()\n",
    "        torch.save(state, checkpoint_dir/f\"{name}.pt\")               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2543609d-46c1-4ff0-83e7-2fec2e666ac8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Default process group has not been initialized, please make sure to call init_process_group.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m minigrid_sampler \u001b[38;5;241m=\u001b[39m \u001b[43mDistributedSampler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mminigrid_dataset_xf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m shakespeare_sampler \u001b[38;5;241m=\u001b[39m DistributedSampler(shakespeare_dataset_xf)\n",
      "File \u001b[0;32m~/.virtualenvs/neko/lib/python3.12/site-packages/torch/utils/data/distributed.py:68\u001b[0m, in \u001b[0;36mDistributedSampler.__init__\u001b[0;34m(self, dataset, num_replicas, rank, shuffle, seed, drop_last)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m dist\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[1;32m     67\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRequires distributed package to be available\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 68\u001b[0m     num_replicas \u001b[38;5;241m=\u001b[39m \u001b[43mdist\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_world_size\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rank \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m dist\u001b[38;5;241m.\u001b[39mis_available():\n",
      "File \u001b[0;32m~/.virtualenvs/neko/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:1832\u001b[0m, in \u001b[0;36mget_world_size\u001b[0;34m(group)\u001b[0m\n\u001b[1;32m   1829\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _rank_not_in_group(group):\n\u001b[1;32m   1830\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1832\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_get_group_size\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.virtualenvs/neko/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:864\u001b[0m, in \u001b[0;36m_get_group_size\u001b[0;34m(group)\u001b[0m\n\u001b[1;32m    862\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Get a given group's world size.\"\"\"\u001b[39;00m\n\u001b[1;32m    863\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m group \u001b[38;5;129;01mis\u001b[39;00m GroupMember\u001b[38;5;241m.\u001b[39mWORLD \u001b[38;5;129;01mor\u001b[39;00m group \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 864\u001b[0m     default_pg \u001b[38;5;241m=\u001b[39m \u001b[43m_get_default_group\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    865\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m default_pg\u001b[38;5;241m.\u001b[39msize()\n\u001b[1;32m    866\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m group\u001b[38;5;241m.\u001b[39msize()\n",
      "File \u001b[0;32m~/.virtualenvs/neko/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:1025\u001b[0m, in \u001b[0;36m_get_default_group\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Get the default process group created by init_process_group.\"\"\"\u001b[39;00m\n\u001b[1;32m   1024\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_initialized():\n\u001b[0;32m-> 1025\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1026\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDefault process group has not been initialized, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1027\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplease make sure to call init_process_group.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1028\u001b[0m     )\n\u001b[1;32m   1029\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n\u001b[1;32m   1030\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m not_none(GroupMember\u001b[38;5;241m.\u001b[39mWORLD)\n",
      "\u001b[0;31mValueError\u001b[0m: Default process group has not been initialized, please make sure to call init_process_group."
     ]
    }
   ],
   "source": [
    "minigrid_sampler = DistributedSampler(minigrid_dataset_xf)\n",
    "shakespeare_sampler = DistributedSampler(shakespeare_dataset_xf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6c746035-3724-4d09-a2fd-3f49d1b4d295",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = [\n",
    "    infinite_dataloader(partial(DataLoader, minigrid_dataset_xf, batch_size=BATCH_SIZE, collate_fn=minigrid_collate_fn, num_workers=4)),\n",
    "    infinite_dataloader(partial(DataLoader, shakespeare_dataset_xf, batch_size=BATCH_SIZE, collate_fn=lambda x: x, num_workers=4)),\n",
    "]\n",
    "dl_it = cycle(dataloaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3be9730a-e374-48a7-a674-875e966fba24",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = init_default_config()\n",
    "model = MiniGato(config).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dca0e9cf-e6fd-4e1f-bb71-14f0bc244eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters())\n",
    "trainer = MiniGatoTrainer(\n",
    "    model,\n",
    "    optimizer,\n",
    "    dataloaders,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6215fe53-e442-41ed-bac5-947582c7eddd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cb8b46d834c4251ac8ceab02468c6a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47e407a175954ddc889f0c52649134fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6db878ff7f544cb892fb86f95e4f7f48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57d10d4a85b848b19220cbbaef111c13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c353bf7e892c4fa0b55abe3ce8430585",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05aedd67508a41d69c7d92eeff93c43c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56e4a98c89c5448a8295af3255914b9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76738a588b614d9da67fd5736c901062",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cefc63dae2084552b2552eeccb1d922c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d8c04a43e6849e7ad46572faf7ee2f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "48717fdf-0c3e-40ef-a835-ca7e142e8eed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10.91530704498291,\n",
       " 11.32962417602539,\n",
       " 8.68697452545166,\n",
       " 9.166634559631348,\n",
       " 1.8496417999267578,\n",
       " 8.839045524597168,\n",
       " 1.2212587594985962,\n",
       " 7.981420516967773,\n",
       " 0.8231542110443115,\n",
       " 7.309205055236816,\n",
       " 0.5401440262794495,\n",
       " 6.6869282722473145,\n",
       " 0.510768711566925,\n",
       " 6.481671333312988,\n",
       " 0.6009461283683777,\n",
       " 6.456746578216553,\n",
       " 0.639145016670227,\n",
       " 6.738391876220703,\n",
       " 0.5269625186920166,\n",
       " 6.91206693649292,\n",
       " 0.37270379066467285,\n",
       " 6.958279132843018,\n",
       " 0.5022042393684387,\n",
       " 6.755616664886475,\n",
       " 0.4185013473033905,\n",
       " 6.541547775268555,\n",
       " 0.4634130895137787,\n",
       " 6.328721523284912,\n",
       " 0.33085793256759644,\n",
       " 6.610764980316162,\n",
       " 0.4138124883174896,\n",
       " 6.7215423583984375,\n",
       " 1.091659665107727,\n",
       " 7.007645606994629,\n",
       " 0.5916869640350342,\n",
       " 7.045884132385254,\n",
       " 0.489769846200943,\n",
       " 7.054982662200928,\n",
       " 0.5234229564666748,\n",
       " 6.774705410003662,\n",
       " 0.4156559407711029,\n",
       " 6.746601104736328,\n",
       " 0.5496558547019958,\n",
       " 6.646432876586914,\n",
       " 0.3648875653743744,\n",
       " 6.730229377746582,\n",
       " 0.4751492738723755,\n",
       " 7.033571243286133,\n",
       " 0.5418722033500671,\n",
       " 7.0171217918396,\n",
       " 0.4678763449192047,\n",
       " 6.592823505401611,\n",
       " 0.34550246596336365,\n",
       " 6.17683219909668,\n",
       " 0.3855654299259186,\n",
       " 5.86269998550415,\n",
       " 0.29410454630851746,\n",
       " 5.60242223739624,\n",
       " 0.2645651400089264,\n",
       " 5.653265953063965,\n",
       " 0.47943782806396484,\n",
       " 5.576663494110107,\n",
       " 0.4773828685283661,\n",
       " 5.527888774871826,\n",
       " 0.4255395233631134,\n",
       " 5.519456386566162,\n",
       " 0.4100383222103119,\n",
       " 5.977946758270264,\n",
       " 0.3532543480396271,\n",
       " 6.382739543914795,\n",
       " 0.3954307734966278,\n",
       " 6.182150840759277,\n",
       " 0.4863552153110504,\n",
       " 5.93947696685791,\n",
       " 0.40664324164390564,\n",
       " 5.74287223815918,\n",
       " 0.33118322491645813,\n",
       " 5.710347652435303,\n",
       " 0.3464682996273041,\n",
       " 6.015328884124756,\n",
       " 0.3341793715953827,\n",
       " 6.266883373260498,\n",
       " 0.41345906257629395,\n",
       " 5.807648658752441,\n",
       " 0.37999650835990906,\n",
       " 5.918004512786865,\n",
       " 0.32813751697540283,\n",
       " 5.738015651702881,\n",
       " 0.3576068580150604,\n",
       " 5.703932285308838,\n",
       " 0.49018457531929016,\n",
       " 6.025463104248047,\n",
       " 0.4222368896007538,\n",
       " 5.940670013427734,\n",
       " 0.33120980858802795,\n",
       " 5.810296058654785,\n",
       " 0.29789236187934875,\n",
       " 5.795718669891357,\n",
       " 0.3769668638706207,\n",
       " 5.844247341156006,\n",
       " 0.5588304400444031,\n",
       " 6.074440002441406,\n",
       " 0.38013604283332825,\n",
       " 6.2115607261657715,\n",
       " 0.3548077344894409,\n",
       " 6.087752342224121,\n",
       " 0.41487300395965576,\n",
       " 6.027153015136719,\n",
       " 0.32377907633781433,\n",
       " 5.923372268676758,\n",
       " 0.4025402367115021,\n",
       " 6.011229038238525,\n",
       " 0.2853648364543915,\n",
       " 5.777636528015137,\n",
       " 0.2653246223926544,\n",
       " 5.504671573638916,\n",
       " 0.4053727090358734,\n",
       " 5.298413276672363,\n",
       " 0.3332378566265106,\n",
       " 5.254349231719971,\n",
       " 0.38918551802635193,\n",
       " 5.54822301864624,\n",
       " 0.3239729702472687,\n",
       " 6.110071659088135,\n",
       " 0.431284099817276,\n",
       " 6.10388708114624,\n",
       " 0.3769364058971405,\n",
       " 5.9153008460998535,\n",
       " 0.32365021109580994,\n",
       " 5.62755012512207,\n",
       " 0.5311828255653381,\n",
       " 5.586864471435547,\n",
       " 0.3099948465824127,\n",
       " 5.896404266357422,\n",
       " 0.3519136607646942,\n",
       " 6.065113544464111,\n",
       " 0.29721567034721375,\n",
       " 5.811524391174316,\n",
       " 0.3337693214416504,\n",
       " 5.526846408843994,\n",
       " 0.411028116941452,\n",
       " 5.372063636779785,\n",
       " 0.3095121383666992,\n",
       " 5.385071754455566,\n",
       " 0.3702358901500702,\n",
       " 5.410765171051025,\n",
       " 0.40076616406440735,\n",
       " 5.7533369064331055,\n",
       " 0.3683907091617584,\n",
       " 5.764414310455322,\n",
       " 0.47321221232414246,\n",
       " 5.668980598449707,\n",
       " 0.38836416602134705,\n",
       " 5.723657608032227,\n",
       " 0.3305310308933258,\n",
       " 5.721536159515381,\n",
       " 0.48406681418418884,\n",
       " 5.5817670822143555,\n",
       " 0.4729881286621094,\n",
       " 5.817500591278076,\n",
       " 0.4075542688369751,\n",
       " 5.7931132316589355,\n",
       " 0.363446980714798,\n",
       " 5.737651824951172,\n",
       " 0.3776651918888092,\n",
       " 5.632650852203369,\n",
       " 0.3524092435836792,\n",
       " 5.498959064483643,\n",
       " 0.39100995659828186,\n",
       " 5.801650047302246,\n",
       " 0.3550957441329956,\n",
       " 5.986876964569092,\n",
       " 0.3883877992630005,\n",
       " 5.717433929443359,\n",
       " 0.3549281358718872,\n",
       " 5.45552396774292,\n",
       " 0.3767350912094116,\n",
       " 5.188975811004639,\n",
       " 0.444754958152771,\n",
       " 5.496545791625977,\n",
       " 0.4237707555294037,\n",
       " 5.578236103057861,\n",
       " 0.379427969455719,\n",
       " 5.5938568115234375,\n",
       " 0.3363168239593506,\n",
       " 5.4302473068237305,\n",
       " 0.3313179910182953,\n",
       " 5.1681060791015625,\n",
       " 0.38940194249153137,\n",
       " 4.91726541519165,\n",
       " 0.5431763529777527,\n",
       " 4.708808898925781,\n",
       " 0.313932865858078,\n",
       " 4.558249473571777,\n",
       " 0.3797454535961151,\n",
       " 4.901717662811279,\n",
       " 0.32791590690612793,\n",
       " 5.261898994445801,\n",
       " 0.5013066530227661,\n",
       " 5.3221821784973145,\n",
       " 0.435524582862854,\n",
       " 5.134552478790283,\n",
       " 0.3578164577484131,\n",
       " 4.875697135925293,\n",
       " 0.40910783410072327,\n",
       " 4.6746697425842285,\n",
       " 0.3850743770599365,\n",
       " 4.431282997131348,\n",
       " 0.32184654474258423,\n",
       " 4.467282295227051]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0957c9-d205-43d1-a90a-441feb4bfec3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e06eed7b-9bd8-4870-a3a2-2a4138eae6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inferencer:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        predicted, targets, attention_mask = self.model(batch)\n",
    "        # Our actual final prediction is at the index of the final attention mask.\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d348de1-63b1-4eca-ad54-c52acadd7f78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6325abb-1dd5-4784-836d-80cc824194c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "790bbdef-0ece-4e1a-a0dd-26d28a21d5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = iter(minigrid_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7b5c9d72-fc5f-4360-a39f-a47d6de39283",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2a3742cf-5d46-404e-9b2a-17b7edc09110",
   "metadata": {},
   "outputs": [],
   "source": [
    "ep = batch\n",
    "em = [e.embed(model.embedder) for e in ep]\n",
    "sq = [e.sequence(model.sequence_length) for e in em]\n",
    "xs, ys, ms = map(torch.stack, zip(*sq))\n",
    "pr, ys, ms = model(ep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "047b052c-1419-497f-92ed-865e02db0f1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(900,\n",
       " 6,\n",
       " tensor([[   0, 1023],\n",
       "         [   2, 1023],\n",
       "         [   0, 1023],\n",
       "         [   2, 1023],\n",
       "         [   2, 1023],\n",
       "         [   2, 1023]]),\n",
       " torch.Size([4, 1024, 50257]),\n",
       " torch.Size([4, 1024]))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ep[0].size, ep[0].size // ep[0][0].size, ep[-1].actions.targets, pr.shape, ys.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e26cddd2-66d4-4f39-b1f8-9ddfb1823449",
   "metadata": {},
   "outputs": [],
   "source": [
    "heat = 0.01\n",
    "prh = pr / heat\n",
    "sm = prh.softmax(dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "946e2e9b-3f56-4704-a1e8-f653a22f765f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1024, 50257])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86b0239-383d-45a5-8466-28310303f819",
   "metadata": {},
   "source": [
    "Which indexes do you want? Not the 1023, because of padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "cef167e1-0158-4db4-9db8-adc1a8e0b6e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 1024, 50257]),\n",
       " torch.Size([4, 1024]),\n",
       " torch.Size([12, 1]),\n",
       " tensor([[148],\n",
       "         [149],\n",
       "         [298],\n",
       "         [299],\n",
       "         [448],\n",
       "         [449],\n",
       "         [598],\n",
       "         [599],\n",
       "         [748],\n",
       "         [749],\n",
       "         [898],\n",
       "         [899]], device='cuda:0'))"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr.shape, ms.shape, ms[0].nonzero().shape, ms[0].nonzero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c6381f89-2bb7-42e3-8f89-77bdfb2df5dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([   1, 1023,    1, 1023,    2, 1023,    2, 1023,    2, 1023,    2, 1023],\n",
       "        device='cuda:0', dtype=torch.int32),\n",
       " tensor([   1, 1023,    2, 1023,    2, 1023,    2, 1023,    2, 1023,    0, 1023],\n",
       "        device='cuda:0', dtype=torch.int32),\n",
       " tensor([   1, 1023,    1, 1023,    2, 1023,    2, 1023,    0, 1023,    2, 1023],\n",
       "        device='cuda:0', dtype=torch.int32),\n",
       " tensor([   0, 1023,    2, 1023,    0, 1023,    2, 1023,    2, 1023,    2, 1023],\n",
       "        device='cuda:0', dtype=torch.int32)]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ys[batch_index][ms[batch_index].nonzero().flatten()].to(torch.int) for batch_index in range(len(batch))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "78896ef4-811c-4031-b828-bd6ea346f049",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([   1, 1023,    2, 1023,    2, 1023,    2, 1023,    2, 1023,    2, 1023],\n",
       "        device='cuda:0', dtype=torch.int32),\n",
       " tensor([   1, 1023,    2, 1023,    2, 1023,    2, 1023,    2, 1023,    2, 1023],\n",
       "        device='cuda:0', dtype=torch.int32),\n",
       " tensor([   1, 1023,    2, 1023,    2, 1023,    2, 1023,    2, 1023,    2, 1023],\n",
       "        device='cuda:0', dtype=torch.int32),\n",
       " tensor([   1, 1023,    2, 1023,    2, 1023,    2, 1023,    2, 1023,    2, 1023],\n",
       "        device='cuda:0', dtype=torch.int32)]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[pr[batch_index][ms[batch_index].nonzero().flatten()].argmax(dim=1).to(torch.int) for batch_index in range(len(batch))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c18f1d46-f2f2-418f-8095-26d6e83dda2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2],\n",
       "        [2],\n",
       "        [2],\n",
       "        [2]], device='cuda:0')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sl = torch.multinomial(sm[:, 893, :], num_samples=1)\n",
    "sl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "d26dda6b-369f-4804-9484-0650618e3e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_text():\n",
    "    model.eval()\n",
    "    text = \"First Citizen:\"\n",
    "    next_word_token = None\n",
    "    i = 0\n",
    "    while i < 20 and next_word_token != __text_tokenizer.eos_token:\n",
    "        with torch.no_grad():\n",
    "            tokens = text_tokenize(text)\n",
    "            x = tokens.embed(model.embedder).to(device)\n",
    "            pr, ys, ms = model([tokens])\n",
    "            heat = 0.7\n",
    "            prh = pr / heat\n",
    "            sm = prh.softmax(dim=2)\n",
    "            last_index = ms.nonzero()[-1].cpu()[1]\n",
    "            next_word_probs = sm[0, last_index-1]\n",
    "            next_word_token = torch.multinomial(next_word_probs, num_samples=1)\n",
    "            next_word = __text_tokenizer.decode(next_word_token)\n",
    "            text += next_word\n",
    "        i += 1\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "e9ebf0e2-653e-46e6-ab13-3da255fd02ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Third voices,\n",
      "\n",
      " and I, be.\n",
      "I, sir.\n",
      "The voices,\n"
     ]
    }
   ],
   "source": [
    "print(eval_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "6a7cd0c1-ddcd-4147-a8ec-391f12069386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Control targets:   [1, 1023, 1, 1023, 2, 1023, 2, 1023, 2, 1023, 2, 1023]\n",
      "Control predicted: [1, 1023, 2, 1023, 2, 1023, 2, 1023, 2, 1023, 2, 1023]\n"
     ]
    }
   ],
   "source": [
    "targets = [ys[batch_index][ms[batch_index].nonzero().flatten()].to(torch.int) for batch_index in range(len(batch))][0]\n",
    "predicted = [pr[batch_index][ms[batch_index].nonzero().flatten()].argmax(dim=1).to(torch.int) for batch_index in range(len(batch))][0]\n",
    "print(f\"Control targets:   {targets.tolist()}\\nControl predicted: {predicted.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "47af1818-be4d-4a83-a86d-bb6f324c484f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024, 50257])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm[0].shape["
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbb29a9-8aaa-4fa5-bdc8-a56133cec03f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b11738a2-9909-4355-a24f-b5728a4b908a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 2, 50257])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "af91e8a1-e5ba-490a-aec1-0e13d278a3dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0],\n",
       "        [0, 1],\n",
       "        [0, 2],\n",
       "        [0, 3],\n",
       "        [0, 4]], device='cuda:0')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ms.nonzero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d041aebe-16d8-41bc-9170-aca57dba4e51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 4])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ms.nonzero()[-1].cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8d1ec768-5107-4489-ac74-95a9bb76d429",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(25)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens.text.tokens[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "61b0de11-c3c0-4168-ab9f-2b83b6d07ee5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 1024, 50257]), tensor([0, 4], device='cuda:0'))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm.shape, last_index[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e86351-6897-4885-a88f-513c595c4b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "    tokens = tokenize_text([text], max_length=SEQUENCE_LENGTH, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "    x = embed_text(tokens[\"input_ids\"]).to(device)\n",
    "    m = tokens[\"attention_mask\"].to(device)\n",
    "    length = m.sum().item()\n",
    "    o = model(inputs_embeds=x)\n",
    "    predicted = lm_head(o.last_hidden_state)\n",
    "    chosen = torch.multinomial(predicted.softmax(dim=2)[:, length-1], num_samples=1)\n",
    "    token = chosen[0]\n",
    "    text += _text_tokenizer.decode(chosen[0])\n",
    "    i += 1\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194e4ad7-95e4-4a27-a683-d221ad1ce0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "text = \"First Citizen:\"\n",
    "token = None\n",
    "i = 0\n",
    "with torch.no_grad():\n",
    "    while i < 20 and token != _text_tokenizer.eos_token_id:\n",
    "        tokens = tokenize_text([text], max_length=SEQUENCE_LENGTH, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "        x = embed_text(tokens[\"input_ids\"]).to(device)\n",
    "        m = tokens[\"attention_mask\"].to(device)\n",
    "        length = m.sum().item()\n",
    "        o = model(inputs_embeds=x)\n",
    "        predicted = lm_head(o.last_hidden_state)\n",
    "        chosen = torch.multinomial(predicted.softmax(dim=2)[:, length-1], num_samples=1)\n",
    "        token = chosen[0]\n",
    "        text += _text_tokenizer.decode(chosen[0])\n",
    "        i += 1\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15942866-1fac-44c0-a0e3-8b0a1eafade1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "batch = {\n",
    "    \"question\": question,\n",
    "    \"image\": image,\n",
    "    \"answer\": [\"\"],\n",
    "}\n",
    "i = 0\n",
    "token = \"\"\n",
    "with torch.no_grad():\n",
    "    while i < 10 and token != _text_tokenizer.eos_token:\n",
    "        x, y, m = sequence_vqa(tokenize_text, embed_text, tokenize_image, embed_image, batch)\n",
    "        x, y, m = x.to(device), y.to(device), m.to(device)\n",
    "        o = model(inputs_embeds=x)\n",
    "        predicted = lm_head(o.last_hidden_state)\n",
    "        token = _text_tokenizer.decode(predicted.softmax(dim=2)[0].multinomial(num_samples=1).squeeze(1)[768+i])\n",
    "        token = _text_tokenizer.decode(predicted.argmax(dim=2).squeeze(0)[768+i])\n",
    "        # with temperature\n",
    "        heat = 0.1\n",
    "        heated = predicted / heat\n",
    "        token = _text_tokenizer.decode(heated.softmax(dim=2)[0].multinomial(num_samples=1).squeeze()[768+i])\n",
    "        batch[\"answer\"][0] += token\n",
    "        i += 1\n",
    "batch[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba529169-b327-4b7e-a262-5325f4d7a08f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
