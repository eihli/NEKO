{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ca5a8b4-c444-4c1c-89b6-be5fd1707120",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22f01b90-ff7b-423d-a19f-9a0cf6dff777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.5.2 (SDL 2.28.2, Python 3.10.12)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import gato.policy.mini_gato as mg\n",
    "from datasets import load_dataset\n",
    "import requests\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms import ToTensor, Resize, RandomCrop\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d895204-b73c-4ea4-b83b-cefd63694cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"NCCL_P2P_DISABLE\"] = \"1\"\n",
    "os.environ[\"NCCL_IB_DISABLE\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a22dbb-0f5d-4c76-8d6a-8ec385303cd9",
   "metadata": {},
   "source": [
    "# Working demo\n",
    "\n",
    "First, a quick demo that this works. The datasets and model parameters are hardcoded in `mini_gato.py` for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e75c3c6f-e0b9-4e5b-9863-941d8e9bef44",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mg.init_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9945316-dd79-4c75-beed-dbafbef6af88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/40], Loss: 10.968055725097656\n",
      "Epoch [10/40], Loss: 6.313449859619141\n",
      "Epoch [20/40], Loss: 7.09780216217041\n",
      "Epoch [30/40], Loss: 6.6844635009765625\n"
     ]
    }
   ],
   "source": [
    "model, lm_head, optimizer, accelerator, text_dataloader, vqa_dataloader = mg.train(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74cc5139-5069-4296-83ea-18938d31ed48",
   "metadata": {},
   "source": [
    "# ยง 2.1 Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30311f70-01d9-43ee-94a5-6cb26dc6ec85",
   "metadata": {},
   "source": [
    "## Text\n",
    "\n",
    "> There are infinite possible ways to transform data into tokens, including directly using the raw underlying byte stream. Below we report the tokenization scheme we found to produce the best results for Gato at the current scale using contemporary hardware and model architectures.\n",
    "> Text is encoded via SentencePiece (Kudo & Richardson, 2018) with 32000 subwords into the integer range [0, 32000).\n",
    "> ...\n",
    "\n",
    "For this example, we'll use GPT2. The only thing to note as you change tokenizers is that discrete/continuous values get tokenized to the 1024 numbers after the vocab size (32000 to 33024 in the case of SentencePiece). So, you'll need to make that update as you change tokenizers.\n",
    "\n",
    "GPT2 has a vocab size of 50256, so our discrete/continuous values will tokenize to the range 50256 to 51280."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed2dda7-732e-40d0-b888-1cbbe22ad239",
   "metadata": {},
   "source": [
    "### Example text dataset/dataloader\n",
    "\n",
    "Here's a couple of example text datasets/dataloaders."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596625cc-4731-4c7d-8566-b17ef5395f3f",
   "metadata": {},
   "source": [
    "#### Dataset\n",
    "\n",
    "How you get the dataset doesn't much matter. All that matters is:\n",
    "\n",
    "- It's an iterator (we expect to be using datasets too large to fit in memory).\n",
    "- It has train/valid/test splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e37acd07-79c0-42e6-b8bf-1e65436ff619",
   "metadata": {},
   "outputs": [],
   "source": [
    "wikitext_dataset = load_dataset(path=\"wikitext\", name=\"wikitext-2-v1\", streaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35313f5b-ac49-46f7-b308-48202531990e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IterableDatasetDict({\n",
       "    test: IterableDataset({\n",
       "        features: ['text'],\n",
       "        n_shards: 1\n",
       "    })\n",
       "    train: IterableDataset({\n",
       "        features: ['text'],\n",
       "        n_shards: 1\n",
       "    })\n",
       "    validation: IterableDataset({\n",
       "        features: ['text'],\n",
       "        n_shards: 1\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikitext_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b359ca63-e0f6-48f2-980c-8278f1250616",
   "metadata": {},
   "source": [
    "#### Cleaning and transforming the dataset\n",
    "\n",
    "The Wikitext dataset contains a lot of samples that are empty.\n",
    "\n",
    "We can remove those with a call to `.filter(lambda: x: x[\"text\"] != '')`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8b6fb0a8-f0df-41d4-8121-6f74421a920c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ''}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(wikitext_dataset[\"train\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5a6310b9-8b72-45f0-86e4-3f606a61b30f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ' = Valkyria Chronicles III = \\n'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(\n",
    "    iter(\n",
    "        wikitext_dataset[\"train\"]\n",
    "          .filter(lambda x: x[\"text\"] != '')\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a1e2b9-f5e1-4912-b7fd-b60194b310d6",
   "metadata": {},
   "source": [
    "Remember, though, each dataset is unique. \n",
    "\n",
    "This filter is necessary and works for wikitext, but it might not be the right filter to use for some other dataset. That's why it's important to have flexible api's, like `filter` and `map`, and a solid set of _composable_ utility functions, like `is_empty` and `not`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5352c8-6f8a-4f83-8f59-2b8d1b97677a",
   "metadata": {},
   "source": [
    "It's debatable whether we should tokenize here, at the stage where we're working with the Dataset, or somewhere else. The dimensions you might need to consider are performance, complexity, and customizability. I'm choosing to tokenize at the Dataset-level for now. But keep in mind that it might not be a hard requirement. As we proceed, consider \"question_type\", \"confidence\"]),\n",
    "    baconsequences of doing so."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8191e62-ab94-4de3-be28-8ae47e279ee9",
   "metadata": {},
   "source": [
    "Tokenizing can be a simple utility function that we can pass to `map`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ad31d853-804f-4324-ab4e-7321a456dcf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2TokenizerFast\n",
    "text_tokenizer = GPT2TokenizerFast.from_pretrained(\"openai-community/gpt2\")\n",
    "text_tokenizer.pad_token = text_tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "73db2639-7d13-4090-aff1-f5634a10533b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ' = Valkyria Chronicles III = \\n',\n",
       " 'input_ids': [796,\n",
       "  569,\n",
       "  18354,\n",
       "  7496,\n",
       "  17740,\n",
       "  6711,\n",
       "  796,\n",
       "  220,\n",
       "  198,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256],\n",
       " 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(\n",
    "    iter(\n",
    "        wikitext_dataset[\"train\"]\n",
    "          .filter(lambda x: x[\"text\"] != '')\n",
    "          .map(lambda x: text_tokenizer(x[\"text\"], truncation=True, padding=\"max_length\", max_length=16))\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa6358c-8195-4eec-a018-6cc7df19c6c4",
   "metadata": {},
   "source": [
    "#### DataLoader\n",
    "\n",
    "Once we have the dataset, the DataLoader's job is easy. It simply grabs `batch_size` number of samples from the Dataset and \"collates\" (instead of `[{text: \"foo\"}, {text: \"bar\"}, ...]`, `{text: [\"foo\", \"bar\", ...]}`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "44c23430-5fb5-47bc-9cb8-c666ce07def0",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_dataset = (\n",
    "    load_dataset(path=\"wikitext\", name=\"wikitext-2-v1\", streaming=True)\n",
    "    .filter(mg.not_empty)\n",
    "    .map(mg.tokenize, batched=True, batch_size=1000)\n",
    ")\n",
    "text_dataloader = DataLoader(\n",
    "    text_dataset[\"train\"], batch_size=2, collate_fn=mg.collate_fn\n",
    ")\n",
    "text_batch = next(iter(text_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "111acbe2-3ffd-4728-9785-de1ad5c9dade",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 1024]),\n",
       " {'input_ids': tensor([[  796,   569, 18354,  ..., 50256, 50256, 50256],\n",
       "          [ 2311,    73, 13090,  ..., 50256, 50256, 50256]]),\n",
       "  'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "          [1, 1, 1,  ..., 0, 0, 0]])})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_batch[\"input_ids\"].shape, text_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab5e0e7-2b2d-4fa9-b5ef-197aeb43310e",
   "metadata": {},
   "source": [
    "## Images\n",
    "\n",
    "Images are first transformed into sequences of non-overlapping 16 ร 16 patches in raster order, as done in ViT (Dosovitskiy et al., 2020). Each pixel in the image patches is then normalized between [โ1, 1] and divided by the square-root of the patch size (i.e. โ16 = 4)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec9799f-84fc-45f8-8c61-414136f9447f",
   "metadata": {},
   "source": [
    "First, let's load a tiny version of a VQA dataset so that we can grab an example and verify we're patching images correctly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb8a421-aa05-4e49-8602-da4020950e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "micro_vqa = load_dataset(\"eihli/micro-ok-vqa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f12137-cb7d-42d4-8a0a-f846a5b60e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "micro_vqa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624eecfe-2999-4310-a6fe-17c7e91a9f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = micro_vqa['train'][0]['image']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fa1ccd-436a-4074-83f1-ac12ead342b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "img, img.size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5b2c2b-9899-4448-92db-6f2651e12cba",
   "metadata": {},
   "source": [
    "What does the image look like? What do we expect to see when we patch it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0dd2cba-ea98-448d-9e2a-8d517ff6a47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1050e9-91f5-4d45-b6e2-c4ae1aed8919",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_tensor = ToTensor()\n",
    "resize = Resize(256)\n",
    "random_crop = RandomCrop(256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729b98b2-2c01-4b4b-ac91-5d7035f2995a",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = micro_vqa['train'][0]['image']\n",
    "img = to_tensor(random_crop(resize(img))).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f4f5a0-dd88-4bf9-a221-f8161b525a44",
   "metadata": {},
   "source": [
    "## Converting to patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61f8466-ba05-4069-bb71-49813903091e",
   "metadata": {},
   "outputs": [],
   "source": [
    "patches = mg.images_to_patches(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53d60c7-24d4-4194-b66b-e7f846656855",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa5bdfa-4060-422f-bbf6-eebafd6f1124",
   "metadata": {},
   "outputs": [],
   "source": [
    "patches.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e16a0f-d0ad-4824-b5d0-578ea350ffd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "patches = patches.view(1, 256, 16, 16, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215e4fd5-182a-4f8c-8116-b4b89903b75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(4, 4, figsize=(10, 10))\n",
    "for row in range(4):\n",
    "    for col in range(4):\n",
    "        axes[row][col].imshow(patches[0][row * 16 + col])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0017ef-f5ff-4164-9e7c-c1533d109925",
   "metadata": {},
   "source": [
    "And then, to verify we can go both ways, let's convert the patches back to the original image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac67c5b-fb02-4659-a0c9-826fad2c6bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "patches.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8177349c-46a0-4d73-9d4c-6c2e65b8e438",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = mg.patches_to_image(patches[0], (3, 256, 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e478e1c9-3e8c-468a-a2a1-5fc7e4c635b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b78b87-5741-4eeb-96b7-608e9d2ffcc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d467dc0b-3031-4b02-bceb-da3a5ba2f9da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3085d36-48e2-412f-95c8-546c21708f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_batch = next(iter(text_dataloader))\n",
    "text_sequence, text_attention_mask, text_targets = mg.embed_and_sequence_text(text_batch)\n",
    "text_sequence.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f2fe11-ef50-4be9-92eb-e6cfa1e1687d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.concat([text_sequence])\n",
    "y = torch.concat([text_targets])\n",
    "m = torch.concat([text_attention_mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f021974-a0b6-4ecf-847d-99c0fbb311de",
   "metadata": {},
   "outputs": [],
   "source": [
    "o = model(inputs_embeds=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611ce7fa-5541-46c6-b521-ee8a3500222b",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = lm_head(o.last_hidden_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b6b2f0-d3ab-495c-83eb-00568e5bd78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d98fc82-a67f-40d3-8c1b-708a4a1b5094",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_tokens = F.softmax(p, dim=2).argmax(dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db86f14-2102-4948-affa-b6364cdd8ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_batch[\"input_ids\"][1][1:200], predicted_tokens[1][:199]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5ddf76-d49f-47d7-9c39-420c741c3bc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abcab9ee-ae4e-4be8-8497-01d073fc5578",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e975661b-deff-4720-95e7-759e934e17a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450faed4-ba55-4a78-8c4d-503f33f390dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "micro_vqa = load_dataset(\"eihli/micro-ok-vqa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3b454c-fcc6-4395-a616-4229def61068",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = micro_vqa['train'][0]['image']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493feca1-824f-4d15-a827-449f9aec5863",
   "metadata": {},
   "outputs": [],
   "source": [
    "img.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941302bc-22bc-42cb-a8d2-a80eacd06795",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_tensor = ToTensor()\n",
    "resize = Resize(256)\n",
    "random_crop = RandomCrop(256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ee4320-c5aa-494a-bcd1-ca9330ef9d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = micro_vqa['train'][0]['image']\n",
    "img = to_tensor(random_crop(resize(img))).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0828a8c1-2de2-4923-bbc8-6cf1bba6f7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "patches = mg.images_to_patches(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c4f2ef-d11f-47bc-8ca1-88c5a4a795aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(mg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de03da7-4200-4e72-ae83-0fc2855e92c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_dataset = (\n",
    "    load_dataset(path=\"wikitext\", name=\"wikitext-2-v1\", streaming=True)\n",
    "    .filter(mg.not_empty)\n",
    "    .map(mg.tokenize, batched=True, batch_size=1000)\n",
    ")\n",
    "text_dataloader = DataLoader(\n",
    "    text_dataset[\"train\"], batch_size=2, collate_fn=mg.collate_fn\n",
    ")\n",
    "text_batch = next(iter(text_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031d569f-1322-4e80-ad26-3981315636ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "vqa_dataset = load_dataset(\"eihli/micro-ok-vqa\", streaming=True).with_format(\n",
    "    \"torch\"\n",
    ")\n",
    "vqa_dataloader = DataLoader(\n",
    "    vqa_dataset[\"train\"]\n",
    "    .map(mg.vqa_img_transform)\n",
    "    .map(mg.vqa_qa_transform, batched=True, batch_size=8)\n",
    "    .map(mg.vqa_img_tokenize, batched=True, batch_size=8, remove_columns=[\"answers\", \"question\", \"answer_type\", \"question_type\", \"confidence\"]),\n",
    "    batch_size=2,\n",
    ")\n",
    "vqa_batch = next(iter(vqa_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8be7f8d-1085-444a-8330-ea185f925576",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_sequence, text_attention_mask, text_targets = mg.embed_and_sequence_text(text_batch)\n",
    "vqa_sequence, vqa_attention_mask, vqa_targets = mg.embed_and_sequence_vqa(vqa_batch)\n",
    "text_sequence.shape, vqa_sequence.shape, vqa_targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be351a6-12ef-43d6-9120-656f5369a148",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(mg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b4b0bc-85a1-439b-a14a-7c6c56430e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mg.init_model()\n",
    "lm_head = torch.nn.Linear(model.config.hidden_size, mg.text_tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9279852e-1115-4170-a138-032b1bac50d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mg.remove_embedding_layer_from_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e86f96-5940-4180-a0b5-0a4914efa242",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = mg.train(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217353d0-b914-4fd1-bd38-2ae315b5c74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = (\n",
    "    list(model.parameters())\n",
    "    + list(mg._lookup_embedding.parameters())\n",
    "    + list(mg._image_embedding.parameters())\n",
    ")\n",
    "optimizer = mg.init_optimizer(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f239523d-4bbd-4aff-af3a-a0db957bf561",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "accelerator = Accelerator()\n",
    "device = accelerator.device\n",
    "model, mg._lookup_embedding_, mg._image_embedding, lm_head, optimizer, text_dataloader, vqa_dataloader = accelerator.prepare(model, mg._lookup_embedding, mg._image_embedding, lm_head, optimizer, text_dataloader, vqa_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea16434-5e21-4116-ab5b-f3d2ce15cd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc; gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f557489-3de7-410c-8862-31d1c48c1856",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_batch = next(iter(text_dataloader))\n",
    "vqa_batch = next(iter(vqa_dataloader))\n",
    "text_sequence, text_attention_mask, text_targets = mg.embed_and_sequence_text(text_batch)\n",
    "vqa_sequence, vqa_attention_mask, vqa_targets = mg.embed_and_sequence_vqa(vqa_batch)\n",
    "x = torch.concat([text_sequence, vqa_sequence])\n",
    "y = torch.concat([text_targets, vqa_targets])\n",
    "m = torch.concat([text_attention_mask, vqa_attention_mask])\n",
    "x.device, y.device, m.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9f419d-eb6a-4242-98a6-50e46f809002",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_batch = next(iter(text_dataloader))\n",
    "vqa_batch = next(iter(vqa_dataloader))\n",
    "text_sequence, text_attention_mask, text_targets = mg.embed_and_sequence_text(text_batch)\n",
    "vqa_sequence, vqa_attention_mask, vqa_targets = mg.embed_and_sequence_vqa(vqa_batch)\n",
    "x = torch.concat([text_sequence, vqa_sequence])\n",
    "y = torch.concat([text_targets, vqa_targets])\n",
    "m = torch.concat([text_attention_mask, vqa_attention_mask])\n",
    "optimizer.zero_grad()\n",
    "o = model(inputs_embeds=x)\n",
    "p = lm_head(o.last_hidden_state)\n",
    "loss = mg.cross_entropy(p, y, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12af2774-d1ab-4145-94b3-e8775cb8ec8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c191983-446c-43ff-a947-e359bed52445",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08834f4c-4a93-47a8-8016-c04b16660d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c335a4-afbe-4cb2-a2f1-5879296c28b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = vqa_attention_mask.squeeze(-1).view(-1)\n",
    "predicted = predicted.view(B * T, C)\n",
    "target = vqa_targets.view(-1)\n",
    "losses = F.cross_entropy(predicted, target, reduction=\"none\")\n",
    "losses_masked = losses * mask\n",
    "loss = losses_masked.sum() / mask.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7bb351-088f-4d1b-94c4-b9ab5593c83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42680ff-bb16-45f0-9265-c83fdac4210a",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses.shape, losses_masked.shape, mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2976fbd5-6671-4f99-b415-20791ef3967b",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daace8ef-082b-4012-86bc-fdb1f23e1ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = mg.cross_entropy(predicted, vqa_targets, vqa_attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be796151-1b8b-413d-97b4-bd387d90145f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a30bb0d-2596-42bf-8b91-7bba0d0c67d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bfee59-48d4-4f1c-aa45-f4fe0bae2303",
   "metadata": {},
   "outputs": [],
   "source": [
    "-math.log(1/mg.text_tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659554be-d5a4-4f63-bba9-45250c6c676d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752f053c-572c-4ce3-8a52-cde32efbe90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c433306e-decf-47f4-99cf-4245a629e1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c30dad-0b01-40d8-84d1-170333cef68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vqa_sequence.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bca209f-a2de-4b00-83b6-35e6bc21e453",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = F.cross_entropy(vqa_sequence.view(-1, vqa_sequence.size(2)), torch.randn(), reduction=\"none\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e45a17-185a-4949-83b7-e40d98c669db",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses.shape, vqa_attention_mask.view(-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6246db66-f3a3-40a5-986b-28a516d8b5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_emb = mg.lookup_embedding(text_batch['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af584f65-973c-4a51-87df-da3eae6f11d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942e1822-8910-42e5-9b5d-a35018246766",
   "metadata": {},
   "outputs": [],
   "source": [
    "vqa_batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b52693-89d5-46b9-92d8-73798f6026cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "vqa_batch[\"question_input_ids\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c76123f-1437-4d0b-8997-ba4c7cd29626",
   "metadata": {},
   "outputs": [],
   "source": [
    "vqa_batch = mg.sequence_vqa(vqa_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d140fdf3-ffc5-42a0-b9ee-b1ac750a7213",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.concat([torch.zeros(8, 3), torch.ones(8, 7)], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d165b6df-46f3-4d43-8fc2-417015c2344a",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_emb = mg.image_embedding(vqa_batch['image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a768f9e-5322-48cc-8cc1-563a211f37b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_emb = mg.lookup_embedding(vqa_batch[\"question_input_ids\"])\n",
    "answer_emb = mg.lookup_embedding(vqa_batch[\"answer_input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb095f3b-ebc7-4cbb-87b8-3efbe1d2e3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "vqa_emb = torch.concat([image_emb, question_emb, answer_emb], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1260c7-762b-4407-9bc2-897a2b7bea35",
   "metadata": {},
   "outputs": [],
   "source": [
    "vqa_emb.shape, text_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2aea69d-9d90-41a1-9ff0-4bc1b0675463",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_emb.shape, vqa_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e653585-a594-4f4f-ad73-c992275bd187",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb.shape, text_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f09f2b-e9ae-46d6-ab9c-32888d47d22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "patches.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3468a577-1d87-4cf0-9d92-21496edef215",
   "metadata": {},
   "outputs": [],
   "source": [
    "patches = patches.view(1, 256, 16, 16, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a627697-8024-40dd-8849-4346a88c2950",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(4, 4, figsize=(10, 10))\n",
    "for row in range(4):\n",
    "    for col in range(4):\n",
    "        axes[row][col].imshow(patches[0][row * 16 + col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32e7835-05a6-4d63-b9c7-0fc38d3ec677",
   "metadata": {},
   "outputs": [],
   "source": [
    "patches = patches.view(1, 256, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc5e0ff-b540-4f92-97e7-82df8d4eb60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = mg.patches_to_image(patches, (3, 256, 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d243699-8143-451b-8891-104f5b43ad0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img[0].permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a157501-3ff7-4062-b0b0-b59239e74ae2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483af8df-d5da-4a1e-afc1-225be879d6c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b29fb5-04e4-4b62-bc40-61a204abb305",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Model\n",
    "\n",
    "# Initialize model\n",
    "model = GPT2Model.from_pretrained('gpt2')\n",
    "\n",
    "# Number of model parameters\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "# Data type size (float32 = 4 bytes)\n",
    "dtype_size = 4\n",
    "\n",
    "# Calculate memory for model parameters\n",
    "param_memory = num_params * dtype_size\n",
    "\n",
    "# Batch size and sequence length\n",
    "batch_size = 8\n",
    "seq_length = 1024\n",
    "\n",
    "# Hidden size from GPT-2 config\n",
    "hidden_size = model.config.hidden_size\n",
    "num_layers = model.config.n_layer\n",
    "\n",
    "# Calculate memory for activations\n",
    "activation_memory = batch_size * seq_length * hidden_size * num_layers * dtype_size\n",
    "\n",
    "# Calculate memory for gradients\n",
    "gradient_memory = num_params * dtype_size\n",
    "\n",
    "# Optimizer states (Adam)\n",
    "optimizer_memory = num_params * dtype_size * 2\n",
    "\n",
    "# Total memory estimate\n",
    "total_memory = param_memory + activation_memory + gradient_memory + optimizer_memory\n",
    "\n",
    "# Convert to MB\n",
    "total_memory_mb = total_memory / (1024 ** 2)\n",
    "\n",
    "print(f\"Estimated Total Memory: {total_memory_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a665ab-8896-41e0-a5aa-a653c9136f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "f\"{param_memory / 1e6:.2f}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b1fb88-1fab-4c96-b92d-efc66d23e568",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d270c9fd-cdf2-410a-b1c9-678a372defe0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d0fbf6-bdbf-4624-ab71-8b99f6cee8fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e87dfc-93d4-4e25-8cc7-f461356084df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "import portalocker\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce322e76-e561-4b2c-b9e7-0599323ee1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid, test = torchtext.datasets.PennTreebank('./', 'text')\n",
    "it = iter(train)\n",
    "ex = next(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb016ec5-5deb-4ffa-b764-faa8b564bcb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "owt = datasets.load_dataset('Skylion007/openwebtext', trust_remote_code=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e571e4b-91cb-4048-bbe1-63b78ec6d0ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72142908-65b9-46a8-ae3b-82c4ef129adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = datasets.Dataset.from_generator(train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
