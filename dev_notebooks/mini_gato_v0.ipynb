{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "949b7cdb-1309-4a42-af31-70ddbbaa978f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "from dataclasses import dataclass, fields\n",
    "from functools import partial\n",
    "from itertools import cycle\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pdb\n",
    "import random\n",
    "import re\n",
    "import tempfile\n",
    "from einops import rearrange\n",
    "import datasets\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import minari\n",
    "from minigrid.core import constants as mgc\n",
    "import requests\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "import torchvision.transforms.v2 as transforms\n",
    "from torchvision.transforms.functional import pil_to_tensor\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import GPT2Tokenizer, GPT2Config, GPT2Model\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7a51868-93a6-4a29-b669-b7dca65d3985",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModalityData:\n",
    "    tokens: torch.Tensor\n",
    "    targets: torch.Tensor\n",
    "    attention_mask: torch.Tensor\n",
    "    embedding: torch.Tensor = torch.tensor([])\n",
    "\n",
    "    def combine(self, other):\n",
    "        \"\"\"Concats attributes of self to attributes of other.\n",
    "\n",
    "        Example:\n",
    "        You might have some text that's part of a control task.\n",
    "        Let's say the text is \"reach the exit\", the task's goal.\n",
    "        The control task episode has 4 observations, the previous 4 frames of video.\n",
    "        Combining the first 3 observations with the last observation would look like:\n",
    "          The shape of the text tokens of the first 3 observations: (3, 5)\n",
    "          The shape of the text tokens of the last observation:     (1, 5)\n",
    "        \"\"\"\n",
    "        return type(self)(\n",
    "            tokens=torch.concat([self.tokens, other.tokens]),\n",
    "            targets=torch.concat([self.targets, other.targets]),\n",
    "            attention_mask=torch.concat([self.attention_mask, other.attention_mask]),\n",
    "            embedding=torch.concat([self.embedding, other.embedding]),\n",
    "            \n",
    "        )\n",
    "\n",
    "    def embed(self, embedder):\n",
    "        return type(self)(\n",
    "            tokens=self.tokens,\n",
    "            targets=self.targets,\n",
    "            attention_mask=self.attention_mask,\n",
    "            embedding=self.embed_fn(embedder)(self.tokens),\n",
    "        ) \n",
    "\n",
    "    def to(self, device):\n",
    "        return type(self)(\n",
    "            tokens=self.tokens.to(device),\n",
    "            targets=self.targets.to(device),\n",
    "            attention_mask=self.attention_mask.to(device),\n",
    "            embedding=self.embedding.to(device),\n",
    "        )\n",
    "        \n",
    "    @property\n",
    "    def size(self):\n",
    "        \"\"\"The number of tokens this will consume of the context window\"\"\"\n",
    "        return self.tokens.size(0) * self.tokens.size(1)\n",
    "\n",
    "class TextData(ModalityData):\n",
    "    def embed_fn(self, embedder):\n",
    "        return embedder.text\n",
    "\n",
    "class ImageData(ModalityData):\n",
    "    def embed_fn(self, embedder):\n",
    "        return embedder.image\n",
    "        \n",
    "class DiscreteData(ModalityData):\n",
    "    def embed_fn(self, embedder):\n",
    "        return embedder.discrete\n",
    "\n",
    "# There is no such thing as ContinuousData.\n",
    "# Continuous just gets binned to discrete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7503a9c7-97ae-47fe-8d43-c92b19671e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://docs.python.org/3.7/library/dataclasses.html#dataclasses.dataclass\n",
    "# The order of the fields of subclasses matters!\n",
    "@dataclass\n",
    "class Observation:\n",
    "    def __getitem__(self, i):\n",
    "        return type(self)(**{\n",
    "            field.name: type(getattr(self, field.name))(\n",
    "                tokens=getattr(self, field.name).tokens[[i]],\n",
    "                targets=getattr(self, field.name).targets[[i]],\n",
    "                attention_mask=getattr(self, field.name).attention_mask[[i]],\n",
    "            )\n",
    "            for field in fields(self)\n",
    "        })\n",
    "\n",
    "    def combine(self, other):\n",
    "        return type(self)(**{\n",
    "            field.name: getattr(self, field.name).combine(getattr(other, field.name))\n",
    "            for field in fields(self)\n",
    "        })\n",
    "\n",
    "    @property\n",
    "    def size(self):\n",
    "        return sum(getattr(self, field.name).size for field in fields(self))\n",
    "\n",
    "    @property\n",
    "    def count(self):\n",
    "        # This is typically safe. But you might need to override it.\n",
    "        # For example, the count of a RL observation might be the number\n",
    "        # of actions, not the number of observations, because the last\n",
    "        # observation is the \"terminated\" observation that we don't care about.\n",
    "        return next(getattr(self, field.name) for field in fields(self)).tokens.size(0)\n",
    "\n",
    "    def embed(self, embedder):\n",
    "        return type(self)(**{\n",
    "            field.name: getattr(self, field.name).embed(embedder)\n",
    "            for field in fields(self)\n",
    "        })\n",
    "\n",
    "    def to(self, device):\n",
    "        return type(self)(**{\n",
    "            field.name: getattr(self, field.name).to(device)\n",
    "            for field in fields(self)\n",
    "        })\n",
    "\n",
    "    def sequence(self, sequence_length):\n",
    "        xs = torch.concat([getattr(self, field.name).embedding for field in fields(self)], dim=1)\n",
    "        ys = torch.concat([getattr(self, field.name).targets for field in fields(self)], dim=1)\n",
    "        ms = torch.concat([getattr(self, field.name).attention_mask for field in fields(self)], dim=1)\n",
    "        T, S, C = xs.shape\n",
    "        xs, ys, ms = xs.reshape(T*S, C), ys.reshape(T*S), ms.reshape(T*S)\n",
    "        padding_len = sequence_length - T*S\n",
    "        xs = F.pad(xs, (0, 0, 0, padding_len), value=0)\n",
    "        ys, ms = [F.pad(x, (0, padding_len), value=0) for x in [ys, ms]]\n",
    "        return xs, ys, ms\n",
    "\n",
    "@dataclass\n",
    "class GenericTextObservation(Observation):\n",
    "    text: TextData\n",
    "\n",
    "@dataclass\n",
    "class GenericVQAObservation(Observation):\n",
    "    question: TextData\n",
    "    image: ImageData\n",
    "    answer: TextData\n",
    "\n",
    "# There is no such thing as a GenericRoboticsObservation.\n",
    "# There's too much variety in robotics datasets.\n",
    "# But here's an example of what a specific one might look like.\n",
    "# https://minigrid.farama.org/environments/minigrid/FourRoomsEnv/\n",
    "@dataclass\n",
    "class FourRoomsObservation(Observation):\n",
    "    mission: TextData\n",
    "    direction: DiscreteData    \n",
    "    image: ImageData\n",
    "    action: DiscreteData\n",
    "\n",
    "    @property\n",
    "    def count(self):\n",
    "        return self.action.tokens.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19bee90e-e4f9-44a3-ac01-29f7cb30a64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def images_to_patches(images, patch_size=16):\n",
    "    return rearrange(images, 'b c (h s1) (w s2) -> b (h w) (c s1 s2)', s1=patch_size, s2=patch_size)\n",
    "\n",
    "def normalize_to_between_minus_one_plus_one(t: torch.Tensor):\n",
    "    min_val, max_val = t.min(), t.max()\n",
    "    if min_val == max_val:\n",
    "        return torch.zeros_like(t)\n",
    "    normalized = 2 * (t - min_val) / (max_val - min_val) - 1\n",
    "    return normalized\n",
    "\n",
    "def apply_along_dimension(func, dim, tensor):\n",
    "    tensor = tensor.transpose(0, dim)\n",
    "    shape = tensor.shape\n",
    "    tensor = tensor.reshape(shape[0], -1)\n",
    "    result = torch.stack([func(tensor[:, i]) for i in range(tensor.size(1))], dim=1)\n",
    "    result = result.reshape(shape).transpose(0, dim)\n",
    "    return result\n",
    "\n",
    "def mu_law_encode(x, M=256, mu=100):\n",
    "    M = torch.tensor(M, dtype=x.dtype)\n",
    "    mu = torch.tensor(mu, dtype=x.dtype)\n",
    "    x_mu = torch.sign(x) * torch.log(torch.abs(x) * mu + 1.0)\n",
    "    x_mu = x_mu / torch.log(M * mu + 1.0)\n",
    "    return x_mu\n",
    "\n",
    "def mu_law_decode(x_mu, M=256, mu=100):\n",
    "    M = torch.tensor(M, dtype=x_mu.dtype)\n",
    "    mu = torch.tensor(mu, dtype=x_mu.dtype)\n",
    "    x = torch.sign(x_mu) * (torch.exp(torch.abs(x_mu) * torch.log(M * mu + 1.0)) - 1.0) / mu\n",
    "    return x\n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self, text_tokenizer):\n",
    "        # This is expected to be a partial with reasonable defaults around something like a HuggingFace Tokenizer.\n",
    "        self.text_tokenizer = text_tokenizer\n",
    "\n",
    "    @property\n",
    "    def bos_token(self):\n",
    "        return self.text_tokenizer.func.bos_token\n",
    "\n",
    "    @property\n",
    "    def eos_token(self):\n",
    "        return self.text_tokenizer.func.eos_token\n",
    "\n",
    "    # The difference between these two is just the defaults.\n",
    "    # Generative tokenizer has a ones attention mask.\n",
    "    # Observative tokenizer has a zeros attention mask.\n",
    "    def text(self, data, **kwargs):\n",
    "        tokenized =  self.text_tokenizer(data, **kwargs)\n",
    "        return TextData(**{\n",
    "            \"tokens\": tokenized[\"input_ids\"][:, :-1].to(torch.long),\n",
    "            \"targets\": tokenized[\"input_ids\"][:, 1:].to(torch.long),\n",
    "            \"attention_mask\": tokenized[\"attention_mask\"][:, :-1],\n",
    "        })\n",
    "\n",
    "    def text_obs(self, data, **kwargs):\n",
    "        text_data = self.text(data)\n",
    "        text_data.attention_mask = torch.zeros_like(text_data.attention_mask)\n",
    "        return text_data\n",
    "\n",
    "    def image(self, data):\n",
    "        if len(data.shape) == 3:\n",
    "          data = data.unsqueeze(0)\n",
    "        patches = images_to_patches(data, patch_size=16)\n",
    "        # Hardcoding as a reminder to do something smarter\n",
    "        SQUARE_ROOT_OF_PATCH_SIZE = 3.464\n",
    "        xs = (\n",
    "            apply_along_dimension(\n",
    "                normalize_to_between_minus_one_plus_one, 2, patches\n",
    "            )\n",
    "            / SQUARE_ROOT_OF_PATCH_SIZE\n",
    "        )\n",
    "        # We don't predict images, but we need ys\n",
    "        # becaues these image ys will be in our\n",
    "        # concatenated ys of text/image/action/etc...\n",
    "        ys = torch.zeros(xs.shape[:2]).to(torch.long)\n",
    "        ms = torch.zeros(xs.shape[:2])  # Same story as above.\n",
    "        return ImageData(tokens=xs, targets=ys, attention_mask=ms)\n",
    "\n",
    "    def discrete_obs(self, data):\n",
    "        if len(data.shape) == 0:\n",
    "            data = data.unsqueeze(0)\n",
    "        if len(data.shape) == 1:\n",
    "            data = data.unsqueeze(1)\n",
    "        # TODO: This sucks. Can simply add vocab size here. Can't for continuous.\n",
    "        xs = data[:, :-1] + self.text_tokenizer.func.vocab_size\n",
    "        ys = data[:, 1:] + self.text_tokenizer.func.vocab_size\n",
    "        ms = torch.zeros_like(ys)\n",
    "        return DiscreteData(tokens=xs, targets=ys, attention_mask=ms)\n",
    "\n",
    "    def discrete_act(self, data):\n",
    "        discrete_data = self.discrete_obs(data)\n",
    "        discrete_data.attention_mask = torch.ones_like(discrete_data.targets)\n",
    "        return discrete_data\n",
    "\n",
    "    def continuous_obs(self, data):\n",
    "        if len(data.shape) == 0:\n",
    "            data = data.unsqueeze(0)\n",
    "        if len(data.shape) == 1:\n",
    "            data = data.unsqueeze(0)\n",
    "        boa, data, eoa = data[:, [0]], data[:, 1:-1], data[:, [-1]]\n",
    "        encoded = torch.clip(mu_law_encode(data), -1, 1)\n",
    "        shifted = (encoded + 1) / 2  # Map [-1, 1] to [0, 1]\n",
    "        # 1022 because 1023 is boa/eoa\n",
    "        scaled = (shifted * 1022).long().clamp(0, 1022) + 50157\n",
    "        data = torch.concat([boa, scaled, eoa], dim=1).to(torch.long)\n",
    "        xs = data[:, :-1]\n",
    "        ys = data[:, 1:]\n",
    "        ms = torch.zeros_like(ys)\n",
    "        return DiscreteData(tokens=xs, targets=ys, attention_mask=ms)\n",
    "\n",
    "    def continuous_act(self, data):\n",
    "        continuous_data = self.continuous_obs(data)\n",
    "        continuous_data.attention_mask = torch.ones_like(continuous_data.targets)\n",
    "        return continuous_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "005471fa-69fb-4f9e-8368-2dc46d7d44e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.4943, -0.4547, -0.3874,  0.0000,  0.3874,  0.4547])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = mu_law_encode(torch.arange(-3, 3) * 0.5)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e226aee4-35de-4a49-9b63-9733c4c77adf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.5000, -1.0000, -0.5000,  0.0000,  0.5000,  1.0000])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mu_law_decode(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3649c8cc-5946-4046-92c5-32c1f210ad65",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformDataset(Dataset):\n",
    "    def __init__(self, dataset, transform):\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.transform(self.dataset[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "948b6001-ceae-4124-b2f4-bf66a59100db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some FourRooms/Minigrid-specific stuff to turn\n",
    "# a 7x7x3 non-pixel observation into an pixel/image observation.\n",
    "lut = np.zeros((256, 3), dtype=np.uint8)\n",
    "for idx, color_name in mgc.IDX_TO_COLOR.items():\n",
    "    lut[idx] = mgc.COLORS[color_name]\n",
    "\n",
    "def minigrid_to_rgb(episode):\n",
    "    \"\"\"Convert discrete \"image\" observations into actual images.\n",
    "    I'm expecting this will improve our image modality while not losing\n",
    "    much. The downside is we can fit less in our context window. Note:\n",
    "    We might need to overlay the color/type image (index 1) with the\n",
    "    state image (index 2), if we really don't want to lose any info.\"\"\"\n",
    "    # Apply lookup to second channel\n",
    "    image = lut[episode.observations['image'][:, :, :, 1]]\n",
    "    # Convert to PyTorch tensor and permute\n",
    "    image = torch.from_numpy(image).permute(0, 3, 1, 2)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40a24ce2-dcb4-42e4-9291-931f80c30da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_transform = transforms.Compose([\n",
    "    transforms.ToDtype(torch.float32, scale=True),    \n",
    "    transforms.RandomResizedCrop((192, 192), (0.6, 1.0)),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f72599f-0d98-43da-8a4e-97d57366d875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: token for discrete is 1023 because we simply add vocab size.\n",
    "# token for continuous is 51280 because we can't.\n",
    "def four_rooms_tokenizer(tokenizer, episode, boa_token_id=1023, eoa_token_id=1023):\n",
    "    # There is always 1 more observation than there are actions,\n",
    "    # the \"terminated\" observation, which we don't care about.\n",
    "    # So take up to the :-1 of everything other than the actions.\n",
    "    image = image_transform(minigrid_to_rgb(episode)[:-1])\n",
    "    image = tokenizer.image(image)\n",
    "    mission = tokenizer.text_obs(episode.observations['mission'][:-1], padding=False)\n",
    "    direction = tokenizer.discrete_obs(torch.from_numpy(episode.observations['direction'])[:-1])\n",
    "    action = tokenizer.discrete_act(torch.stack(\n",
    "        [torch.stack([torch.tensor(boa_token_id), action, torch.tensor(eoa_token_id)]) \n",
    "        for action in torch.from_numpy(episode.actions)]\n",
    "    ))\n",
    "    return FourRoomsObservation(mission=mission, image=image, direction=direction, action=action)\n",
    "    \n",
    "def vqa_tokenizer(tokenizer, sample):\n",
    "    image = tokenizer.image(image_transform(pil_to_tensor(sample[\"image\"])))\n",
    "    answer = tokenizer.text(random.choice(sample[\"answers\"])[\"answer\"], padding=False)\n",
    "    question = tokenizer.text_obs(sample[\"question\"])\n",
    "    all_observations = GenericVQAObservation(question=question, image=image, answer=answer)\n",
    "    i = random.randint(0, all_observations.count - 1)\n",
    "    observation = all_observations[i]\n",
    "    i += 1\n",
    "    while i < all_observations.count and observation.size + observation[0].size < SEQUENCE_LENGTH:\n",
    "        observation = observation.combine(all_observations[i])\n",
    "        i += 1\n",
    "    return observation\n",
    "\n",
    "def text_tokenizer(tokenizer, text, bos_token='<|endoftext|>', eos_token='<|endoftext|>'):\n",
    "    return GenericTextObservation(text=tokenizer.text(bos_token + text + eos_token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38d670a2-e681-44f0-a108-ca3190c2e8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def minigrid_collate_fn(batch):\n",
    "    \"\"\"TODO: This isn't collation.\"\"\"\n",
    "    result = []\n",
    "    for sample in batch:\n",
    "        i = random.randint(0, sample.count - 1)\n",
    "        # Starting at that index, we'll continue adding observations to our context window until\n",
    "        # we run out of space.\n",
    "        step = sample[i]\n",
    "        i += 1\n",
    "        while i < sample.count and step.size + step[0].size < SEQUENCE_LENGTH:\n",
    "            step = step.combine(sample[i])\n",
    "            i += 1\n",
    "        result.append(step)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633d613d-ef9e-4add-9b04-0fe36dde97fd",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "312f9ba5-3195-4562-9f04-3ba50d4ba393",
   "metadata": {},
   "outputs": [],
   "source": [
    "def acquire_shakespeare_dataset():\n",
    "    temp_dir = tempfile.gettempdir()\n",
    "    shakespeare_filepath = Path(temp_dir)/\"shakespeare.txt\"\n",
    "    if not os.path.exists(shakespeare_filepath):\n",
    "        data_url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
    "        with open(shakespeare_filepath, 'w', encoding='utf-8') as f:\n",
    "            f.write(requests.get(data_url).text)\n",
    "    \n",
    "    with open(shakespeare_filepath, 'r', encoding='utf-8') as f:\n",
    "        data = f.read()\n",
    "\n",
    "    # Split the dataset into each character's lines.\n",
    "    # Continue taking lines until you have at least 250 words in the sample.\n",
    "    # Add that sample to the dataset.\n",
    "    characters_lines = re.split(r\"\\n\\s*\\n\", data.strip())\n",
    "    MIN_WORDS_PER_BATCH = 250\n",
    "    sample = [characters_lines[0]]\n",
    "    num_words_in_sample = len(characters_lines[0].split())\n",
    "    text_dataset = []\n",
    "    i = 1\n",
    "    while i < len(characters_lines):\n",
    "        if num_words_in_sample > MIN_WORDS_PER_BATCH:\n",
    "            text_dataset.append(\"\\n\\n\".join(sample))\n",
    "            num_words_in_sample -= len(sample[0].split())\n",
    "            sample = sample[1:]\n",
    "        sample += [characters_lines[i]]\n",
    "        num_words_in_sample += len(characters_lines[i].split())\n",
    "        i += 1\n",
    "\n",
    "    return text_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0bc2e24a-3114-4330-aaa3-e1f3b7063c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "minigrid_dataset = minari.load_dataset('D4RL/minigrid/fourrooms-v0', download=True)\n",
    "vqa_dataset = datasets.load_dataset(\"eihli/micro-ok-vqa\")\n",
    "shakespeare_dataset = acquire_shakespeare_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf258ea-2c58-46db-8089-98694c53fa3a",
   "metadata": {},
   "source": [
    "## Transform and Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bae1599c-4eec-453b-af2d-6cffe906c216",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQUENCE_LENGTH = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "98e0883f-5c10-40c1-b766-9a0ad9a9839f",
   "metadata": {},
   "outputs": [],
   "source": [
    "__text_tokenizer = GPT2Tokenizer.from_pretrained(\"openai-community/gpt2\", clean_up_tokenization_spaces=True)\n",
    "__text_tokenizer.pad_token = __text_tokenizer.eos_token\n",
    "_text_tokenizer = partial(\n",
    "    __text_tokenizer,\n",
    "    max_length=SEQUENCE_LENGTH,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4c9b01e0-e4db-469a-9f64-cc969e27b99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(_text_tokenizer)\n",
    "four_rooms_tokenize = partial(four_rooms_tokenizer, tokenizer)\n",
    "vqa_tokenize = partial(vqa_tokenizer, tokenizer)\n",
    "text_tokenize = partial(text_tokenizer, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "630c6912-5e40-4930-b294-be6d6ca55fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 453])\n",
      "torch.Size([6, 2]) torch.Size([6, 144, 768]) torch.Size([6, 144])\n"
     ]
    }
   ],
   "source": [
    "batch = [text_tokenize(x) for x in shakespeare_dataset[:3]]\n",
    "print(batch[0].text.tokens.shape)\n",
    "batch = minigrid_collate_fn(four_rooms_tokenize(minigrid_dataset[i]) for i in range(3))\n",
    "print(batch[0].action.tokens.shape, batch[0].image.tokens.shape, batch[0].image.attention_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c054f8d1-f4ca-404b-94b8-9244969ed53f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenericTextObservation(text=TextData(tokens=tensor([[50256, 31373,    11,   995]]), targets=tensor([[31373,    11,   995, 50256]]), attention_mask=tensor([[1, 1, 1, 1]]), embedding=tensor([])))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_tokenize('hello, world')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b03852e-8e6d-43a5-b948-9026bfc67173",
   "metadata": {},
   "source": [
    "## Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7f88bdb1-9ef9-47f4-b80a-09ca2241bcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "shakespeare_dataset_xf = TransformDataset(shakespeare_dataset, text_tokenize)\n",
    "minigrid_dataset_xf = TransformDataset(minigrid_dataset, four_rooms_tokenize)\n",
    "vqa_dataset_xf = TransformDataset(vqa_dataset['train'], vqa_tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca58132-e76b-4ad2-baf5-21df9e707a91",
   "metadata": {},
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f9e6367a-96f1-4799-ab03-ab87f728de2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "619eed52-4e50-4f40-afaf-8f7e86ea0efe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 453])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shakespeare_dataloader = DataLoader(shakespeare_dataset_xf, batch_size=BATCH_SIZE, collate_fn=lambda x: x)\n",
    "shakespeare_batch = next(iter(shakespeare_dataloader))\n",
    "shakespeare_batch[0].text.tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3ead6b39-d219-44ad-a336-6b9fe2315ad9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, torch.Size([6, 2]), torch.Size([6, 144, 768]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minigrid_dataloader = DataLoader(minigrid_dataset_xf, batch_size=BATCH_SIZE, collate_fn=minigrid_collate_fn)\n",
    "minigrid_batch = next(iter(minigrid_dataloader))\n",
    "len(minigrid_batch), minigrid_batch[0].mission.tokens.shape, minigrid_batch[0].image.tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "46748389-170e-4fd0-800d-0ab4d7e2fafe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 9]), torch.Size([1, 144, 768]), torch.Size([1, 2]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vqa_dataloader = DataLoader(vqa_dataset_xf, batch_size=BATCH_SIZE, collate_fn=lambda x: x)\n",
    "vqa_batch = next(iter(vqa_dataloader))\n",
    "vqa_batch[0].question.tokens.shape, vqa_batch[0].image.tokens.shape, vqa_batch[0].answer.targets.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52aebc21-96d0-452e-8b69-7173526b9160",
   "metadata": {},
   "source": [
    "# Embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "95e806b1-9d64-4bbf-8604-ce70793e2198",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# From section 2.2 of the Gato paper:\n",
    "#\n",
    "#    Tokens belonging to image patches for any time-step are embedded using a\n",
    "#    single ResNet (He et al., 2016a) block to obtain a vector per patch. For\n",
    "#    image patch token embeddings, we also add a learnable within-image position\n",
    "#    encoding vector.\n",
    "class ResNetV2Block(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, num_groups=24):\n",
    "        super(ResNetV2Block, self).__init__()\n",
    "        self.gn1 = nn.GroupNorm(1, in_channels)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size=3,\n",
    "            stride=stride,\n",
    "            padding=1,\n",
    "            bias=False,\n",
    "        )\n",
    "        self.gn2 = nn.GroupNorm(num_groups, out_channels)\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            out_channels, in_channels, kernel_size=3, stride=1, padding=1, bias=False\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, CHW = x.shape\n",
    "        # TODO: Remove these hardcoded values.\n",
    "        out = rearrange(x, 'b t (c h w) -> (b t) c h w', c=3, h=16)\n",
    "        out = self.gn1(out)\n",
    "        out = self.gelu(out)\n",
    "        out = self.conv1(out)\n",
    "        out = self.gn2(out)\n",
    "        out = self.gelu(out)\n",
    "        out = self.conv2(out)\n",
    "        return x + rearrange(out, '(b t) c h w -> b t (c h w)', b=B, t=T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "efc91d0f-dd03-450a-8207-9d5cb47e04c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Embedder:\n",
    "    discrete_embedding: Callable\n",
    "    image_embedding: Callable\n",
    "\n",
    "    def text(self, data):\n",
    "        return self.discrete_embedding(data)\n",
    "\n",
    "    def discrete(self, data):\n",
    "        return self.discrete_embedding(data)\n",
    "\n",
    "    def image(self, data):\n",
    "        return self.image_embedding(data)\n",
    "\n",
    "@dataclass\n",
    "class MiniGatoConfig:\n",
    "    embedding_dim: int\n",
    "    sequence_length: int\n",
    "    vocab_size: int \n",
    "    transformer_config: GPT2Config\n",
    "    transformer: GPT2Model\n",
    "\n",
    "def init_default_config() -> MiniGatoConfig:\n",
    "    transformer_config = GPT2Config()\n",
    "    return MiniGatoConfig(\n",
    "        embedding_dim=768,\n",
    "        sequence_length=1024,\n",
    "        vocab_size=__text_tokenizer.vocab_size,\n",
    "        transformer_config=transformer_config,\n",
    "        transformer=GPT2Model(transformer_config),\n",
    "    )\n",
    "\n",
    "default_config = init_default_config()\n",
    "\n",
    "class MiniGato(nn.Module):\n",
    "    def __init__(self, config: MiniGatoConfig=default_config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.sequence_length = self.config.sequence_length\n",
    "        # text + discrete + continuous (text and continuous _are_ discrete when you think about it)\n",
    "        discrete_embedding = nn.Embedding(self.config.vocab_size + 1024, self.config.embedding_dim)\n",
    "        image_embedding = ResNetV2Block(3, self.config.embedding_dim)\n",
    "        self.embedder = Embedder(discrete_embedding=discrete_embedding, image_embedding=image_embedding)\n",
    "        self.transformer = self.config.transformer\n",
    "        self.lm_head = nn.Linear(self.transformer.config.hidden_size, self.config.vocab_size + 1024)     \n",
    "\n",
    "    def forward(self, batch):\n",
    "        batch = [\n",
    "            sample.embed(self.embedder).sequence(self.sequence_length) for sample in batch\n",
    "        ]\n",
    "        xs, ys, ms = map(torch.stack, zip(*batch))\n",
    "        xs, ys, ms = [x.to(device) for x in [xs, ys, ms]]\n",
    "        out = self.transformer(inputs_embeds=xs)\n",
    "        predicted = self.lm_head(out.last_hidden_state)\n",
    "        return predicted, ys, ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b901d325-a96d-463d-88f6-31702432d715",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2e40fcf0-be16-4f91-89cf-17e8e1d4fa47",
   "metadata": {},
   "outputs": [],
   "source": [
    "minigrid_iterator = iter(minigrid_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bdf45b06-7b45-4379-90c7-ccd72f8e67e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "minigrid_batch = next(minigrid_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9e420165-26b3-4144-b11b-432223186e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infinite_dataloader(fn):\n",
    "    it = iter(fn())\n",
    "    while True:\n",
    "        try:\n",
    "            yield next(it)\n",
    "        except StopIteration:\n",
    "            it = iter(fn())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "101caa7d-1002-4dc0-851c-b771020928b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loss\n",
    "##\n",
    "## See section 2.3 of the Gato paper.\n",
    "##\n",
    "##   Let b index a training batch of sequences B. We define a masking function m\n",
    "##   such that m(b, l) = 1 if the token at index l is either from text or from\n",
    "##   the logged action of an agent, and 0 otherwise. The training loss for a\n",
    "##   batch B can then be written as...\n",
    "def cross_entropy(predicted, target, mask):\n",
    "    # See: https://youtu.be/kCc8FmEb1nY?list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ&t=1553\n",
    "    B, T, C = predicted.shape\n",
    "    predicted = predicted.view(B * T, C)\n",
    "    target = target.view(-1).to(torch.long)\n",
    "    losses = F.cross_entropy(predicted, target, reduction=\"none\")\n",
    "    losses = losses * mask.squeeze(-1).view(-1)\n",
    "    loss = losses.sum() / (mask.sum() + 1e-8)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "35d28ab7-f0d6-4a22-8065-ee253676c11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniGatoTrainer:\n",
    "    def __init__(self, model, optimizer, dataloaders, scheduler=None, lr=3e-4, num_iterations=10):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.dataloaders = dataloaders\n",
    "        self.scheduler = scheduler\n",
    "        self.dl_it = cycle(dataloaders)\n",
    "        self.losses = []\n",
    "        self.num_iterations = num_iterations\n",
    "        self.lr = lr\n",
    "\n",
    "    def train(self):\n",
    "        self.model.train()\n",
    "        for i in tqdm(range(self.num_iterations)):\n",
    "            dl = next(self.dl_it)\n",
    "            batch = next(dl)\n",
    "            optimizer.zero_grad()\n",
    "            predicted, targets, attention_mask = self.model(batch)\n",
    "            loss = cross_entropy(predicted, targets, attention_mask)\n",
    "            self.losses.append(loss.item())\n",
    "            loss.backward()\n",
    "            if self.scheduler:\n",
    "                self.scheduler.step()\n",
    "            self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "68a0a785-567a-42ca-a161-30deef63b579",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6c746035-3724-4d09-a2fd-3f49d1b4d295",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = [\n",
    "    infinite_dataloader(partial(DataLoader, minigrid_dataset_xf, batch_size=BATCH_SIZE, collate_fn=minigrid_collate_fn)),\n",
    "    infinite_dataloader(partial(DataLoader, shakespeare_dataset_xf, batch_size=BATCH_SIZE, collate_fn=lambda x: x)),\n",
    "    infinite_dataloader(partial(DataLoader, vqa_dataset_xf, batch_size=BATCH_SIZE, collate_fn=lambda x: x)),\n",
    "]\n",
    "dl_it = cycle(dataloaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3be9730a-e374-48a7-a674-875e966fba24",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = init_default_config()\n",
    "model = MiniGato(config).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988eb2e1-507a-4b56-b33e-bd26d78840ae",
   "metadata": {},
   "source": [
    "# Pre-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "743f54d1-3aeb-4926-9c04-516c713c3cf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|endoftext|>'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "__text_tokenizer.bos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313dbd1a-9f9e-48f5-8e9e-94b18f66f759",
   "metadata": {},
   "source": [
    "## Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dab6c80-9730-4a29-9f48-26d8561420bb",
   "metadata": {},
   "source": [
    "### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "58426528-b3bd-493f-838b-8adc683c34ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenericTextObservation(text=TextData(tokens=tensor([[50256]]), targets=tensor([[50256]]), attention_mask=tensor([[1]]), embedding=tensor([])))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\n",
    "text_observation = text_tokenize(\"\")\n",
    "text_observation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10443e09-4083-4055-8220-185ffe83690a",
   "metadata": {},
   "source": [
    "### Embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c33d772c-aad9-48f7-aee1-708e8a2056ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 768])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_embedding = text_observation.embed(model.embedder)\n",
    "text_embedding.text.embedding.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362cb79a-460e-4b47-97fc-f8c06f26bbbe",
   "metadata": {},
   "source": [
    "### Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "87e4a2a5-d0d6-485d-bc4c-e71d8ecfcbad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1024, 768]), torch.Size([1024]), tensor(1))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_sequence_tokens, text_sequence_targets, text_sequence_attention_mask = text_embedding.sequence(model.sequence_length)\n",
    "text_sequence_tokens.shape, text_sequence_targets.shape, text_sequence_attention_mask.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f25648ef-6701-4df2-8d27-548bb0417ed7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1024, 51281])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = [text_observation]\n",
    "predicted, targets, attention_mask = model(batch)\n",
    "predicted.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ff1ec0f5-c295-43d6-8a6c-b9419555c547",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([2.4269, 2.2634, 2.2386, 2.1586, 2.1561, 2.0914, 2.0670, 2.0486, 2.0144,\n",
       "        2.0136], device='cuda:0', grad_fn=<TopkBackward0>),\n",
       "indices=tensor([14659, 36145,  3507, 31709, 19704,  9251, 26949, 49029, 38878, 47098],\n",
       "       device='cuda:0'))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted[0, text_observation.size].topk(k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "81c9394b-08ef-463b-bfcb-45376588b81a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top predicted tokens:  chains,  pedd, osition, Tele,  SY, ologists, probably, _>, lust,  ushered\n",
      "Token probabilities: 0.0002, 0.0002, 0.0002, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001\n"
     ]
    }
   ],
   "source": [
    "topk = (predicted.exp() / predicted.exp().sum(dim=2, keepdims=True))[:, [text_observation.size]].topk(k=10, dim=2)\n",
    "print(f\"Top predicted tokens: {', '.join(__text_tokenizer.decode([x]) if x < __text_tokenizer.vocab_size else f\"<{str(x.item())}>\" for x in topk.indices.flatten())}\")\n",
    "print(f\"Token probabilities: {', '.join([f'{x:.4f}' for x in topk.values.flatten().tolist()])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39dca732-d20d-4ea4-83ff-09f5132aacb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7618f18e-0d30-4a54-840c-0a03f3c30990",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EpisodeData(id=0, total_steps=19, observations={direction: ndarray of shape (20,) and dtype int64, image: ndarray of shape (20, 7, 7, 3) and dtype uint8, mission: ['reach the goal', 'reach the goal', 'reach the goal', 'reach the goal', 'reach the goal', 'reach the goal', 'reach the goal', 'reach the goal', 'reach the goal', 'reach the goal', 'reach the goal', 'reach the goal', 'reach the goal', 'reach the goal', 'reach the goal', 'reach the goal', 'reach the goal', 'reach the goal', 'reach the goal', 'reach the goal']}, actions=ndarray of shape (19,) and dtype int64, rewards=ndarray of 19 floats, terminations=ndarray of 19 bools, truncations=ndarray of 19 bools, infos=dict with the following keys: [])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = minigrid_dataset[0]\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "64e0ff11-c49b-4d52-8df0-5a2d1414c657",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DiscreteData(tokens=tensor([[51280, 50258],\n",
       "        [51280, 50258],\n",
       "        [51280, 50259],\n",
       "        [51280, 50259]]), targets=tensor([[50258, 51280],\n",
       "        [50258, 51280],\n",
       "        [50259, 51280],\n",
       "        [50259, 51280]]), attention_mask=tensor([[1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1]]), embedding=tensor([]))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's predict the action after the first 4 observations.\n",
    "fourrooms_observation = four_rooms_tokenize(sample)[:4]\n",
    "fourrooms_observation.action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dc9461ed-bd0d-46e5-964e-115f83714186",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 2])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fourrooms_observation.mission.tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "db94e0fb-9584-49b3-a449-879d4eae1b35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 2, 768])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fourrooms_embedding = fourrooms_observation.embed(model.embedder)\n",
    "fourrooms_embedding.action.embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ad0a7a7b-a91d-4d4a-be91-84e50c2244c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024, 768])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fourrooms_sequence_tokens, fourrooms_sequence_targets, fourrooms_sequence_attention_mask = fourrooms_embedding.sequence(model.sequence_length)\n",
    "fourrooms_sequence_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c59d1336-c8f4-4ab2-9132-e21c068b2fb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1024, 51281])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = [fourrooms_observation]\n",
    "predicted, targets, attention_mask = model(batch)\n",
    "predicted.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "efd55bfe-969d-4976-8e17-647e81d2f5b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(592, 2)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What's the index of the 4th action that we're trying to predict?\n",
    "fourrooms_observation.size, fourrooms_observation.action.tokens.size(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d4ad7c09-2807-4b3a-9f56-5d50e9545467",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(51280)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The first token is the \"beginning of action\" token â€“ a separator between the observations and actions.\n",
    "i = fourrooms_observation.size - fourrooms_observation.action.tokens.size(1)\n",
    "(model.embedder.discrete_embedding.weight.data @ fourrooms_sequence_tokens[i]).argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "526b26c1-c752-4dc8-badc-e0a9885f4141",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(50259)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The next token is the first one we want to predict.\n",
    "(model.embedder.discrete_embedding.weight.data @ fourrooms_sequence_tokens[i+1]).argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "456fdfc1-06a4-4a98-8247-e670c61840f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1024, 51281])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = [fourrooms_observation]\n",
    "predicted, targets, attention_mask = model(batch)\n",
    "predicted.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "da4a131d-2bea-4954-b3bf-041c5f79095c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top predicted tokens: 39384, 15581, 34170, 21831, 42588, 47151, 45489, 26300, 13914, 14287\n",
      "Token probabilities: 0.0002, 0.0002, 0.0002, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001\n"
     ]
    }
   ],
   "source": [
    "topk = (predicted.exp() / predicted.exp().sum(dim=2, keepdims=True))[:, [i]].topk(k=10, dim=2)\n",
    "print(f\"Top predicted tokens: {', '.join(f'{x}' for x in topk.indices.flatten())}\")\n",
    "print(f\"Token probabilities: {', '.join([f'{x:.4f}' for x in topk.values.flatten().tolist()])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914a5120-81c6-46ca-a3f3-db41561ba72e",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "dca0e9cf-e6fd-4e1f-bb71-14f0bc244eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_ITERATIONS = 20\n",
    "LR = 1e-3\n",
    "optimizer = torch.optim.AdamW(model.parameters())\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=NUM_ITERATIONS, eta_min=1e-5)\n",
    "trainer = MiniGatoTrainer(\n",
    "    model,\n",
    "    optimizer,\n",
    "    dataloaders,\n",
    "    num_iterations=NUM_ITERATIONS,\n",
    "    lr=LR,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1ac9f52f-6d9f-4fbe-85ac-be1338ff5839",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bcab5811afc4fdabe37f7b2c968a371",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7a3ee547-0441-4697-8ef6-5c604f5bd496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = init_default_config()\n",
    "# model = MiniGato(config).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "66cd5e7d-ac07-4ca6-afee-3124d1c491d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9.7428560256958,\n",
       " 15.470067977905273,\n",
       " 2.795074939727783,\n",
       " 7.820310592651367,\n",
       " 12.686233520507812,\n",
       " 4.088628768920898,\n",
       " 6.843344211578369,\n",
       " 9.532252311706543,\n",
       " 2.8441479206085205,\n",
       " 6.756758689880371]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.losses[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0acd56fc-67f0-4fb7-86ba-c3a71e501de9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7eddad051580>]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh8AAAGdCAYAAACyzRGfAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAABq0klEQVR4nO3deXwc9X0//tfsrXN1WbdsyfdtzFkDCSQQwCFgQgIkX0oumrQpOQhpvoTft0DTtHXSpC1Nyo/k24RA24SEpOEoJBAgYO7LxgYb35Zt2bqsa1fXnjPfP3Y/syuxknZXM7Mzs6/n46EHWF6tRl5J+97P+5IURVFAREREZBBHoS+AiIiIiguDDyIiIjIUgw8iIiIyFIMPIiIiMhSDDyIiIjIUgw8iIiIyFIMPIiIiMhSDDyIiIjKUq9AXMJ0sy+ju7kZFRQUkSSr05RAREVEWFEXB6Ogompub4XDMfrZhuuCju7sbbW1thb4MIiIiykNXVxdaW1tnvU3Owcfzzz+P733ve9i+fTt6enrw0EMP4aqrrppym7179+LWW2/Ftm3bEIvFsHr1avz3f/83Fi5cOOf9V1RUqBdfWVmZ6+URERFRAQSDQbS1tanP47PJOfgYHx/Hhg0b8LnPfQ5XX331e/7+8OHDOP/883HjjTfiW9/6FiorK7Fnzx74fL6s7l+kWiorKxl8EBERWUw2JRPSfBbLSZL0npOPT3ziE3C73fjP//zPvO4zGAzC7/cjEAgw+CAiIrKIXJ6/Ne12kWUZjz/+OJYvX45LL70U9fX1OOecc/Dwww9r+WmIiIjIwjQNPvr7+zE2NobvfOc7uOyyy/CHP/wBH/3oR3H11Vdj27ZtGT8mHA4jGAxOeSMiIiL70rTbRZZlAMCWLVvwta99DQBw2mmn4eWXX8aPfvQjXHDBBe/5mK1bt+Jb3/qWlpdBREREJqbpyUddXR1cLhdWr1495f2rVq3C8ePHM37MbbfdhkAgoL51dXVpeUlERERkMpqefHg8Hpx11lnYv3//lPcfOHAAixYtyvgxXq8XXq9Xy8sgIiIiE8s5+BgbG8OhQ4fUP3d2dmLnzp2oqanBwoUL8Y1vfAPXXXcd3v/+9+MDH/gAnnjiCfzP//wPnnvuOS2vm4iIiCwq51bb5557Dh/4wAfe8/5Pf/rTuO+++wAA9957L7Zu3YoTJ05gxYoV+Na3voUtW7Zkdf9stSUiIrKeXJ6/5zXnQw8MPoiIiKynYHM+iIiIiObC4IOIiIgMxeCDiIiIDMXgg8jmfvn6cbx8aKDQl0FEpGLwQWRjB/pG8c3fvoNbHtxV6EsxpZMjkzjUP1boyyAqOgw+iGxsX+8oAKB/NARZNlVjW8HJsoKP3/MyPvLDFxAMRQt9OURFhcEHkY2JV/WyAoxFYgW+GnM5MTyJnkAIoaiMk8OThb4coqLC4IPIxg6fSqUUAhN8dZ/u3Z7UBu2h8UgBr4So+DD4ILKxw2n1DMMTfIJNt683FXwMMvggMhSDDyKbissKjgyMq38e4cnHFHvTTz7GwgW8EqLiw+CDyKZODk8iEpPVP49MMvhIt7dnVP1/nnwQGYvBB5FNHTo1OuXPAaZdVGPhGI4PTah/ZvBBZCwGH0Q2dbh/fMqfmXZJ2Z9W7wEAQ2MMPoiMxOCDyKZEp4tDSvyZaZcUkXIR/zbsdiEyFoMPIpsSMz5WNiZWW/PkI0UUm65rrQIADI6z4JTISAw+iGxKnHycsagaABCY5Kt7QQQf5y+tBcCTDyKjMfggsqHBsTCGkycdpy+qAsCTD0GWFexPjp0/b2kdgERKKs7x80SGYfBBZEOHTyWKTVuqStBYWQKANR9C1/AExiNxeFwO9VRIUTiEjchIDD6IbEikXJbUl6Oq1A2AJx+CSLksbyiH1+VU/32YeiEyDoMPIhsSxaZLF6SCj8BkBIrC1ILodFmVLMStKfMAAAbZbktkGAYfRDaUOvkoQ1VJ4sk1GlcwHokX8rJMQZx8rGxKBB+1yeCDJx9ExmHwQWRD6ScfPrcDHlfiR32EdQ3Ylyw2XdVUASB18jHEdlsiwzD4ILKZyUgcJ0cmASRqPiRJQlUJ6z4AYDQUVceqp9IuXgAcsU5kJAYfRDbTOTAORQH8JW41pZCq+yju4EO02DZW+lCd/Ldh2oXIeAw+iGzmULLeY2ny1AOAWvdR7Ccfe6elXAAWnBIVAoMPIps5nKz3WLKgTH2fX7TbFvmUU1FsuipZbAoAteXJ4IM1H0SGYfBBZDNqp8uCcvV9rPlImN7pAqQXnBZ3YEZkJAYfRDajdrrUp4IPUd9QzDUf6WPVV2dIuzD4IDIOgw8iG4nLCjoHEqPV008+/OrJR/E+wR4fmsBEJA6vy4H22lRKqjbZ7TI8EYXM/S5EhmDwQWQjJ4cnEY7J8DgdaKspVd/PEevpY9Ur4HKmfvVVlyX+beKyUtQnQ0RGYvBBZCOi3qOjrgxOh6S+X+12KeIn10ydLgDgdTlR4XMB4KwPIqMw+CCykfSx6unUOR88+cDKxsr3/B1nfRAZi8EHkY2kj1VPJ2o+inlt/L7e97bZChyxTmQsBh9ENpI6+ZgafKg1H5PRotxsOxqKomsoMXJ+etoF4Ih1IqMx+CCykcOn3tvpAgBVpYlX9pGYjFBUNvy6Ck0sk2vy+9R/i3Rq2oVTTokMkXPw8fzzz+OKK65Ac3MzJEnCww8/PONt/+Iv/gKSJOGuu+6axyUSUTaGxiNqzcLiBVNrPso8TriSBajFOOV0X4bJpulq1CmnxfdvQ1QIOQcf4+Pj2LBhA+6+++5Zb/fQQw/h1VdfRXNzc94XR0TZEymXlqoSlHpcU/5OkqSibrd9tydzp4vAglMiY7nmvslUmzdvxubNm2e9zcmTJ/HlL38ZTz75JC6//PK8L46IsieKTafXewj+EjcGxiJFGXzM1ukCcMopkdFyDj7mIssybrjhBnzjG9/AmjVr5rx9OBxGOJyqMA8Gg1pfElFRyLRQLl2i1mEcgSJLu6SPVZ8x7ZIMPgbG2O1CZATNC06/+93vwuVy4Stf+UpWt9+6dSv8fr/61tbWpvUlERWFTAvl0hXrcrljQxOYjCbGqnfUZQ7MxIh1nnwQGUPT4GP79u3413/9V9x3332QJGnuDwBw2223IRAIqG9dXV1aXhJR0Th06r0L5dKJLo9im3IqUi4rGiumTH1NJwpOhyciRdmKTGQ0TYOPF154Af39/Vi4cCFcLhdcLheOHTuGr3/962hvb8/4MV6vF5WVlVPeiCg3oWgcJ4YTcyxmPPko0oJTtdNlhnoPIFVwGo0rCIZihlwXUTHTtObjhhtuwMUXXzzlfZdeeiluuOEGfPazn9XyUxFRms6BcSgKUOlzoa78vXMsgFTapdhqPkSny8oZOl0AwOd2otTjxEQkjqHxiDoRloj0kXPwMTY2hkOHDql/7uzsxM6dO1FTU4OFCxeitrZ2yu3dbjcaGxuxYsWK+V8tEWWkjlWvL58x5Vm0Jx+zjFVPV1vuwcTQJIbGwzPWhhCRNnJOu7z55pvYuHEjNm7cCAC45ZZbsHHjRtxxxx2aXxwRZWeuYlMA8Jem6hqKRTAUVdNRs6VdgLQR65xySqS7nE8+LrzwwpwKso4ePZrrpyCiHKWffMykGLtd9iVTLs1+H/yls6dSOGiMyDjc7UJkAzPtdEkn0i6BIup2yTblAqRmfXDEOpH+GHwQWZwsKzgywzbbdFUlyVbbIjr52DvHTpd0PPkgMg6DDyKLOzkyiXBMhsfpQFt1yYy3E2mHyWgcoWjcqMsrqGw6XQSOWCcyDoMPIosTw8Xa60rhcs78I13hdUHM2AoWQeolLis4MMdY9XRMuxAZh8EHkcUdzqLYFAAcDkmdX1EMU06PDY5jMhqHz+1Ae+3crbO15eLkg/tdiPTG4IPI4rJpsxXUEetFUPexL3nqsaJh5rHq6USr7RBbbYl0x+CDyOIO98/d6SKkBo3Z/wk2l2JTIFVwOjDO/S5EemPwQWRxcy2US1dVRGkXEXysbJy72BRI1XxEYjLGI8VRkEtUKAw+iCxsaDyidmdkMxJcpF0CRZB22duTfbEpAJR6nPC6Er8SmXoh0heDDyILE/M9mv0+lHnnHlicKji195NrYDKKkyOJseorsww+JElSUy+DLDol0hWDDyILE2PVZxsulq5YlsvtS6ZcWqpKctpQW1POWR9ERmDwQWRhuXS6AMWz32WfOt8ju3oPoVYsl2PwQaQrBh9EFiZ2umRTbAqktdraPO2Sa6eLwBHrRMZg8EFkYWraJcuTD3+RpF1SnS65BR8csU5kDAYfRBYVisbRNTwBAFhSP3enC1AcaZe4rGB/X35pF1HzMchuFyJdMfggsqjOgXEoClDpc2FBuTerj1FbbW085+Po4DhCURklbicWZTFWPV0q7cJuFyI9Mfggsii12LS+HJI09/hwIHXyMRaOIRqXdbu2QtqXnO+xvDG7serp1BHrTLsQ6YrBB5FF5TJWXahMazu16+mHqPdYnWPKBeBmWyKjMPggsqhcxqoLToeESl9iGJld6z7yLTYF2O1CZBQGH0QWdTjHThehukzUfdjzCTY14yP34EMUnE5E4pjkfhci3TD4ILIgWVZwZCD3kw/A3h0vgYn0seq5p10qvC64nYk6EY5YJ9IPgw8iCzo5MolQVIbbKaGtuiSnj/WLQWM2DD729qbGqlf6sh+rLkiSxFkfRAZg8EFkQaLTpb22DC5nbj/G6smHDQtO9+U52TRdDUesAwD+5akDOHfrM+pJEpGWGHwQWZCYbJprygVIXy5nvyfXvck223w6XQS16LSIB40pioJfvH4c3YEQXjk8WOjLIRti8EFkQWKnS67FpoC9az72JdMuK+d18sG0S3cghFOjiZqXk8M8+SDtMfggsqDUgLHcJngCaTUfNku7TB2rnn/wUVvOWR87j4+o/39yZKJwF0K2xeCDyIJEm+3SBbmnF1InH/Z6cu0cSBurXlOa9/1wxDqw68SI+v+s+SA9MPggspjh8Yj6qnzxgtxPPkTNh90mnIqUy4rGCjhyHKuejiPWp518MO1COmDwQWQxYr5Hs9+HMq8r549PFZzaK/jYq0GnC8AR67G4jHdOBtQ/d4+EIMtKAa+I7IjBB5HFiE6XJXl0ugCAv0TM+bDXk6sWnS5AquajWE8+9veNYjIaR4XXBYcEROIyBsaKNwVF+mDwQWQx8+l0AVInH8FQDHEbvaIVMz7m0+kCpHW7FGmr7c6uEQDAhrYqNFb6AAAnWPdBGmPwQWQx8z35qErbbBu0Sd3HyEQE3YEQgETNx3yIgtPRcAzhWPHtdxH1Hhva/GhJTs89wboP0hiDDyKLUdts8yg2BQCX04GKZK2IXdptRcqltTq/serpKn1uOJMFq8Pj9vj3yYU4+TitrRotVYngg0WnpDUGH0QWEorG0TWUmLuwNM+0CwD4bTblVHS6zLfYFAAcDgnVpaLotLhqHUZDURxKBrentVWpJx+c9UFayzn4eP7553HFFVegubkZkiTh4YcfVv8uGo3i1ltvxbp161BWVobm5mZ86lOfQnd3t5bXTFS0jg6OQ1aACp8LCyq8ed+P2vFim5MP7YIPIJV6GSyyuo+3TwSgKInFfAsqvGipSsxL4ckHaS3n4GN8fBwbNmzA3Xff/Z6/m5iYwI4dO3D77bdjx44d+O1vf4v9+/fjyiuv1ORiiYrd4f5Usakk5T/LospmHS/7epOTTedZ7yEU64h1NeWysAoA0k4+GHyQtnIeErB582Zs3rw549/5/X489dRTU973b//2bzj77LNx/PhxLFy4ML+rJCIAqXqPfBbKpfPbaNZHLC5jf+/8x6qnqynSEesi+NjYVgUAU2o+FEWZV8BLlE73mo9AIABJklBVVaX3pyKyPbXTZR71HoC9lssdHRxHOCaj1OPEwnmMVU9XjCPWFUVJKzatApAKPsYjcdtNxKXCyn08Yg5CoRBuvfVWfPKTn0RlZeZXJOFwGOFw6gc8GAzqeUlEljbfThfBTiPWRafLfMeqpyvGtIvYZOt0SFjb4gcAlHicqC3zYHA8ghPDk6hKFuISzZduJx/RaBTXXnstFEXBPffcM+Pttm7dCr/fr761tbXpdUlElibLimZpFzvVfGhdbAoAteWJYt5iKjgV8z1WNlbA53aq729l3QfpQJfgQwQex44dw1NPPTXjqQcA3HbbbQgEAupbV1eXHpdENtYXDOH6n7yKR3fZu6uqOzCJUFSG2ynNO73gt1G3iy7BRxGefOzsGgaQSrkIatEpO15IQ5qnXUTgcfDgQTz77LOora2d9fZerxdeb/4tg0T3v3wULx0axLvdQVyyumHKqzY7EWPV22vL4HLO73WDnWo+tO50AYoz7TK93kNQi0558kEayjn4GBsbw6FDh9Q/d3Z2YufOnaipqUFTUxM+/vGPY8eOHXjssccQj8fR29sLAKipqYHHw3whaUtRFPx+d+J7bHgiit/uOIn/dY49u6q0KjYFoOburV7zMTIRQY9GY9XT1RbZZtto2ibbjck2W4FTTkkPOb98evPNN7Fx40Zs3LgRAHDLLbdg48aNuOOOO3Dy5Ek8+uijOHHiBE477TQ0NTWpby+//LLmF0+0v28UnQPj6p9/+uIR267/VotN6+dXbAqkDRmzeM3Hu8mUS1tNCSrmOVY9nTj5CExGEY3Lmt2vWR3oG0UoKqPC58LiuqnBbUt1ctAYTz5IQzmffFx44YVQlJl/uc/2d0Ra+907iVOPc5fU4p0TARw+NY7nDvTjgysbCnxl2hMnH/MtNgWmdrvIsqJZl4jR9vWIlIt29R5A4mRIkgBFAYYnIqiv8Gl6/2ajbrJtrXrP9wLTLqQH7nYhS/v9Oz0AgI+f0YpPJtMtP3mhs5CXpJsjp7RLu/iTNR+yktjealV6FJsCgDNtv0sx1H2ITpfp9R5AquB0aDyCiYh1v1fIXBh8kGUd7BvFwf4xuJ0SLlrVgM+c2w6nQ8LLhwexpztQ6MvT1MhEBAPJts/FGgQfXpcTpZ5EYW7AwkWnarFpk3b1HoJadFoE7bYzFZsCiUBVbEFm3QdphcEHWZYoND1/aR38JW40V5Xg8nVNAICfvmiv0w9R79Hk96Hcq02Tmuh4GbZo3UcsLmN/n7Zj1dPVFEnRafom2w0Zgg8gdfpxgqkX0giDD7Ks3yVTLpuTAQcA/Nn7OgAA/7OrG33BUEGuSw/pC+W04k+mFaw666NzYByRmIwyjxNt1dqMVU+X2mxr7xHr0zfZZsKOF9Iagw+ypCOnxrCvdxQuh4RLVqeKS9e3VuHs9hpE4wruf/lo4S5QY1pNNk2XmvVhzVf2e3u1H6uerlhmfUzfZJsJt9uS1hh8kCWJlMumJbXv2TdxY/L04+evHbdNgVxqxsf822wFq+930avYVCiWWR9vJYtNN86QcgF48kHaY/BBlvT73YmUy4fTUi7CxasasKi2FIHJKP57+wmjL00XhzXsdBFSsz4YfGRSDCcfmTbZZsKTD9Iagw+ynOODE9h9MgiHhCkpF8HpkHDj+YnTj5++2Gn5oWOhaBzHhyYAaJt28avL5awZfKgzPnTodAHSlsvZOPjoDoQwMBaGK22TbSY8+SCtMfggyxGnHn+yuFZ9gpju42e0wl/ixtHBCTy9t8/Iy9PcscEJyApQ4XXNWBCYD/XkY9J6T67D4xH0BsVYdX3TLnY++VA32TZVzLoTSZx89I2GEInZf+Ir6Y/BB1mOqPfYnCHlIpR6XOqOl59YvO1WpFwW15dDkrQrrBQFp1ac87G3N5FyWVhTqlnr8XQ15UUQfMywyXa6BeVeeF0OKArQG7BPFxkVDoMPspSTI5PY2TUCSQIuXTP7CPVPb2qHyyHh9c4hvH1ixJgL1IE6Vl3Deg8g/eTDgsGHzikXIFXzMTwRQdziqbuZpOo9qme9nSRJaurlxMiE3pdFRYDBB1nKE8lTj7Paa+bct9Ho9+GKDc0ArD10TMuFculEl5AVW231LjYFoI5XVxRr/hvNJX2T7WltM9d7CGrRKes+SAMMPshSxC6XzWsbs7q9KDx97O0edFu0Ul+d8aHTyYcVW233JdMuK3Wq9wAAt9Oh7sCxY+plf+/Mm2wz4YI50hKDD7KM3kAIbx5L5KgvyzL4WNvix6bFtYjL1hw6JstKarqphp0uAFCV1u1ipW3UsbiMA32JgGy1jicfgL1nfcy2yTYTdryQlhh8kGU8uSeRcjl9YRWa/CVZf5wYuf6L149jzGIbXHuCIUxG43A5JCys0XaEuDj5iMkKxiNxTe9bT0eSY9XLvS60Vmf/fZAPO8/6yGa+RzrO+iAt6VMmbiGKoiAuK4jJCuTk/6tvaX+WZSAmy8nbJP9fxpTbKIqC5Q0VqC7zzP2JKWdil0umwWKz+cCKeiyuK8ORgXH8+s0ufPa8Dj0uTxei2LS9rgxup7avFXxuJ7wuB8IxGcPjEd26RrQm6j30Gquezs7L5XblGnww7UIassZvGw2EonGc9XdPTwkW4ooCrU+bXQ4JFyxfgCtPa8aHVjeg1FM0/8S6OjUaxutHhwBkn3IRHA4Jnzu/A3/98G7c+1InPrWpHU6dn7S0cliHserpqkrd6AuGEZiMok2Xz6A9IzpdhFrRbjtmr+AjfZPtbDtd0omTj+6RSciyonvgR/ZWNM+MDknCaI5H7pIEOCUJTkfyTZLgdCb+63BIcDkkOJJ/73JIiMRlnBiexDP7+vHMvn6UuJ24ZE0DtpzWjPctW6D5K9di8uSeXigKsKHVj9Y8Nph+7PRW/NMf9qNraBJ/2NM764wQM9FjrHq6qhIP+oJhS005NaLTRUidfNhrs63YZNtaXYK6GQb1TddY6YPTISEaV9A/Gkajf/ZuM6LZFE3w4XZK+OPXL0gFEiKYSP7/9GBCBBi5Otg3ikd3deORnd04PjSBR3Ym/r+61I0Pr2vCltNacOaiar5qyJGYappv0FDiceJP/2QRfvjHQ/jJi52WCT7UGR8aF5sKfgtOOTWi00WoKbPniPVc6z0AwOV0oLHSh5Mjkzg5MsHgg+alaIIPSZKwWKdXj+mWNVTg65eswC0fWo6dXSN4ZGc3Hnu7BwNjYfz8teP4+WvH0ez34YrTmrFlQwtWNVVoOrXSjobGI3j1SCLlkm2LbSY3bFqEH287gu3HhrHj+DBOXzj7YCUzOHwq2emi28mHtZbLDY1H0BdMnEKsbDQg7VJmz7SL2GSbS/ABJOo+To5M4sTwJM5YpP11UfFgHkAnkiRh48Jq/M2Va/DqbR/Ef954Nj5+RivKvS50B0L48bYj+PAPXsAl//I8/u2PB3F8kFMDZ/LUu72IywrWNFdiUW3+tQ/1FT5ceZp1ho4FJqIYGEs80WrdZitYbdbHvmTKZVFtKcoMKJC1Y7dLtptsM2HHC2mFwYcBXE4H3rdsAb5/zQa8+dcX457rT8dlaxrhcTpwsH8M3//DAbz/e8/io///S7jvpU6cGrVXfnm+fvdOosU21y6XTMTQsd+/04OuIXMHfKIgsLHSp1snitWmnL4r6j0MSLkAqYJTO6VdTo5MZrXJNhPO+iCtFE3axSx8bic2r2vC5nVNCExG8eSeXjy6sxsvHx7AW8dH8NbxEfztY+/ivKV12HJaCy5d04AKn7vQl10wgYkoXjo0AGB+KRdhVVMl3resDi8cHMB9Lx/F7R9ZPe/71IteY9XT+S2Wdkl1uhgUfCRrPoYnIrbp8BCnHnNtss2EJx+kFQYfBeQvcePaM9tw7Zlt6A+G8NjbPXhkVzd2dY3ghYMDeOHgAP7PQw5ctKoeV25owYUrFuT8y8Lqntrbh5isYGVjhWY1Ozee34EXDg7gV2904asXL0OlSYO7wzotlEtnteVyarGpAW22AFBdlvj3icsKgqGoelJkZTvzrPcAePJB2mHaxSTqK3343PkdeOSm8/DcX12IWz60HIsXlCEck/G7d3rxF/+1HWf//dN47chgoS/VUKldLtp1p1ywfAGW1ZdjLBzDg290aXa/WkudfOgXfIjlaQELnHxE4zIOGjRWXfC6nKhIprzsknrZldzwPNcm20xa004+rDSSn8yHwYcJtdeV4SsXLcMzt1yAx758Pr7w/sVYUOFFMBTD/7zdXejLM0wwFMULB5Mpl3XzT7kIkiSptR8/e+koYnFZs/vWkt6dLkBat4sFWm2PnBpHJG7MWPV0NeX2KTqdusm2KuePb06efExE4pZJ1ZE5MfgwMUlKFIT9fx9ehW9cugIAcKyIumL+uLcfkbiMJQvKsEzjV/9XbWxBbZkHJ0cm8fvdvZretxbCsTiODSaCD71mfABpcz4s8ESSmu9hbHu6OmjMBu22UzfZ5l5L5HM71aFkrPug+WDwYREdyV8UnQPjBb4S46TvctH6ycbnduKGTYlBBT954YjpjpCPDU5AVoByrwv1FdlNoMxHqtvF/Jtt3zVwsmm6Whu126a32OZbPCuKTk+w7oPmgcGHRSyqTYwU7x6ZRDhmnQ2k+RoLx/DcgVMAtK33SPenf7IIHpcDu04EsP3YsC6fI1/qTpf6cl1f5Yu0SyQuYzJq7u+rfQZ3ugipWR/Wb4EXwceG1qq876OVC+ZIAww+LGJBuRdlHidkBaafT6GFZ/f1IxKT0V5bqtsCsbpyL67e2AIA+MkL5ho6dkjnhXJCqccJtzMR3Jg99SJ2uhjV6SLYacR6vsPF0qnttjz5oHlg8GERkiShPZl6OTpg/+AjfZeLnq/8P5csPH3y3V61xsIM9F4oJ0iSBH9JKvViVoNjYfSPhiFJwIoGY4MPu6RdgqGo+n2V7SbbTNR22xH7/x4i/TD4sJD25GjxoyZ6ktTDZCSOZ/clUi4f1inlIixvqMAFyxdAURKdL2YhppvqWWwqVFlgudy+3kTKZVGNMWPV09ml4PTtrtw32WbSwrQLaYDBh4W01yXqPuxedLrtQD8mo3G0VpdgbYv++f0/e1/i9OPBN7tMMe9ClhUc7te/zVYQdR9m+NpnsrdAxaZAqtXW6mmX1HyPqnndDwtOSQsMPixEnHzYvd02fZeLES2V5y+tw8rGCkxE4njgjeO6f7659AZDmIzG4XJIaqGxnqww5bRQnS5AetrF2gWn+W6ynU4EHyMTUYyHY/O8KipWDD4spBjabUPROJ7Z2wdAm10u2UgfOnbfS0cRLfDQMVFsuqi2FG6n/j+iVqj5EJ0uKxuNrfcApm62NXs78kzSN9lunEe9BwBU+tyo8CVSX0y9UL5y/s32/PPP44orrkBzczMkScLDDz885e8VRcEdd9yBpqYmlJSU4OKLL8bBgwe1ut6iJtbJdwcmETJ5W2S+Xjg4gPFIHM1+37xfoeXiytOaUVfuRW8whMff7jHs82ZiVLGpYPaaj2hcVgOywpx8eJPXoWDUoq/00zfZrmnObZNtJtzxQvOVc/AxPj6ODRs24O6778749//4j/+IH/zgB/jRj36E1157DWVlZbj00ksRCoXmfbHFrq7cg3KvC4qN223FLpfL1hqTchG8Lic+LYaOvVjYoWOHDSw2BYDqUnPXfBw+NYZIXEaFwWPVhRKPE6WexELHIYsWnYpTj1VNlZospxSPwwmefFCecg4+Nm/ejL/7u7/DRz/60ff8naIouOuuu/DXf/3X2LJlC9avX4//+I//QHd393tOSCh3iXbbRA3AURvWfYRjcTwlUi4a7nLJ1vV/sgg+twO7TwbxWueQ4Z9fSM34MCb48JeaO+2iplyajB2rnk7teLFo0anYZLuhbf6nHgBPPmj+NE0od3Z2ore3FxdffLH6Pr/fj3POOQevvPJKxo8Jh8MIBoNT3mhmarutDes+Xj40iNFQDPUVXpyxMPeNm/NVU+bBx05vBVDYoWPqQjmDTj7MvlyukJ0ugtVnfaSGi2nzc9VSzXZbmh9Ng4/e3kSXQkNDw5T3NzQ0qH833datW+H3+9W3trY2LS/JdkTw0WnDWR+/U1MujXnvnZgvMXTsmX19OJJMfxgpMBnFqdFEV4Xe002FKpMvl9vbW5ix6umsPGJ9vptsM2mtTpzAnhy23wksGaPg3S633XYbAoGA+tbV1VXoSzK11JRTewUf0biMP7wrulz0HSw2myULynHRynooCnDvS8affoh6j4ZKLyp8bkM+Z5XJu13UseoF6HQRrDxifX/vKMKx/DfZZsJBYzRfmgYfjY2JPH1fX9+U9/f19al/N53X60VlZeWUN5pZR7Lmw26zPl45PIjAZBS1ZR6c3VFT0Gu5MTl07DfbT2DY4CcbsVDOqGJTwNzdLgNjYZwSY9ULGHzUJgeNWbHgVItNttOJtEv/aBiRWGFb08maNA0+Ojo60NjYiGeeeUZ9XzAYxGuvvYZNmzZp+amKll3bbcUul0vXNsJZoJSLsGlxLVY3VSIUlfGL140dOnbI4DZbAPAng49QVDbd95QoNm2vLUOpx9ix6ulqLFzzocUyuelqyzzwuR1QFKAnwNMPyl3OwcfY2Bh27tyJnTt3AkgUme7cuRPHjx+HJEm4+eab8Xd/93d49NFH8c477+BTn/oUmpubcdVVV2l86cWptsyDimS77XGbtNvG4jL+sCdxWqb3LpdsSJKEz78/OXTs5aMIx4x7QjZyrLpQ4XWpAV/AZFNOU8WmhTv1AKzd7aJH8CFJEprZ8ULzkHPw8eabb2Ljxo3YuHEjAOCWW27Bxo0bcccddwAA/vf//t/48pe/jC984Qs466yzMDY2hieeeAI+n0/bKy9SU7fb2qPu4/WjQxgcj6C61I1zFhc25SJcvq4ZDZVenBoN4392GTd07IjBMz4AsdnWnEWne3uTwUdjYdOxVu12Sd9ku0HjoX2i7oOzPigfOQcfF154IRRFec/bfffdByDxi+xv//Zv0dvbi1AohKeffhrLly/X+rqLmhp82KTj5ffJXS6XrG40ZJx4NjwuBz59bjsA4CcvGDN0LBKTcSx5mmXkyQeQ1m47Ya4nV1HbZFTb8UxSm22t1e2i1SbbTMSgMZ58UD7M8ZuectJeK7bbWj/tEpcVPLEnEXwUYrDYbP7X2QtR4nZiX+8oXj48qPvnOzY4jrisoNzrQkOltk8Uc/GbdLlcd/JVtTjiL5TatG4XK+132dk1DEDblIvAjheaDwYfFmSnQWPbjw3j1GgYlT4Xzl1SV+jLmaKq1INrzhRDx47o/vlSk03LDJ/kKU4+zDRiPRaX0RdMrGVorips2rYm2e0SjsmYiJirKHc2etR7CC08+aB5YPBhQSLtcswGaRcxWOxDqxvhcZnv2/Fz53VAkoBn95/C9mP6jlw3eqFcuioxYt1E7bZ9o2HICuBxOlBXZuxJ0HRlHqf6/WmVug8tN9lm0lKVOIE9MWL9E9hcKYqCI6fGLHUKZjbm+21Pc+qoE+22IdO1RuZClhU8sTuZcllrrpSL0F5XhktXJ67t2h+/im8/9i7GdNpsavRY9XRmnHIqUi5NVb6CTbwVJElCncU6XhKbbCOabbKdTpx89IyEEJeL60n4h388hA/+0zb8946Thb4Uy2LwYUHVpW5U+BIzD6w8bOytrhH0BkMo97pw/jJzpVzS/cPV63D5+ibEZQU/fbETF//TNvzunR7NX/UYvVAunTrl1EQ1H2rw4TdHp5xIvVhlxLrWm2yna6jwwumQEJMV9I8Wz9bySEzG/S8fBZAYjkj5YfBhQZIkqacfnRau+/h9MuVy0ap6XX45aqWmzIO7/9fpuP9zZ2NRbSl6gyH85c934DM/e0Oz1JeiKGraZWm9MTtd0qVOPszzqr57RNR7FLbYVFBHrFtkyqnYZKtHvQcAuJwONFYmAsNiqvv4474+9fTrcAH2P9kFgw+LEkWnVq37UBQFv1dTLoUfLJaNC5YvwJM3vx9fuWgZPE4Hth04hUv+5Xn84JmD8x5E1hsMYSISh8shqVNsjWTmtEuLSYIPq8360LPYVCjG7ba/eiO1f+ww6z7yxuDDokS7rVVnfbx9IoCTI5Mo9Thx4YoFhb6crPncTtzyoeV44ub34fyldQjHZPzzUwew+a4X8NKhgbzvV6RcFtaWFmTWiRmHjKXSLuYIPqw0Yn3KJlsdik2FVjForEhOPnoCk9h24BQAQJKA0VAMpyw2+8UsGHxYVLvF0y7i1OMDK82dcpnJ4gXl+M8bz8YPPrkRCyq8ODIwjut/8hq+8sBbeeW/Dxew3gNIdbuYabx6d8AcbbaClUasp2+y7dDxJK21yE4+fvPmCcgKcHZHDRbWJF4AipUIlBsGHxaVare1XsFpIuWSqPcwwy6XfEmShCs3NOOZr1+Az5zbDocEPLqrGxd9fxvuf/loTh0AhwowVj2dGSecMu2Sv7d02GSbSTHN+pBlBQ9uT6RcrjuzTX2hwLqP/DD4sCjxaqYnEMKkhYYeAcC7PUEcG5yAz+2wVMplJpU+N/7myjV45KbzsaHVj9FwDHc+ugdX3f0S3j4xktV9FGKhXDpR8zEeiZtiRfpYOKaewjSZJPiw0smH3sWmgpj1UQwnH68cGUTX0CQqvC58eF0TlixI/A5m8JEfBh8WVVXqRqVotx2y1rGf2OVy4fJ6lHkLtyZda+ta/fjtX56Hb1+1FhU+F945GcCWu1/C7Q/vnjOdcbjAJx8VPjfEUFUzpF56kk9mlT4Xyk3yPVJroVZbPceqp0s/+bB74aUoNL3ytGaUeJxpJx/W+v1rFgw+LCq93dZKY9YVRVGnmpptl4sWnA4JN/zJIvzx6xfioxtboCjAf756DBf90zY8/NbJjL+gg6Eo+kcTT2iLFxjf6QIkrrvSlxyxboIpp6l6D3OcegCpVtshk7faJjbZJn4n6B18iBksk9E4hk1UrKy1kYmIuoPqurPaAKSGAYp6LcoNgw8LS223tU7dx4G+MRwZGIfH5cAHV9YX+nJ0s6DCi3+57jT84s/OweIFZRgYC+PmX+3E9T95Te1sEcQvr/oKrxoAFIKZ2m3NVu8BpNIu45G4qScLv92V6HJpqylBrcabbKfzuZ1YUJH4HHau+3j4rZOIxGSsaqrEupbEtFhx8nFyZNJyqW8zYPBhYVZcMCdOPd6/bAEqCvhEa5Rzl9bh9199H/7qkuXwuhx4+fAgNv/r8/j+k/vVJzARjBQq5SJUmajdNn20ullU+lxwOxO5KTPXfaRSLtWGfL7UdlvrvAjKhaIo+NWbJwAA153Zqi59rCnzoDoZsB8Z4OlHrhh8WFh7XaLYy0rttmqXiw1TLjPxupz40geX4amvXYAPrFiAaFzBvz17CB/6l214dl9/aqdLgYpNhdRyOTMEH+ZLu0iShOrkv5GZUy9GDBdLJ+o+7DrrY/fJIPb2BOFxOXDVxpYpf8e6j/wx+LAw9eTDIoPGDvWP4kDfGNxOCRetaij05RhuYW0p7v3MWfjRn56OJr8PXUOT+Ox9b+A/XjkKAGr1fKGYacS6GdMuANQ0xqBJi07TN9me1qb9MrlMWqvsPevjl28cBwBctqZRDdAFUaN1hB0vOWPwYWGi4LQvGMZERJ9Nq1oSXS7nLa1TJ2oWG0mScNnaJjx9ywX4/Ps64HRImEjmi5fWVxT02kTaxQzdLt0Bc003Fcw+6+PEsL6bbDOx86yPyUgcj+7sBpAqNE3Hk4/8MfiwsKpSj/okboVhY79LTjW18mAxrZR5Xfg/l6/GY18+H5sW12JZfTk26jgGOxv+5Ku64QKffMiygp4Rc003Fcw+Yl3vTbaZtNh4xPrvd/dgNBxDW00JNi2ufc/fq8EHO15yZo4Gespbe10ZdnWN4OjAOFY1VRb6cmZ0dGAce3uCcDokfGh18aVcZrKqqRIPfOFPCn0ZAMxTcDo4HkEkLsMhAQ2V5gw+zFpwanS9B2Dv5XK/TM72uPaMtoyTYkW77ZGBMciyous0WbvhyYfFdagL5sx98iF2uZy7pBbVZZ45bk2FIGo+Cp12EfUeDZW+gizZm42adjFpwemuQgQfyZOPwGQUY2Hzp3+z1Tkwjtc7h+CQgI+f2ZrxNm3VJXA7JYSispoqpOyY6yebctZukUFjostlM1MupmWWOR+pbbbmOvUAgJpy8558GLXJdroKX2rasp3qPh58M3Hq8f7lC2asPXI5HWrhP+s+csPgw+LEN36niTte+oMhvH0iAEkCLlnDlItZ+UtEq21hn1jNON1USBWcmq/bRWyyrdR5k20mLdVix4u5T2CzFYvL+M32xGyPT2QoNE3Huo/8MPiwOCucfIhXY8vqy1Gn88RFyp/ZTj7M1mYLpI1YN+HJh9hku0HnTbaZtNqs4+XZ/adwajSM2jIPPrhy9hdMS+q5YC4fDD4sTrzC6R81b7vt7pNBADCs9Y/yIwpOR0MxxOKF22xr6rSLiQtOxSbbjQbWewhqx4tNik7FErmrT2+BxzX702Sq3ZbBRy4YfFicv9StvmI9OmDOI8893YmTjzXN5u3GIUyZvRIMFS6QFcGHmdMuo6EYIrHCBWiZiLHqGwoQfNjp5KM/GMKz+/sBZJ7tMR1nfeSHwYcNmH3S6Z7uxMnH2haefJiZy+lARXJ9fSGnnJq55sNf4oYzmdIo9DyUdIFJ4zbZZtJioymnv9lxAnFZwRmLqrMa/CemnJ4aDRe8U8xKGHzYgJh0asYdL0PjEfUX0mqefJieX9R9FOiXaDgWx6nRRDGnGWs+HA5JXSY2MGaeotO3T4wAMGaTbSZ2mXKqKAoeTKZcrjtz7lMPINHt01CZ+DfnmPXsMfiwAXHyccyEJx8i5dJeW1rQdfGUHbE4LVCgotPe5KmHz+1Q04lmY8Ypp6Lew6hNttOJQLF/NIxwzLrr5V/rHMLRwQmUeZy4fH32YwGYeskdgw8bENttzVjzwWJTa1E7XgrUbpu+zVasLjcbMwYfu5InH4VIuQCJfxOfO/F0IkbjW5E49bhiQzPKvNkPAGfRae4YfNiAmWd9qMWmLUy5WIEoOh0eL8zJh5nbbAV1s61JppxO3WRbVZBrkCTJ8nUfgckofpcchnhtFoWm6bjdNncMPmxAzPo4NRrGuMnGG6vFpjz5sISqAtd8mLnNVjDbZluxydbtlAraUaYOGrNo3ceju7oRispY3lCec7sy0y65Y/BhA/4St3oUbKaOl9FQVC2CZZutNVSViJqPAqVdAuZtsxXMNuujEJtsM7H6rA+Rcrn2zLacU35iwdyxwXFECzgjx0oYfNjEolrz1X28mzz1aPb7ClKBT7kr/MmHedtsBbONWBfBx4bWqoJeh5j1cWLYPL+DsvVudxDvnAzA7ZRw9emZl8jNpqnShxK3E9G4gq4h6339haB58BGPx3H77bejo6MDJSUlWLJkCb797W9DURStPxWl6TDhrI/dyeBjDed7WIao+SjUiHUr1HyYbcR6oes9BLXmw4JpF7FE7pLVjerJVi4cDkmt+2DqJTvZl/Nm6bvf/S7uuece3H///VizZg3efPNNfPazn4Xf78dXvvIVrT8dJZlxxwsnm1pPValYLmd88KEoiiVqPsyUdonGZewuwCbbTNRZHxZLu4SicTz01kkAuReapluyoBx7uoM4fGoMHwIXaM5F8+Dj5ZdfxpYtW3D55ZcDANrb2/HAAw/g9ddf1/pTURo17WKik489J1lsajUi7VKImo9gKIbxSGJGhKnTLuXmKTjd11O4TbbTiZOP3kAIcVlRJ8Ga3ZN7ehGYjKLZ78P5S+vyvh9ut82N5mmXc889F8888wwOHDgAANi1axdefPFFbN68WetPRWlSU07NkW+cjMRxsH8UAMeqW4lYLleIkw9x6lFb5ilo4eRcxMnHyES0oAv4AGBncr5HITbZTtdQ6YPLISEmK+gLWmfWh0i5XHNm27wCJm63zY3mJx/f/OY3EQwGsXLlSjidTsTjcfz93/89rr/++oy3D4fDCIdThVvBYFDrSyoKIu0yMBbGaCiKigJPE93XG4SsAHXlHnX0MJmfGK8emIxClhVDn9DUlEuVeVMuQGIKrCQBigIMT0SxoKJw39+F3GQ7ndMhoanKh66hSZwcmTT16ZVwfHACLx0ahCQB15yZe6FpuvR2W0VRTDskzyw0P/l48MEH8fOf/xy/+MUvsGPHDtx///34/ve/j/vvvz/j7bdu3Qq/36++tbXln3MrZpU+t1qFf2yw8KcfarFps58/hBYiWm0VJbG51UjqNlu/uZ+0nA5JPSEqdOpFbLItdL2HYLWi019vT5x6nL+0Dq3JOSX56qgrgyQlAncz1AOZnebBxze+8Q1885vfxCc+8QmsW7cON9xwA772ta9h69atGW9/2223IRAIqG9dXV1aX1LRMFPdx7ssNrUkj8uBMk8i5WH0iHUzb7OdLlV0Wrh22/RNtoVusxVaqpKDxixQdBqXFfz6zRMAgOvmUWgq+NxOtd2YdR9z0zz4mJiYgMMx9W6dTidkOXNu1Ov1orKycsob5cdMHS9ipwvrPaxH7XgxuN1WPfkwedoFAGrLCj9iXWyyXVhTapo5Oi3qrA/zBx/PHziF3mAI1aVufGi1Nt0pnHSaPc2DjyuuuAJ///d/j8cffxxHjx7FQw89hH/+53/GRz/6Ua0/FU2TmvVR2LRLJCZjf2+y2JSdLpaj7ncxuOMlFXxY5+SjkGmXXWK4mAnqPYRWC+13+VVyoulVG1vgdWlT4MwFc9nTvOD0hz/8IW6//Xb85V/+Jfr7+9Hc3Iw///M/xx133KH1p6JpzHLycbB/FJG4jAqfC2015n8ioamq0opOjWSF6aZCTXnhZ33s7EqkNje0mifAV2d9mHzK6cBYGE/v7QOgTcpFYPCRPc2Dj4qKCtx111246667tL5rmkO7Saacps/3YLGp9agj1g1Mu8RlBb3J9kwzTzcV6kwwYv3ttDZbs0jfbGvmjo/f7jiBmKxgQ1sVVjZql+pfsoDtttnibhcbaa9LFHsNjEUwGirMeGyAk02tzl9ifM1H/2hiMJXLIaHOJPULsyl02qU3EEL/aBhOR2E32U4n2qRDUbngnUAzURRFTblcd6a23ZWLkycfJ4YnEYrGNb1vu2HwYSMVPjfqygvfbivabFlsak2p5XLGPXmIeo9Gv88SkzFrygtbcLoreeqxrL4cpR7ND7Dz5nU5UZ+ce2LWuo8dx4dx+NQ4StxOXLGhSdP7riv3oNLngqIU/gTa7Bh82MyiWjHptDDf+HFZUbfZrm0xzysyyp6YYREw8OTDSvUeQPpm2wIFHybZZJtJqu7DnMHHL19PnHpcvr5J82GMkiRhSb0Ys87gYzYMPmxGrfsoUPDROTCGyWgcJW4nOurKC3INND+pkw8jgw8xYMz8bbZA4dMub59IpDbXt5nvdLHFxB0vY+EYHn+nB4C2habpWHSaHQYfNtORrPvoLNCRn5jvsbq50hLH5/ReqZoP49MuVjv5GJ6IQJYVQz+3oiipYlMTn3yYcdbHY7u6MRGJY/GCMpy5qFqXz8HgIzsMPmxGtNsWquaDxabWV5CTDwtNNwWA6mTwISvGL+E7OjiBYCgGj8uBFY0Vhn7ubIhZH2YMPn6ZVmiqVycOO16yw+DDZgqddtmd1mZL1qTO+TC05iPxRGWFNlsAcDsdqPQlCj2NbrcVpx5rmivhdprvV7ha82GytMuBvlHs7BqByyHh6tPnt0RuNuk1H0afilmJ+b5zaV7EycfgeARBg9ttFUXBbnHywWJTyxLL5UYmo1AUY355WmWjbbraAnW87FKHi1UZ+nmzpe53MdmgMdFee9Gqel03ES+sKYXLIWEyGldn19B7MfiwmXKvS52TcGzA2B/+rqFJjIZi8DgdWFZvvuNgyo44+YjLCsbC+m+2nYzEMZw8ZbFK2gUoXNGpOPlYb6LJpunEyUcwFCvovKF04Vgcv92h3RK52bidDnXJJ1MvM2PwYUOFKjoVpx4rGivgcfFby6p8bid87sTjZ8Sgse5A4tSjwutCpcatj3pKbbY1LviIxWX152y9SU8+yr0uNYA1S+rl6Xf7MTwRRUOlF+9ftkD3z6cWnXK77Yz4DGFDiwpU98FiU/uoMnDKqRVTLkBhZn0c6BtDKCqjwuvC4mSK1YzUdluTFJ3+6s1EyuWaM9rgMqBORq374HbbGTH4sKGOAi2YE8WmazjZ1PKMnHJqtTZbQT35GDOu4FSkXNa2+OEwcSu7mWZ9nBiewAsHTwEArtV4nPpM2G47NwYfNlSIBXOKomD3ycTJx1qefFiev8S45XJWm24qFCLtssvEw8XSmWnK6W+2n4CiAJsW12JhshZDb2y3nRuDDxsSxU5HDZz10RcMY3A8AqdDwqomBh9WZ+SsD6u12QqisNvItIs4+TjNpPUegngsTxT45CMuK/j1m4lC00+cbcypB5BaMNcXDJum6NZsGHzYkGi3HRqPIGDQACRR77F0QTl8bqchn5P0I2o+AgZMORUFp00WGa0uGN3tEorGsb93FACwvq3KkM+Zr1aTnHy8dGgAJ0cmUelz4dI1jYZ9Xn+JW23nLdSeLbNj8GFD5V6X+o1vVN2HWu/BlIstqCcfTLvMyOi0y7s9QcRkBXXlHtPvwFFnfRT45EMUml61scXwF0WiIJipl8wYfNhUh8F1H6nhYubORVN2/AalXRRFsWzapbY8ud9lPGLIMDaxyXZ9a5Vuo8G1Imo+To2GEYrGC3INQ+MRPLWnD4D+sz0y4Xbb2TH4sCm17sOgQWN7WGxqK0a12g6NRxCOyZAkoKHS3K/mpxMnHzFZQXBS/2Fs6iZbkw4XS1dd6kZJ8qShJ1CYKZ8PvXUSkbiMtS2VWFOAdQ/seJkdgw+bEnUfRpx8DI1H1MVgqxl82IK630XnVluRcllQ7rXcYDqvy4lyb2K/y6AB+112mXiT7XSSJBW040VRFDyYtkSuENjxMjtr/bRT1joMDD5EsWlHXRkqLDShkmZWZVCrrSg2tVq9h2BU0WkwFMWR5MAqK5x8AGkdLwXY8bLrRAD7+0bhdTlw5Wkthn9+IHXycXRgArG4XJBrMDMGHzZl5HZbUWzKUw/7qCpNLZfTk1XrPQSjik53J1MurdUl6kI7syvkdtsHk4Wmm9c2qjNrjNZSVQKvy4FIXMYJE8w7SffW8eGC1eIIDD5sStR8DE9EdV+NLopN1xYgr0r6UNMuE/putlVHq5u8e2MmRo1YF8PFrJByEQo1Yj0UjeN/dnUDAK4pUMoFABwOSZ33YabUSzAUxcd/9ArWf+sPGDBwOu90DD5sqszrQr3oM9c59aIWm7bw5MMuRPARicuYiOj3CsmqbbaCUWmXVKeLdQJ8MevD6EFjT+7pxWgohpaqEmxaXGvo557OjHUfrx4eRFxW0FJVog7KKwQGHzYmik6P6Rh8BENRdZJqISrKSR8lbic8yQVceqZeLF/zUS72u+gbfIjJpmbdZJtJoU4+frM9MdH0Y2e0Fnz/TWq7rXnabV86NAAAOG9pYQMzBh821p5Mveg5YW9vd6Leo6WqRH0VSNYnSVJq1oeOU05TS+WsnnbR7/j61GgY3YEQJAlYZ6mTj8Tvn95gyLCCy5Mjk3gx+eR6zRmthnzO2aS225rn5EP8+5y/tK6g18Hgw8baDdhuu7ubxaZ2JTpe9KoZisRk9I8mnrQte/JRlji21rPgVJx6LFlQrrb2WkF9hRdup4S4rKBv1Jjagt8ml8j9yeIatNUYs0RuNmZLu/QEJnH41DgcErBpMYMP0omYctqp44K51HAx67wio+zovVyuLxiCogAel0M9QbCaWgPSLrssNFwsncMhoclvXOpFURT8Zkci5XLNGYUrNE23uC5x8jE8ETV0AeFMXjyYOPVY11qlnmwWCoMPGzOi5kPtdGGxqe34dZ5yqqZc/D7TjwufiRHdLuomW5Mvk8tErfsY0X/Wx+udQzg2OIFyrwub1xm3RG42JR6n+m9ghtOPl9SUS2HrPQAGH7Ym2m1HJqK65O0nI3Ec6k/8QK3lThfbSZ186PPEavViU2Bqt4seLcmKoqSNVa/S/P71ZuSU018nC00vX9eEUo950lOi7uNIgYMPRVHw4qFBAMD5SxcU9FoABh+2VupxoaFSv7XO+3qDkBWgrtyrtvWSfehd82H1NlsAqE3WfETiMsbC2u93OTE8iaHxCNxOCauaKjS/f72lTj70DT7GwzH87p0eAMA1Zxa+0DRdarttYTte9veNYmAsDJ/bgdMXVRX0WgAGH7YnJp0e06HuQxSbrmmutOyxOc1MPfnQO+1i4eCjxONUF6jpkXoR+1xWNlbC6zJ2JbwWxMmH3hM+H3+nBxOROBbXleGMRdW6fq5cpbbbFvbkQ9R7nN1Ra4rvJQYfNid2vOhx8sHhYvbmV0es65R2Sav5sDI9R6xbaZNtJq0GnXz85s3UbA+zvRAyS8eLmeo9AAYftreoVr8Fcxyrbm96L5ezQ9oFSHW8DOnQ8SImm1pprHo6cfLRPTKp25j+owPjeP3oEBwS8LHTzZVyAYClyUFjx4cmEI4VZp9KJCbjtc4hAOao9wAYfNheR12i6FTrWR+RmIz9vaMAWGxqV9XJk4+ATq22dig4BfQbsR6XFexOni6ub7Pmz1iTvwSSBISism6zUMRE0/ctW4BGE56iLajwosLrgqzok/7OxlvHhzERiaO2zIOVjeaoHWLwYXPqoDGNv+kP9o8iGldQ6XOpOxzIXkTNx7AOnVLBUBSjoUSBplWnmwp6pV2OnBrDeCSOUo8Ty+rN8YSRK4/LoRaj69HxEpcV/LeY7WGyQlNBkiQsLnDdh0i5nLu0ruAj5wVdgo+TJ0/iT//0T1FbW4uSkhKsW7cOb775ph6fiuawqCYRfAQmoxjW8JfjnpOi2NRvuhwracOvY9qlJ5lyqSp1m6otMh96jVgXw8XWNvvhNMkTRj5Ex4seRacvHRpATyAEf4kbF69q0Pz+tVLouo8XTVbvAegQfAwPD+O8886D2+3G73//e7z77rv4p3/6J1RXm6sCuViUeJxorEy8stRyuy2Hi9mfOPkIx2SEotrmqtWUi9/6p2Z6jVi34ibbTFqSO170GDQmZntsOa0ZPnfhOzhmoi6YK0C7bTAUVQPZ85eZo94DADR/yfHd734XbW1t+NnPfqa+r6OjQ+tPQzlorytFbzCEowPjOH2hNkHgbrXTxdq/GGlm5V4XnI7Ebo6RiSga/dr9crdDm62g15RTdZOtBSebptNru21gIoon9/QCAD5ugiVys0kFH8affLx6eBBxWUFHXZn6WJiB5icfjz76KM4880xcc801qK+vx8aNG/Hv//7vM94+HA4jGAxOeSNtdWhc9xGXFeztSRSbrmGni21JkpTqeNG43dbq22zT6VFwGonJ6s/YBsuffOjTbvvo292IxGSsaKjAOpO/CFpan0y79I/p1vUzE1HvcZ6JUi6ADsHHkSNHcM8992DZsmV48skn8cUvfhFf+cpXcP/992e8/datW+H3+9W3tjZzLASyE7XdVqOOl86BMUxGE4VwIrAhe/LrNGjMLm22AFCjw3K5fb1BROIyqkrdWGiC7azz0arToLHfvNkFIFFoava6s4U1ZXA6JIxH4ugLGrPhV0jVexR2i+10mgcfsizj9NNPxz/8wz9g48aN+MIXvoDPf/7z+NGPfpTx9rfddhsCgYD61tXVpfUlFb12jWd97E4Wm65qqrR0IRzNTa9ZH3ZKu9SpNR/aPamIHP26FusXdOsxaOxA3yh2nQjA5ZBw1cYWze5XLx6XA4uSQaSRqZeewCQOnxqHQwI2LbZ58NHU1ITVq1dPed+qVatw/PjxjLf3er2orKyc8kbaSp9yqsWRn1rv0czHyu6q1FkfGqddAvaYbgqkTj5CURkTEW32u7ydLDa14ibb6UTaZTQUQzCkTRD76+SpxwdW1qOu3Bp7pRYXoO5DjFRf11qlnmKahebBx3nnnYf9+/dPed+BAwewaNEirT8VZUlstx0NxTCswStY0emyxuR5Vpo/PU4+ZFlBb8A+aZcyjxMeV+JXqVapFytvsp2u1ONCdfKJT4ui02hcxkNvdQMArjF5oWm6JWl1H0Yx20j1dJoHH1/72tfw6quv4h/+4R9w6NAh/OIXv8D//b//FzfddJPWn4qy5HM70ZR8hTnfHS+KomBPcqEcx6rbn1rzoeGU04GxMKJxBU6HZIttyJIkadrxMh6O4WC/PYpNBbXoVIPgY9v+UxgYC6Ou3IMPrKyf9/0ZRXS8HNFhz1YmiqLgxUODAIDzTFbvAegQfJx11ll46KGH8MADD2Dt2rX49re/jbvuugvXX3+91p+KctCuUdFp19AkRkMxeJwOLGso1+LSyMSqSpLL5TQ8+RC5/8ZKH1xOewxZ1rLjZffJAGQl8e9TX2n9tBSQ1m6rQd3Hr7cnUi5XndYCt4W+f9RBYwadfBzoG8PAWBg+t8N0m34BHeZ8AMBHPvIRfOQjH9HjrilP7XVleOXIII7Ns+hUpFxWNFZY6gef8lNdljj50LLmQ3S6NNmg3kPQcsS61TfZZtJSJQaNzS/4GBwL45m9/QCAa860Vmfk4rrEi7XuQAjj4RjKvPpO9n3h4CkAwNkdtfC6zDeAjc8eRaI9WffROc9ZH6nhYiw2LQZixPrwuHYnH3bqdBG0HLG+KzlcbIMNik0FrdIuD+/sRkxWsL7VjxUmWZCWreoyj/p9Mt/0dzbMXO8BMPgoGuqCuXl+0+/uTu10IfsT3S5a1nzYZZttOi1HrNvz5CM562MeJx+KoqhdLlYqNE1n1KTTSEzGa51DAMxZ7wEw+CgaHWnBR77ttoqiYA/HqhcV0e0S0HCzrZ2mmwq1yXbboXl2uwyPR3B8KHE6ub6lar6XZRqtGpx87OkOYl/vKDwuB67cYP7ZHpkY1fGys2sEE5E4ass8WNVozlNqBh9FYmFNKSQJGA3H8i6K6wuGMTgegdMhYaXFjjwpP1U6dLuo001tsFRO0Krg9O1kcN9RV2a6uQzzIU4+BsbCeS8pFKcel6xusOy/jVEL5l5M1nucu7QODpMOgmTwUSR8bieakpXz+U46FfUeSxeUm3qDJGlHdLtMROIIx7TZbNtjy7SLNgWndtlkO11VqRulnsTvjO48Ui/hWByP7ErO9rBYoWk6o9IuL5q83gNg8FFU2tVJp/kVnaaGi5nzGI+0V+FzQUz3Dmhw+hGKxjGQTE2YacPmfGk150PdZGuD4WLpJElK1X3kkXp5+t1+jExE0eT3mW5HSS7SZ33EZX0WzAVDUXU8v1nrPQAGH0VFBB/5ttuKnS4cLlY8HA5J7XgJaDDroyc52bTU40Rlib6thkbSIu2iKIr6pGGX4WLp5rPdVsz2uPr0Fkvvk2qpLoHH5UAkJmsycC2TVw8PIi4r6KgrQ2u1eZcSMvgoIh21qR0v+djTzWLTYqSOWNfg5CO9zdbqC9PS1Sa7XcbCsbxrGnqDIZwaDcPpkGzZTaYOGsvxSbc3EMLzBxI1DB8/w7opFwBwOiQsTr4I1Cv1IlpszzNxygVg8FFUxI6XfGo+BsfC6qvW1VwoV1T8pdpNObXjjA8AqCxxwZV8RZ7v6ceurkRwv6y+HCUe+9VUiVfhuZ58/PatE5AV4Kz2arVrz8r0rvtI1XuYN+UCMPgoKql224mc223FPpeOujKU6zyZj8wltVxu/u22qU4X+7TZAomahvmmXkS9hx022WaSz6AxRVHwmzdPAACusfiph6COWdch+OgJTOLwqXE4JGDTYgYfZBJtyXbbsXAs56p8tdiUpx5FR7TbalFwateTD2D+HS922mSbST77XXYcH8aRgXGUuJ348PomvS7NUEvqkycf/dq32754MHHqsa61yvTtyAw+iojP7VRnK+Q66XSPKDZlvUfRSZ18aBB82LDNVlAHjeUxYl2WFXWsut3abAUxaKw3GEIsLmf1Mb9Onnp8eF2TbU5cUx0v2p98mH2kejoGH0WmvS654yXH4EOcfLDTpfiIEevDmqRdksGHzdIuQNqI9TymnB4dHMdoKAavy2G5nSXZWlDuhcfpQFxW0BsMzXn7iUgMj73dAwD4uEXHqWci0t8DYxFNUpmCoih48dAgAHO32AoMPopMe7LjJZei02AoimPJhXRMuxQfraacKoqSqvmw48nHPGo+RMpldXOlbbdFOxwSmpIj9bOp+3hidy/GwjG01ZTgnI4avS/PMGVel7rRWctJpwf6xjAwFobP7cAZi6o1u1+92PO7nGakFp3msN323WSxaUtVCaqTv2CpeKg1H/NMu4xMRDGZbENttOXJR/7Bh7rJ1qb1HkIudR8i5fLx09tMOyI8X3p0vLyQHKl+dkctvC7zd0sx+Cgyi2pz324rxqrz1KM4iRHrI5PzOyIW9R515V5bjuefT8GpOPnY0GbvtGa2sz66hibwypFBSBLwsTOsuURuNnp0vFip3gNg8FF0OpI1H7lstxVttiw2LU6ian6+BaeplIv9Tj2A/NMusbisDvCza6eLkO2U0//ekTj1OHdJramndOZL646XSEzGa51DAKxR7wEw+Cg6bTWlcEjAeCS1Y2MuqcmmPPkoRlUajVdPFZvar94DyD/tcqBvDKGojAqvS51CbFfZpF1kWcFvtttrtsd0aseLRicfO7tGMBGJo7bMg1WN1vg9zeCjyHhdTrXYL5ui08lIHIf6Ez8g7HQpTqLbZTQcQzTLFslM7NxmC6RabQfHcmu1FfUe61r9tqttmC6bQWOvdg7ixPAkKrwuXLqm0ahLM5QIPo4NTSASy/9nSngxWe9x7tI6y3wPMfgoQu057HjZ2xuErCTy9PWV9jwup9lV+lLzFYLz6Hixe9pFtNoGQ7kFaXbdZJtJa1VqxPpMaV8x0fQjG5ptOWYeABoqvSjzOBGXFRwfmn/q5UWL1XsADD6KUnta3cdc9pxkyqXYuZwOVCQDkPm029p5uimQSE+JF53DOaRexE4XO26yna7R74MkAeGYjFMZTohGQ1H8bnditsc1Z9pntsd0kiSpdR+H5ln3EQxF1W3IVqn3ABh8FCVx8nEsi3bb3WKyKVMuRa1Kg6JTuwcfDkdqv0u29VShaBz7+0YBAOttutMlncflQEPFzLM+Hn+7B6GojCULyrDR5v8eWrXbvnZkCHFZQUddmaWKcxl8FKFc0i57enjyQal220Ce7baxuIy+oL3TLkDuRad7uoOIywrqyr22nPqayWwdL78WhaZntkGSrFG7kC+t2m1Fvcd5Fkq5AAw+ilJ7XWrK6WzttpGYjP29iVdla3jyUdTme/LRNxqGrABup4S6ZG2EHaVmfWRXdPq2OlzMb/snW6F1hqLTw6fGsP3YMJwOCVdvtN9sj+lSJx/zS7uk6j2sk3IBGHwUpYXJdtuJSBynRmf+JXmgbxTRuAJ/iVv9hUHFSXS85Bt8iJRLk7/EMtX4+ahNBlbZnnzYfZNtJjO124r22guWLyiK4nZR83GkfyzrmUvT9QQmcfjUOBwSsGkxgw8yOY/LoR59zjZmXcz3WNNcWTSvyiiz1Gbb/NIuqXoPez+p5Jp22dU1AgBYb/PJpukytdvGZQW/3SFme9i30DTdotrEi8DRcCxj8W02XkouklvXWqUOA7QKBh9Fqj2LMetqsSknmxa9+S6XU9tsbTpgTMhlxHpgMoojyZ8/u+90SZfp5OP5g6fQFwyjutSNi1Y1FOrSDOV1ObGwJlEgmu+kU1HvYaUWW4HBR5FSi05nGTSWfvJBxc1fMr+aD7t3ughi0NhQFt0uYmdSa3WJGrQUg0w1H2K2x5bTWuBxFc/T0uJ5dLwoioIXkycfVmqxFYrnUaYpRNHpsRmCj7is4N0ennxQglrzkffJR3EEH7mkXYplk+104ntgNBxDYDKKkYkInnq3D4C9Z3tkMp+OlwN9YxgYC8PnduCMRdVaX5ruXHPfhOxILJjrHMhc83HkVGLfRKnHaft9EzS31H6XPGs+AvZvswVy63Z5u6s4NtlOV+pxoabMg6HxCE4OT+KNo0OIxGWsbqosuq66+XS8iC6Xsztq4XVZbxIsTz6K1KLa1MlHpkrr3cmUy+qmSlt3J1B25l/zURwnH7l0uxTTWPXp0us+fr29C0DxnXoA6dttcz/5sHK9B8Dgo2i1VafabfsztNuy2JTSzWfOx3jyeB0Ammw+SEucfIxMRhGXZ26f7B8NoTsQgiQV58+YCD6e2duH3SeDcDslbDnN/rM9phMnHydHJjEZiWf9cZGYjNc6hwBYs94DYPBRtDwuhzqKN1PHC4tNKZ0/OeE0GJr9STWTnuQ220qfCxU+a7UD5qo6GaQpCjA8S4pKpFyWLihHubf4st+i3VbM9rhoZUNRFd0KNWUe9XvmyED2px87u0YwEYmjtsyDVY3W/B3N4KOILapNBh/Tik5lWcEennxQGtHtoiiJ5V+5OKlus7V3ygVILOETp0SzpV6KOeUCpE4+YslAthhTLkI+dR8i5XLu0jrLpsV1Dz6+853vQJIk3HzzzXp/KspRR53Y8TK16LRreAKj4Rg8LgeWJnOSVNw8LgfKkuvNc029FEu9h6AWnc7Sbiu2kBZbsanQkjYxeUGFFxcsX1DAqyksNfjIoe4jNVLdmvUegM7BxxtvvIEf//jHWL9+vZ6fhvI006AxUe+xsrECbicPxygh33bbYpluKojdNTN1vCiKwpOPtED06o0tcBXx75kl9bm12wZDUTV4tWq9B6Bj8DE2Nobrr78e//7v/47qauv1IBeDjrQFc+l2q/UexfmqjDLz5zlivbuI0i7A3LM+TgxPYngiCrdTwqqmCiMvzTTakvulgOJOuQC5p11eOzKEuKygvbZUrduzIt2Cj5tuugmXX345Lr74Yr0+Bc2TqPk4Njgxpd12T7eo97BmIRPpo7osv44X9eTD5qPVhZry2dMuYrjYqqZKS85n0IK/xI3vfGw9vvuxdVhaX5wBmCCCjyOnxiBnUcytttgus+6pB6DTkLFf/vKX2LFjB9544405bxsOhxEOp44ng8GgHpdEGbTVlMLpkDAZjaMvGEaj3wdFUbAnOfZ5LU8+KE1Vidhsm+PJR6C4aj5q5zj5SG2yLe6fr2vPbCv0JZhCa3UJPE4HwjEZJ0cm0VYz+2lGqt7D2sGH5icfXV1d+OpXv4qf//zn8PnmzvFu3boVfr9ffWtr4zekUdxOh7pnoTNZ99EbDGFwPAKnQ8KKxuJ+RUJT+fMYNCbLCnqKZLqpMFfaZafYZFuk9R40lcvpQHty4vSRWRZ9Aom29cOnxiFJwKbFDD6m2L59O/r7+3H66afD5XLB5XJh27Zt+MEPfgCXy4V4fOogldtuuw2BQEB96+rq0vqSaBbttVN3vIhi02X15fC5i/NImDKrymO53OB4BJGYDEkCGiqLK/jIVHAalxV1oVyx7XShmWXb8fJScpHc+ha/+mLAqjRPu1x00UV45513przvs5/9LFauXIlbb70VTufUJzSv1wuv16v1ZVCW2mtLsQ2p7bbiFyOLTWk6Mb8ikMPJh6j3aKjwFU3n1Gwj1g+fGsNEJI5Sj5Nt7KRanOWCObvUewA6BB8VFRVYu3btlPeVlZWhtrb2Pe+nwhPbbUW7LYtNaSb51HwUW5stMHvaZVcy5bK22Q+nRYdDkfZSHS8zBx+KouDF5MmHlVtsheJ4KUIzEsHHscHEoDExVp2TTWm6fGo+xDbbpiIpNgWA2mS3y/BE9D3dC28X+XAxyiybdtsDfWMYGAvD53bg9IXWH19hyFKB5557zohPQ3lQB40NjuPUaBg9yWVXq5p48kFTiZqPQA41H+Lko6WIgo/q5DC2uKwgMBlFddrOkmIfLkaZibTLqdEwApNRdaZOOtHlclZ7jS3q8XjyUeRaq0vgdEgIRWX8cV8fAKCjtqwol13R7PKZcJqa8VE8aRePy4EKX+LnZzAt9RKOxfFuTyKtyWJTSlfhc6OhMlErdGSG1Iuo93ifDeo9AAYfRc/tdKAt2W772Ns9AIA1TLlQBqLgdGQiktUwJCCVdimWGR9Cplkf+3pGEY0rqC51o62muP49aG6zpV4iMRmvdQ4BsEe9B8Dgg5Cq+3j5cKKYaW0zUy70XuIoWFaAsUgsq48ptqVyQqroNNVuK1Iu61qrIEksNqWpZis63dk1golIHLVlHqxqtMfvZwYfpNZ9xJOvZllsSpn43E743IlfGdnUfYRjcZwaTTz5Fl/wIZbLpU4+1E22RT7ZlDJbItptM8z6EPUe5y6tg8MmXVIMPgjttVPH+a7hyQfNQBRTDmfRbtubTLn43A5UW3wgUq7qMux3YbEpzWZJ/cwnH+p8j6W1hl6Tnhh8kJp2ARJdCaKwkGg6fw5TTtVttv6SokszTJ/1MR6O4VDyFS1PPigTkXY5NjiBaFxW3x8MRdVTM7vUewAMPghAR1rwweFiNJuqHGZ9FGu9B5A+Yj0RfOw+GYCsAE1+H+qLZMw85aax0odSjxMxWcHxoQn1/a8dGUJcVtBeW4rW6tmXzlkJgw9CS1UJXMk8IjfZ0mzElNNAFmmXYpxuKohBY6LgdJeacuHPF2XmcEipMetpdR8viS22NmmxFRh8UHKrYuKbfh1/OdIsUu22WZx8FGmbLZBWcJqs+RDH5qz3oNlkard9Qa33sFfwwUlSBAD49pa1eOPoEN63bEGhL4VMLJcR66kBY8UXfEyf8yGKTTlcjGYjgg8xaKwnMInDp8YhScCmxQw+yIY2LanFpiX2qaQmfaSWy7HmYzai5mN4IoKh8Qi6hhL/FjxZpNlMn/XxUnKR3PoWvxr42wXTLkSUNZF2CUzOXvOhKEpR13yI4CMaV9ScfUddWcadHUSCWvNxahyKoti23gNg8EFEOajKstU2GIphPBIHADQVYdrF53aizJNY/vXs/n4AbLGluXXUlUGSgMBkFANjEXW4mJ1abAUGH0SUtWxrPsSpR02ZByUe62/gzEdNsuNl2/5EwSCLTWkuPrcTrcldW0/s7sGp0TB8bgdOX1hd4CvTHoMPIspatjUfxZxyEaaPWN/QxpMPmpuo+7j/lWMAgLPaa+Bz2y+AZ/BBRFlLr/lQlJk326pttkWYchFExwsAOB0SVjcx+KC5ieBDTMR9nw3rPQAGH0SUAxF8ROOKWtORSTF3ugg1acHH8oaKok0/UW5E8CHYsd4DYPBBRDkocTvhcSV+bYzMMuWUaZepJx8sNqVsie22QCKAXdVoz5UXDD6IKGuSJGXV8cKTj9SIdQDY0FZVuAshSxHbbQHg3CW1cDjsuZSRwQcR5SRV9zFb8JGo+SjGNltBFJwC3OlC2ast86jzYOxa7wEw+CCiHM3V8RKXFfQGE8FHSzGffCTTLl6XA8sbKgp8NWQVkiThU5sWYUOrH5euaSz05eiG49WJKCepWR+Zaz76R0OIywpcDgkLKrwZb1MM1rRUotLnwkWrGuB28nUeZe/rl6zA1y9ZUejL0BWDDyLKyVw1HyLl0uj3wWnTfHU26it8ePOvPwS3s3j/DYhmwuCDiHIyV81HMW+znU50BhHRVPzJIKKcVJWKmo/MaRe22RLRXBh8EFFO/HOmXdhmS0SzY/BBRDmpmmO5nBit3sTgg4hmwOCDiHIiWm0Dc5x8tDDtQkQzYPBBRDmpmqPVlmkXIpoLgw8iyokIPoYnou/ZbDsZiWM4eSJSzNNNiWh2DD6IKCei2yUSkxGKylP+rjuQOPUo97pQ6WMnPxFlxuCDiHJS5nHClRweNj31kt5mK0kcrkVEmTH4IKKcSJKUqvuYVnTak5xuynoPIpoNgw8iytlMsz5OJk8+WO9BRLNh8EFEORN1H4EZ0i5ssyWi2WgefGzduhVnnXUWKioqUF9fj6uuugr79+/X+tMQUQHNtFxOFJwy7UJEs9E8+Ni2bRtuuukmvPrqq3jqqacQjUZxySWXYHx8XOtPRUQF4p9hyqmo+WDahYhmo3kv3BNPPDHlz/fddx/q6+uxfft2vP/979f60xFRAYgpp+knH4qiqDUfLTz5IKJZ6N6IHwgEAAA1NTUZ/z4cDiMcDqt/DgaDel8SEc2T6HZJr/kYGo8gHJMhSUCD31uoSyMiC9C14FSWZdx8880477zzsHbt2oy32bp1K/x+v/rW1tam5yURkQYytdr2JBfKLSj3wutyFuS6iMgadA0+brrpJuzevRu//OUvZ7zNbbfdhkAgoL51dXXpeUlEpIFMrbZqmy1TLkQ0B93SLl/60pfw2GOP4fnnn0dra+uMt/N6vfB6eURLZCWi1XZ4IpV2YZstEWVL8+BDURR8+ctfxkMPPYTnnnsOHR0dWn8KIiqwarXmI3XyoY5WZ6cLEc1B8+Djpptuwi9+8Qs88sgjqKioQG9vLwDA7/ejpIS/lIjsIFO3S3ey5oNpFyKai+Y1H/fccw8CgQAuvPBCNDU1qW+/+tWvtP5URFQgYs7HZDSOUDQOgGkXIsqeLmkXIrK3Cq8LDgmQFSA4GYXP7UzbaMuTDyKaHXe7EFHOHA4p1fEyGUU0LqN/NDGvh8EHEc2FwQcR5UV0vIxMRNEbCEFRAI/LgdoyT4GvjIjMTvcJp0RkT6lZHxE13drs90GSpEJeFhFZAIMPIspLVdpyufFIDABTLkSUHQYfRJSXquTJR2AiikhcBsBttkSUHQYfRJQXteZjMqLO+2CbLRFlgwWnRJSX9P0ubLMlolww+CCivKRvthUbbRl8EFE2GHwQUV6q09IuJ9WTD6ZdiGhuDD6IKC9ixHrX0CRGQ4luFxacElE2GHwQUV5Et8vxoYnEn0vdKPOyhp2I5sbgg4jyIrpdBJ56EFG2GHwQUV7EyYfANlsiyhaDDyLKS+W04IOdLkSULQYfRJQXp0NCpS9V48G0CxFli8EHEeUtve6DbbZElC0GH0SUNzFoDABamHYhoiwx+CCivPnT6j5Y80FE2WLwQUR5E2kXhwTUV3gLfDVEZBUMPogob6LdtrHSB5eTv06IKDv8bUFEeRM1H0y5EFEuGHwQUd5Ee217XVmBr4SIrISLGIgob1dtbEZMlvGh1Q2FvhQishAGH0SUt1KPC5/a1F7oyyAii2HahYiIiAzF4IOIiIgMxeCDiIiIDMXgg4iIiAzF4IOIiIgMxeCDiIiIDMXgg4iIiAzF4IOIiIgMxeCDiIiIDMXgg4iIiAzF4IOIiIgMxeCDiIiIDMXgg4iIiAxluq22iqIAAILBYIGvhIiIiLIlnrfF8/hsTBd8jI6OAgDa2toKfCVERESUq9HRUfj9/llvIynZhCgGkmUZ3d3dqKiogCRJmt53MBhEW1sburq6UFlZqel9m00xfa1AcX29/Frtq5i+Xn6t9qMoCkZHR9Hc3AyHY/aqDtOdfDgcDrS2tur6OSorK239DZCumL5WoLi+Xn6t9lVMXy+/VnuZ68RDYMEpERERGYrBBxERERmqqIIPr9eLO++8E16vt9CXorti+lqB4vp6+bXaVzF9vfxai5vpCk6JiIjI3orq5IOIiIgKj8EHERERGYrBBxERERmKwQcREREZynbBx91334329nb4fD6cc845eP3112e9/a9//WusXLkSPp8P69atw+9+9zuDrjR/W7duxVlnnYWKigrU19fjqquuwv79+2f9mPvuuw+SJE158/l8Bl3x/PzN3/zNe6595cqVs36MFR9XAGhvb3/P1ypJEm666aaMt7fa4/r888/jiiuuQHNzMyRJwsMPPzzl7xVFwR133IGmpiaUlJTg4osvxsGDB+e831x/7o0w29cajUZx6623Yt26dSgrK0NzczM+9alPobu7e9b7zOdnwQhzPa6f+cxn3nPdl1122Zz3a8bHFZj76830MyxJEr73ve/NeJ9mfWz1Yqvg41e/+hVuueUW3HnnndixYwc2bNiASy+9FP39/Rlv//LLL+OTn/wkbrzxRrz11lu46qqrcNVVV2H37t0GX3lutm3bhptuugmvvvoqnnrqKUSjUVxyySUYHx+f9eMqKyvR09Ojvh07dsygK56/NWvWTLn2F198ccbbWvVxBYA33nhjytf51FNPAQCuueaaGT/GSo/r+Pg4NmzYgLvvvjvj3//jP/4jfvCDH+BHP/oRXnvtNZSVleHSSy9FKBSa8T5z/bk3ymxf68TEBHbs2IHbb78dO3bswG9/+1vs378fV1555Zz3m8vPglHmelwB4LLLLpty3Q888MCs92nWxxWY++tN/zp7enpw7733QpIkfOxjH5v1fs342OpGsZGzzz5buemmm9Q/x+Nxpbm5Wdm6dWvG21977bXK5ZdfPuV955xzjvLnf/7nul6n1vr7+xUAyrZt22a8zc9+9jPF7/cbd1EauvPOO5UNGzZkfXu7PK6Koihf/epXlSVLliiyLGf8eys/rgCUhx56SP2zLMtKY2Oj8r3vfU9938jIiOL1epUHHnhgxvvJ9ee+EKZ/rZm8/vrrCgDl2LFjM94m15+FQsj0tX76059WtmzZktP9WOFxVZTsHtstW7YoH/zgB2e9jRUeWy3Z5uQjEolg+/btuPjii9X3ORwOXHzxxXjllVcyfswrr7wy5fYAcOmll854e7MKBAIAgJqamllvNzY2hkWLFqGtrQ1btmzBnj17jLg8TRw8eBDNzc1YvHgxrr/+ehw/fnzG29rlcY1EIviv//ovfO5zn5t1yaKVH9d0nZ2d6O3tnfLY+f1+nHPOOTM+dvn83JtVIBCAJEmoqqqa9Xa5/CyYyXPPPYf6+nqsWLECX/ziFzE4ODjjbe30uPb19eHxxx/HjTfeOOdtrfrY5sM2wcfAwADi8TgaGhqmvL+hoQG9vb0ZP6a3tzen25uRLMu4+eabcd5552Ht2rUz3m7FihW499578cgjj+C//uu/IMsyzj33XJw4ccLAq83POeecg/vuuw9PPPEE7rnnHnR2duJ973sfRkdHM97eDo8rADz88MMYGRnBZz7zmRlvY+XHdTrx+OTy2OXzc29GoVAIt956Kz75yU/Oungs158Fs7jsssvwH//xH3jmmWfw3e9+F9u2bcPmzZsRj8cz3t4ujysA3H///aioqMDVV1896+2s+tjmy3RbbSk3N910E3bv3j1nbnDTpk3YtGmT+udzzz0Xq1atwo9//GN8+9vf1vsy52Xz5s3q/69fvx7nnHMOFi1ahAcffDCrVxNW9dOf/hSbN29Gc3PzjLex8uNKCdFoFNdeey0URcE999wz622t+rPwiU98Qv3/devWYf369ViyZAmee+45XHTRRQW8Mv3de++9uP766+csBLfqY5sv25x81NXVwel0oq+vb8r7+/r60NjYmPFjGhsbc7q92XzpS1/CY489hmeffRatra05fazb7cbGjRtx6NAhna5OP1VVVVi+fPmM1271xxUAjh07hqeffhp/9md/ltPHWflxFY9PLo9dPj/3ZiICj2PHjuGpp57Ked36XD8LZrV48WLU1dXNeN1Wf1yFF154Afv378/55xiw7mObLdsEHx6PB2eccQaeeeYZ9X2yLOOZZ56Z8sow3aZNm6bcHgCeeuqpGW9vFoqi4Etf+hIeeugh/PGPf0RHR0fO9xGPx/HOO++gqalJhyvU19jYGA4fPjzjtVv1cU33s5/9DPX19bj88stz+jgrP64dHR1obGyc8tgFg0G89tprMz52+fzcm4UIPA4ePIinn34atbW1Od/HXD8LZnXixAkMDg7OeN1WflzT/fSnP8UZZ5yBDRs25PyxVn1ss1boilct/fKXv1S8Xq9y3333Ke+++67yhS98QamqqlJ6e3sVRVGUG264QfnmN7+p3v6ll15SXC6X8v3vf1/Zu3evcueddyput1t55513CvUlZOWLX/yi4vf7leeee07p6elR3yYmJtTbTP9av/WtbylPPvmkcvjwYWX79u3KJz7xCcXn8yl79uwpxJeQk69//evKc889p3R2diovvfSScvHFFyt1dXVKf3+/oij2eVyFeDyuLFy4ULn11lvf83dWf1xHR0eVt956S3nrrbcUAMo///M/K2+99Zba4fGd73xHqaqqUh555BHl7bffVrZs2aJ0dHQok5OT6n188IMfVH74wx+qf57r575QZvtaI5GIcuWVVyqtra3Kzp07p/wch8Nh9T6mf61z/SwUymxf6+joqPJXf/VXyiuvvKJ0dnYqTz/9tHL66acry5YtU0KhkHofVnlcFWXu72NFUZRAIKCUlpYq99xzT8b7sMpjqxdbBR+Koig//OEPlYULFyoej0c5++yzlVdffVX9uwsuuED59Kc/PeX2Dz74oLJ8+XLF4/Eoa9asUR5//HGDrzh3ADK+/exnP1NvM/1rvfnmm9V/l4aGBuXDH/6wsmPHDuMvPg/XXXed0tTUpHg8HqWlpUW57rrrlEOHDql/b5fHVXjyyScVAMr+/fvf83dWf1yfffbZjN+74muSZVm5/fbblYaGBsXr9SoXXXTRe/4dFi1apNx5551T3jfbz32hzPa1dnZ2zvhz/Oyzz6r3Mf1rnetnoVBm+1onJiaUSy65RFmwYIHidruVRYsWKZ///OffE0RY5XFVlLm/jxVFUX784x8rJSUlysjISMb7sMpjqxdJURRF16MVIiIiojS2qfkgIiIia2DwQURERIZi8EFERESGYvBBREREhmLwQURERIZi8EFERESGYvBBREREhmLwQURERIZi8EFERESGYvBBREREhmLwQURERIZi8EFERESG+n+yq0a0GNLdgwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(trainer.losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3962376d-19da-4072-bea3-f69556e0ccbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 10\n",
    "data = torch.tensor(trainer.losses)\n",
    "moving_avg = torch.conv1d(\n",
    "    data.view(1, 1, data.size(0)), \n",
    "    torch.ones(1, 1, window_size) / window_size,\n",
    ").squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "753ee6c9-94dc-422e-84c0-e8a5beeb49a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7eddad03f9b0>]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(moving_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5814617-ce4f-426b-ba13-049e7ffe536b",
   "metadata": {},
   "source": [
    "# Post-Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7b499dfd-3c62-44e8-9224-10bce5abade0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EpisodeData(id=0, total_steps=19, observations={direction: ndarray of shape (20,) and dtype int64, image: ndarray of shape (20, 7, 7, 3) and dtype uint8, mission: ['reach the goal', 'reach the goal', 'reach the goal', 'reach the goal', 'reach the goal', 'reach the goal', 'reach the goal', 'reach the goal', 'reach the goal', 'reach the goal', 'reach the goal', 'reach the goal', 'reach the goal', 'reach the goal', 'reach the goal', 'reach the goal', 'reach the goal', 'reach the goal', 'reach the goal', 'reach the goal']}, actions=ndarray of shape (19,) and dtype int64, rewards=ndarray of 19 floats, terminations=ndarray of 19 bools, truncations=ndarray of 19 bools, infos=dict with the following keys: [])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = minigrid_dataset[0]\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3eb77588-2f24-4d1f-8a28-fadf3623c6ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DiscreteData(tokens=tensor([[51280, 50258],\n",
       "        [51280, 50258],\n",
       "        [51280, 50259],\n",
       "        [51280, 50259]]), targets=tensor([[50258, 51280],\n",
       "        [50258, 51280],\n",
       "        [50259, 51280],\n",
       "        [50259, 51280]]), attention_mask=tensor([[1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1]]), embedding=tensor([]))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's predict the action after the first 4 observations.\n",
    "fourrooms_observation = four_rooms_tokenize(sample)[:4]\n",
    "fourrooms_observation.action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "27d5f8d5-e182-4712-bf12-d9063564881f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 2])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fourrooms_observation.mission.tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fe851d87-a802-4063-b4da-1565b61178d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 2, 768])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fourrooms_embedding = fourrooms_observation.embed(model.embedder)\n",
    "fourrooms_embedding.action.embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "03ba44a1-0905-42e8-8803-1e67f015cbd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024, 768])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fourrooms_sequence_tokens, fourrooms_sequence_targets, fourrooms_sequence_attention_mask = fourrooms_embedding.sequence(model.sequence_length)\n",
    "fourrooms_sequence_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9a4bbb24-4600-46cd-a35d-e6e8e7f5c8b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1024, 51281])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = [fourrooms_observation]\n",
    "predicted, targets, attention_mask = model(batch)\n",
    "predicted.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5912d274-2ee5-4b29-94b1-2ba40a943a56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(592, 2)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What's the index of the 4th action that we're trying to predict?\n",
    "fourrooms_observation.size, fourrooms_observation.action.tokens.size(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "732aba8c-dfb7-43c7-ab45-998f4452f293",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(51280)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The first token is the \"beginning of action\" token â€“ a separator between the observations and actions.\n",
    "i = fourrooms_observation.size - fourrooms_observation.action.tokens.size(1)\n",
    "(model.embedder.discrete_embedding.weight.data @ fourrooms_sequence_tokens[i]).argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a909f9ab-11e3-4770-ac21-dd563885e413",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(50259)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The next token is the first one we want to predict.\n",
    "(model.embedder.discrete_embedding.weight.data @ fourrooms_sequence_tokens[i+1]).argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b456249b-679b-4b1e-8268-1a4f5b22b953",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1024, 51281])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = [fourrooms_observation]\n",
    "predicted, targets, attention_mask = model(batch)\n",
    "predicted.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4710e558-fb8e-479a-b2b3-6fe9c81abcb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top predicted tokens: 50259, 198, 50258, 37850, 514, 25, 340, 284, 374, 11\n",
      "Token probabilities: 0.9194, 0.0029, 0.0024, 0.0023, 0.0015, 0.0014, 0.0010, 0.0009, 0.0009, 0.0008\n"
     ]
    }
   ],
   "source": [
    "topk = (predicted.exp() / predicted.exp().sum(dim=2, keepdims=True))[:, [i]].topk(k=10, dim=2)\n",
    "print(f\"Top predicted tokens: {', '.join(f'{x}' for x in topk.indices.flatten())}\")\n",
    "print(f\"Token probabilities: {', '.join([f'{x:.4f}' for x in topk.values.flatten().tolist()])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "fe29e5e4-81fe-4120-af4e-2b1c0269bbe6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([1, 1, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]),\n",
       " array([1, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2]),\n",
       " array([1, 1, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 1, 2,\n",
       "        2, 2, 2, 2, 2, 2]),\n",
       " array([0, 2, 0, 2, 2, 2, 2, 2, 2, 2, 1, 2, 0, 2]),\n",
       " array([1, 1, 2, 2, 0, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# It _should_ be predicting 1, but it's predicting 2 (50259 - 50257).\n",
    "# 2 is such a common action that it can get a low loss by just predicting 2 for everything.\n",
    "# I would expect with more training, the probability for 1 will go up, \n",
    "# even if the `topk` still predicts 2 for a long time to come.\n",
    "[minigrid_dataset[i].actions for i in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "fa98fbbf-8642-40d5-9873-4a0b660c6cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target token: tensor([[51280, 50259],\n",
      "        [51280, 50259],\n",
      "        [51280, 50259],\n",
      "        [51280, 50257]])\n",
      "Top predicted tokens: 50259, 37850, 198, 50258, 514, 25, 340, 284, 4910, 374\n",
      "Token probabilities: 0.9090, 0.0030, 0.0027, 0.0026, 0.0018, 0.0017, 0.0011, 0.0010, 0.0009, 0.0009\n"
     ]
    }
   ],
   "source": [
    "sample = minigrid_dataset[100]\n",
    "fourrooms_observation = four_rooms_tokenize(sample)[:4]\n",
    "batch = [fourrooms_observation]\n",
    "predicted, targets, attention_mask = model(batch)\n",
    "topk = (predicted.exp() / predicted.exp().sum(dim=2, keepdims=True))[:, [i]].topk(k=10, dim=2)\n",
    "print(f\"Target token: {fourrooms_observation.action.tokens}\")\n",
    "print(f\"Top predicted tokens: {', '.join(f'{x}' for x in topk.indices.flatten())}\")\n",
    "print(f\"Token probabilities: {', '.join([f'{x:.4f}' for x in topk.values.flatten().tolist()])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c192ada-dfb9-4e91-a38e-9bc499bef604",
   "metadata": {},
   "source": [
    "```\n",
    "Target token: tensor([[51280, 50259],\n",
    "        [51280, 50259],\n",
    "        [51280, 50259],\n",
    "        [51280, 50257]])\n",
    "Top predicted tokens: 50259, 50257, 50258, 44558, 42175, 53, 2879, 3539, 321, 45\n",
    "Token probabilities: 0.8277, 0.0258, 0.0069, 0.0056, 0.0041, 0.0037, 0.0032, 0.0022, 0.0020, 0.0019\n",
    "```\n",
    "\n",
    "If you run a few more training iterations, you should see 0.0258 go up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912def86-6f79-42e9-9858-65a68e6c38a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b0cc6863-b6f7-44ff-b109-15114c56b8ab",
   "metadata": {},
   "source": [
    "# More Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1a1c7f23-ea4a-45c9-bc97-b0b6f924fa26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# minigrid_dataset = minari.load_dataset('D4RL/minigrid/fourrooms-v0', download=True)\n",
    "# vqa_dataset = datasets.load_dataset(\"eihli/micro-ok-vqa\")\n",
    "# shakespeare_dataset = acquire_shakespeare_dataset()\n",
    "pointmaze_dataset = minari.load_dataset('D4RL/pointmaze/open-v2', download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d0f0d1-574b-4d14-81be-b66443c197e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "205c55dc-e65c-4e5e-952a-14bffebd3006",
   "metadata": {},
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e4bd03b0-80ee-4e0e-8960-0527a17f3fcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 453])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shakespeare_dataloader = DataLoader(shakespeare_dataset_xf, batch_size=BATCH_SIZE, collate_fn=lambda x: x)\n",
    "shakespeare_batch = next(iter(shakespeare_dataloader))\n",
    "shakespeare_batch[0].text.tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b71bfe-6e33-403a-9874-d93046cda5ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6cf42ff1-cd56-4498-b1b6-277d274bb1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "episode = pointmaze_dataset[0]\n",
    "boa_token_id, eoa_token_id = torch.tensor(51280), torch.tensor(51280)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2a2f7143-13dc-4ac3-b49b-a6c680e5fffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "observation = tokenizer.continuous_obs(torch.from_numpy(episode.observations[\"observation\"]))\n",
    "goal = tokenizer.continuous_obs(torch.from_numpy(episode.observations[\"desired_goal\"]))\n",
    "action = tokenizer.continuous_act(torch.stack(\n",
    "    [torch.concat([torch.tensor([boa_token_id]), action, torch.tensor([eoa_token_id])]) \n",
    "    for action in torch.from_numpy(episode.actions)]\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5272fbc5-84bc-46a2-8539-4184c502874a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[51280, 50900, 50900],\n",
       "        [51280, 50900, 50900],\n",
       "        [51280, 50885, 50900],\n",
       "        [51280, 50478, 50900],\n",
       "        [51280, 50900, 50900]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action.tokens[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "fe5dd435-3758-4a9a-a323-7cad5ecba16a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2,)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "episode.actions[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9668f7b5-0759-47bb-ba18-ddb0e67fecbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([1024]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "be88e9dd-88c2-48ee-b5dd-6008677b148f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class PointMazeObservation(Observation):\n",
    "    observation: DiscreteData\n",
    "    goal: DiscreteData\n",
    "    action: DiscreteData\n",
    "\n",
    "    @property\n",
    "    def count(self):\n",
    "        return self.action.tokens.size(0)\n",
    "\n",
    "def pointmaze_tokenizer(tokenizer, episode, boa_token_id=51280, eoa_token_id=51280):\n",
    "    # There is always 1 more observation than there are actions,\n",
    "    # the \"terminated\" observation, which we don't care about.\n",
    "    # So take up to the :-1 of everything other than the actions.\n",
    "    observation = tokenizer.continuous_obs(\n",
    "        torch.stack(\n",
    "            [torch.concat([torch.tensor([boa_token_id]), action, torch.tensor([eoa_token_id])]) \n",
    "            for action in torch.from_numpy(episode.observations[\"observation\"][:-1])]\n",
    "        )\n",
    "    )\n",
    "    goal = tokenizer.continuous_obs(\n",
    "        torch.stack(\n",
    "            [torch.concat([torch.tensor([boa_token_id]), action, torch.tensor([eoa_token_id])]) \n",
    "            for action in torch.from_numpy(episode.observations[\"desired_goal\"][:-1])]\n",
    "        )\n",
    "    )\n",
    "    action = tokenizer.continuous_act(torch.stack(\n",
    "        [torch.concat([torch.tensor([boa_token_id]), action, torch.tensor([eoa_token_id])]) \n",
    "        for action in torch.from_numpy(episode.actions)]\n",
    "    ))\n",
    "    return PointMazeObservation(observation=observation, goal=goal, action=action)\n",
    "\n",
    "def pointmaze_collate_fn(batch):\n",
    "    \"\"\"TODO: This isn't collation.\"\"\"\n",
    "    result = []\n",
    "    for sample in batch:\n",
    "        i = random.randint(0, sample.count - 1)\n",
    "        # Starting at that index, we'll continue adding observations to our context window until\n",
    "        # we run out of space.\n",
    "        step = sample[i]\n",
    "        i += 1\n",
    "        while i < sample.count and step.size + step[0].size < SEQUENCE_LENGTH:\n",
    "            step = step.combine(sample[i])\n",
    "            i += 1\n",
    "        result.append(step)\n",
    "    return result\n",
    "\n",
    "pointmaze_tokenize = partial(pointmaze_tokenizer, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c5b85b49-9bb8-4795-a87e-a059daa51f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "pointmaze_tokenize(pointmaze_dataset[0]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "20eb4cc8-e9f8-42bf-a16f-50269b3f640c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pointmaze_dataset_xf = TransformDataset(pointmaze_dataset, pointmaze_tokenize)\n",
    "pointmaze_dataloader = DataLoader(pointmaze_dataset_xf, batch_size=BATCH_SIZE, collate_fn=pointmaze_collate_fn)\n",
    "pointmaze_batch = next(iter(pointmaze_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9995a20-6636-4829-b138-1d914b803bb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7efd2c-9e3d-47c4-80e6-e629673faec7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "68598475-1804-43ac-a845-f290b77af013",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = pointmaze_dataset[0]\n",
    "sample = next(iter(pointmaze_dataloader))[0]\n",
    "sample;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "4e4b559a-fe25-4eec-9938-14dd7d939311",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DiscreteData(tokens=tensor([[51280, 50900, 50900],\n",
       "        [51280, 50900, 50900],\n",
       "        [51280, 50885, 50900],\n",
       "        [51280, 50478, 50900]]), targets=tensor([[50900, 50900, 51280],\n",
       "        [50900, 50900, 51280],\n",
       "        [50885, 50900, 51280],\n",
       "        [50478, 50900, 51280]]), attention_mask=tensor([[1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1]]), embedding=tensor([]))"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's predict the action after the first 4 observations.\n",
    "pointmaze_observation = pointmaze_tokenize(pointmaze_dataset[0])[:4]\n",
    "pointmaze_observation.action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "50fd1ddb-ae50-4ee4-94f8-535c0d344f1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 3]), torch.Size([4, 5]))"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's predict the action after the first 4 observations.\n",
    "#pointmaze_observation = pointmaze_tokenize(sample)\n",
    "pointmaze_observation.action.tokens.shape, pointmaze_observation.observation.tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "58f8ee7b-ff56-4a1f-a608-6580e267fb1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 768])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pointmaze_embedding = pointmaze_observation.embed(model.embedder)\n",
    "pointmaze_embedding.action.embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "3cb715c1-afe8-493c-90e6-d3078379aabb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024, 768])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pointmaze_sequence_tokens, pointmaze_sequence_targets, pointmaze_sequence_attention_mask = pointmaze_embedding.sequence(model.sequence_length)\n",
    "pointmaze_sequence_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f2c9e809-95d7-4f0c-a885-19262b0c5561",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1024, 51281])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = [pointmaze_observation]\n",
    "predicted, targets, attention_mask = model(batch)\n",
    "predicted.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "7308cd05-11a5-4c12-8316-e95efed36675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top predicted tokens: 50259, 51280, 198, 37850, 4910, 514, 50258, 13, 25, 11\n",
      "Token probabilities: 0.2218, 0.0597, 0.0312, 0.0247, 0.0174, 0.0159, 0.0119, 0.0082, 0.0073, 0.0063\n"
     ]
    }
   ],
   "source": [
    "i = pointmaze_observation.size - pointmaze_observation.action.tokens.size(1)\n",
    "topk = (predicted.exp() / predicted.exp().sum(dim=2, keepdims=True))[:, [i]].topk(k=10, dim=2)\n",
    "print(f\"Top predicted tokens: {', '.join(f'{x}' for x in topk.indices.flatten())}\")\n",
    "print(f\"Token probabilities: {', '.join([f'{x:.4f}' for x in topk.values.flatten().tolist()])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "998efa8d-52c9-4bbc-9ee5-7273cb576a4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  1.        ],\n",
       "       [ 1.        ,  1.        ],\n",
       "       [ 0.74168795,  1.        ],\n",
       "       [-0.42298877,  1.        ],\n",
       "       [ 1.        ,  1.        ]], dtype=float32)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# It _should_ be predicting 1, but it's predicting 2 (50259 - 50257).\n",
    "# 2 is such a common action that it can get a low loss by just predicting 2 for everything.\n",
    "# I would expect with more training, the probability for 1 will go up, \n",
    "# even if the `topk` still predicts 2 for a long time to come.\n",
    "[pointmaze_dataset[i].actions for i in range(5)][0][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "4500c886-e5a6-429a-8498-e66493eddce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets: [-0.42298877  1.        ] - [50478, 50900, 51280]\n",
      "Target token: tensor([[51280, 50900, 50900],\n",
      "        [51280, 50900, 50900],\n",
      "        [51280, 50885, 50900],\n",
      "        [51280, 50478, 50900]])\n",
      "Top predicted tokens: 50259, 11, 25, 198, 13, 262, 514, 340, 284, 30\n",
      "Token probabilities: 0.2410, 0.0566, 0.0425, 0.0302, 0.0294, 0.0236, 0.0182, 0.0179, 0.0170, 0.0138\n"
     ]
    }
   ],
   "source": [
    "sample = next(iter(pointmaze_dataset))\n",
    "pointmaze_observation = pointmaze_tokenize(sample)[:4]\n",
    "batch = [pointmaze_observation]\n",
    "predicted, targets, attention_mask = model(batch)\n",
    "topk = (predicted.exp() / predicted.exp().sum(dim=2, keepdims=True))[:, [i]].topk(k=10, dim=2)\n",
    "pad = torch.tensor([51280])\n",
    "print(f\"Targets: {sample.actions[3]} - {tokenizer.continuous_act(torch.concat([pad, torch.from_numpy(sample.actions[3]), pad])).targets[-1].tolist()}\")\n",
    "print(f\"Target token: {pointmaze_observation.action.tokens}\")\n",
    "print(f\"Top predicted tokens: {', '.join(f'{x}' for x in topk.indices.flatten())}\")\n",
    "print(f\"Token probabilities: {', '.join([f'{x:.4f}' for x in topk.values.flatten().tolist()])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "c0c28fa6-b3b9-4c7f-b2df-0f20b3c730e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1.0988, 0.7270, 0.7270]),\n",
       " tensor([ 0.1977, -0.5460, -0.5460]),\n",
       " tensor([ 0.0644, -2.5419, -2.5419]),\n",
       " array([1., 1.], dtype=float32))"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We're kinda screwed because of the clip.\n",
    "\n",
    "# encoded = torch.clip(mu_law_encode(data), -1, 1)\n",
    "# shifted = (encoded + 1) / 2  # Map [-1, 1] to [0, 1]\n",
    "# # 1022 because 1023 is boa/eoa\n",
    "# scaled = (shifted * 1022).long().clamp(0, 1022) + 50157\n",
    "\n",
    "unscaled = (pointmaze_observation.action.tokens[0] - 50157).float() / 1022\n",
    "shifted = (unscaled - 1) * 2\n",
    "unscaled, shifted, mu_law_decode(shifted), sample.actions[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "fc2ebdb5-bcef-4bf3-8d89-6c8d12a6fc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = [\n",
    "    infinite_dataloader(partial(DataLoader, minigrid_dataset_xf, batch_size=BATCH_SIZE, collate_fn=minigrid_collate_fn)),\n",
    "    infinite_dataloader(partial(DataLoader, shakespeare_dataset_xf, batch_size=BATCH_SIZE, collate_fn=lambda x: x)),\n",
    "    infinite_dataloader(partial(DataLoader, vqa_dataset_xf, batch_size=BATCH_SIZE, collate_fn=lambda x: x)),\n",
    "    infinite_dataloader(partial(DataLoader, pointmaze_dataset_xf, batch_size=BATCH_SIZE, collate_fn=lambda x: x)),\n",
    "]\n",
    "dl_it = cycle(dataloaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b45a7e-54af-483d-bb37-3899ccbbe305",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63df4737-6785-4dd5-84a4-52f2763faec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_ITERATIONS = 20\n",
    "LR = 1e-3\n",
    "optimizer = torch.optim.AdamW(model.parameters())\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=NUM_ITERATIONS, eta_min=1e-5)\n",
    "trainer = MiniGatoTrainer(\n",
    "    model,\n",
    "    optimizer,\n",
    "    dataloaders,\n",
    "    num_iterations=NUM_ITERATIONS,\n",
    "    lr=LR,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f868177f-fd52-4528-bb62-21e51004ffd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = init_default_config()\n",
    "# model = MiniGato(config).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550289a2-9933-4d1b-bc2c-4f4ec20eb95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475d98ad-51b1-4049-8bdd-1907f93c8f0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2a69aafc-b0ca-4ba5-af08-dcfca89a611f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-100,  -99,  -98,  -97,  -96])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.0119, 0.0316, 0.0861, 0.2341, 0.6364])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = torch.arange(-100, -95)\n",
    "#q = q-q.max()\n",
    "print(q)\n",
    "probs = q.exp() / q.exp().sum()\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fce509ca-bc78-473e-bd35-e051e2caf878",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.15781409, -1.16204705,  0.        ,  0.        ],\n",
       "       [-2.15543246, -1.15966541,  0.23816384,  0.23816384],\n",
       "       [-2.15067485, -1.15490781,  0.47576046,  0.47576046]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pointmaze_dataset[0].observations[\"observation\"][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b2e3f4ed-7364-4226-b697-a4597a24bd73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EpisodeData(id=0, total_steps=45, observations={achieved_goal: ndarray of shape (46, 2) and dtype float64, desired_goal: ndarray of shape (46, 2) and dtype float64, observation: ndarray of shape (46, 4) and dtype float64}, actions=ndarray of shape (45, 2) and dtype float32, rewards=ndarray of 45 floats, terminations=ndarray of 45 bools, truncations=ndarray of 45 bools, infos=dict with the following keys: ['goal', 'qpos', 'qvel', 'success'])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pointmaze_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "48605df4-75bb-427f-ad4c-e973913951ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.15781409, -1.16204705],\n",
       "       [-2.15543246, -1.15966541],\n",
       "       [-2.15067485, -1.15490781]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pointmaze_dataset[0].observations['achieved_goal'][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7acf9e55-601d-4715-9eac-2c048182391b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.22308949,  0.86017994],\n",
       "       [-2.22308949,  0.86017994],\n",
       "       [-2.22308949,  0.86017994]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pointmaze_dataset[0].observations['desired_goal'][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "26429547-d400-44f4-829f-22a68463da08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 1.        ],\n",
       "       [1.        , 1.        ],\n",
       "       [0.74168795, 1.        ]], dtype=float32)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pointmaze_dataset[0].actions[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9db8768-7650-4bb5-afb0-74e1640b6b6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7520f46-69f6-40d2-b3d4-6dcca0a6b0ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a5d15b-c34c-486a-b60c-69b2aae2a734",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862fecb5-c238-439b-8252-42f362b58358",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1515a4c2-6b35-489b-adb2-482eaf0615cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "714453a9-5116-4633-be17-7fe0536163c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vqa_batch = next(iter(DataLoader(vqa_dataset_xf, batch_size=BATCH_SIZE, collate_fn=lambda x: x, num_workers=4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "67122707-b677-4e8a-b0e9-c87d0563ef84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1024, 768])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = vqa_batch[0].embed(model.embedder)\n",
    "emb.question.embedding.shape, emb.image.embedding.shape, emb.answer.embedding.shape\n",
    "cc = torch.concat([emb.question.embedding, emb.image.embedding, emb.answer.embedding], dim=1)\n",
    "F.pad(cc, (0, 0, 0, 1024 - cc.size(1)), value=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "addb2b50-3530-4931-87ea-688515668def",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1024, 768]), torch.Size([1024]))"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vqa_batch[0].embed(model.embedder).sequence(1024)[0].shape, vqa_batch[0].embed(model.embedder).sequence(1024)[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "790bbdef-0ece-4e1a-a0dd-26d28a21d5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = iter(minigrid_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7b5c9d72-fc5f-4360-a39f-a47d6de39283",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "2a3742cf-5d46-404e-9b2a-17b7edc09110",
   "metadata": {},
   "outputs": [],
   "source": [
    "ep = batch\n",
    "em = [e.embed(model.embedder) for e in ep]\n",
    "sq = [e.sequence(model.sequence_length) for e in em]\n",
    "xs, ys, ms = map(torch.stack, zip(*sq))\n",
    "pr, ys, ms = model(ep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "9317da5e-6591-4e77-b390-273c5a305c00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DiscreteData(tokens=tensor([[51280, 50259],\n",
       "        [51280, 50258],\n",
       "        [51280, 50259],\n",
       "        [51280, 50259],\n",
       "        [51280, 50259],\n",
       "        [51280, 50259]]), targets=tensor([[50259, 51280],\n",
       "        [50258, 51280],\n",
       "        [50259, 51280],\n",
       "        [50259, 51280],\n",
       "        [50259, 51280],\n",
       "        [50259, 51280]]), attention_mask=tensor([[1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1]]), embedding=tensor([]))"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ep[0].action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "047b052c-1419-497f-92ed-865e02db0f1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(882,\n",
       " 6,\n",
       " tensor([], size=(6, 0), dtype=torch.int64),\n",
       " torch.Size([4, 1024, 50257]),\n",
       " torch.Size([4, 1024]))"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ep[0].size, ep[0].size // ep[0][0].size, ep[-1].action.targets, pr.shape, ys.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "e26cddd2-66d4-4f39-b1f8-9ddfb1823449",
   "metadata": {},
   "outputs": [],
   "source": [
    "heat = 0.01\n",
    "prh = pr / heat\n",
    "sm = prh.softmax(dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "946e2e9b-3f56-4704-a1e8-f653a22f765f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1024, 50257])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86b0239-383d-45a5-8466-28310303f819",
   "metadata": {},
   "source": [
    "Which indexes do you want? Not the 1023, because of padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "cef167e1-0158-4db4-9db8-adc1a8e0b6e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 1024, 50257]),\n",
       " torch.Size([4, 1024]),\n",
       " torch.Size([0, 1]),\n",
       " tensor([], device='cuda:0', size=(0, 1), dtype=torch.int64))"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr.shape, ms.shape, ms[0].nonzero().shape, ms[0].nonzero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "c6381f89-2bb7-42e3-8f89-77bdfb2df5dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([], device='cuda:0', dtype=torch.int32),\n",
       " tensor([], device='cuda:0', dtype=torch.int32),\n",
       " tensor([], device='cuda:0', dtype=torch.int32),\n",
       " tensor([], device='cuda:0', dtype=torch.int32)]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ys[batch_index][ms[batch_index].nonzero().flatten()].to(torch.int) for batch_index in range(len(batch))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "78896ef4-811c-4031-b828-bd6ea346f049",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([], device='cuda:0', dtype=torch.int32),\n",
       " tensor([], device='cuda:0', dtype=torch.int32),\n",
       " tensor([], device='cuda:0', dtype=torch.int32),\n",
       " tensor([], device='cuda:0', dtype=torch.int32)]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[pr[batch_index][ms[batch_index].nonzero().flatten()].argmax(dim=1).to(torch.int) for batch_index in range(len(batch))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "624c8593-d98d-4e74-87fc-c9581f1f4db3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([],\n",
       " tensor([-9.6881, -8.5413, -9.0677], device='cuda:0', grad_fn=<IndexBackward0>))"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ms[0].nonzero().flatten().tolist(), pr[0][0, [1, 2, 1023]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "c18f1d46-f2f2-418f-8095-26d6e83dda2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4399],\n",
       "        [4399],\n",
       "        [ 504],\n",
       "        [ 377]], device='cuda:0')"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sl = torch.multinomial(sm[:, 893, :], num_samples=1)\n",
    "sl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "d26dda6b-369f-4804-9484-0650618e3e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_text():\n",
    "    model.eval()\n",
    "    text = \"First Citizen:\"\n",
    "    next_word_token = None\n",
    "    i = 0\n",
    "    while i < 20 and next_word_token != __text_tokenizer.eos_token:\n",
    "        with torch.no_grad():\n",
    "            tokens = text_tokenize(text)\n",
    "            x = tokens.embed(model.embedder).to(device)\n",
    "            pr, ys, ms = model([tokens])\n",
    "            heat = 0.5\n",
    "            prh = pr / heat\n",
    "            sm = prh.softmax(dim=2)\n",
    "            last_index = ms.nonzero()[-1].cpu()[1]\n",
    "            next_word_probs = sm[0, last_index-1]\n",
    "            next_word_token = torch.multinomial(next_word_probs, num_samples=1)\n",
    "            next_word = __text_tokenizer.decode(next_word_token)\n",
    "            text += next_word\n",
    "        i += 1\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "e9ebf0e2-653e-46e6-ab13-3da255fd02ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen: were\n",
      ":\n",
      "\n",
      "\n",
      ",I\n",
      "USI:US\n",
      ":Now Cor, you\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(eval_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "6a7cd0c1-ddcd-4147-a8ec-391f12069386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Control targets:   []\n",
      "Control predicted: []\n"
     ]
    }
   ],
   "source": [
    "targets = [ys[batch_index][ms[batch_index].nonzero().flatten()].to(torch.int) for batch_index in range(len(batch))][0]\n",
    "predicted = [pr[batch_index][ms[batch_index].nonzero().flatten()].argmax(dim=1).to(torch.int) for batch_index in range(len(batch))][0]\n",
    "print(f\"Control targets:   {targets.tolist()}\\nControl predicted: {predicted.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "47af1818-be4d-4a83-a86d-bb6f324c484f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024, 50257])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm[0].shape["
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8670300c-b303-4d7c-83ff-9f7028f9a3c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "20da0246-8084-48ab-8f17-69b7c448a84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vqa_sample = next(iter(vqa_dataset[\"train\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "1b6d28f9-2803-468d-a63e-c5c84747aecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "vqa_batch = vqa_tokenize(vqa_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "dd54483f-1db6-4d93-86d0-6f1fe0b12f14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VQATimestep(question=TextTokenData(tokens=tensor([[ 2061,   318,   262, 42658,  2349,   286,   262, 32749,  1444,    30]]), targets=tensor([[ 2061,   318,   262, 42658,  2349,   286,   262, 32749,  1444,    30]]), attention_mask=tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), embedding=tensor([])), image=ImageTokenData(tokens=tensor([[[-0.2027, -0.2171, -0.2428,  ...,  0.2012,  0.2362,  0.2887],\n",
       "         [-0.1774, -0.1694, -0.1800,  ..., -0.0863, -0.0728, -0.0485],\n",
       "         [-0.1409, -0.1409, -0.1700,  ...,  0.0578,  0.0631,  0.0739],\n",
       "         ...,\n",
       "         [ 0.0972,  0.0924,  0.0732,  ...,  0.2398,  0.2446,  0.2495],\n",
       "         [ 0.1705,  0.1728,  0.1752,  ...,  0.2814,  0.2162, -0.0036],\n",
       "         [-0.0532, -0.0909, -0.1568,  ...,  0.2024,  0.1832,  0.1880]]]), targets=tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), attention_mask=tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), embedding=tensor([])), answer=TextTokenData(tokens=tensor([[  79, 1647]]), targets=tensor([[1647, 7894]]), attention_mask=tensor([[1, 1]]), embedding=tensor([])))"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vqa_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "6ef0608e-1851-43dd-99f9-1f2d782da6e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VQATimestep(question=TextTokenData(tokens=tensor([[ 2061,   318,   262,  4190,  3918,   286,   262, 21541,  1444,    30]]), targets=tensor([[ 2061,   318,   262,  4190,  3918,   286,   262, 21541,  1444,    30]]), attention_mask=tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), embedding=tensor([[[-0.5805, -0.0026, -2.2463,  ...,  0.1824,  0.5532, -0.5343],\n",
       "         [ 1.5129, -0.6796,  0.2971,  ..., -0.0411,  0.3358,  0.3959],\n",
       "         [ 0.9331,  1.9109, -0.0039,  ...,  0.5315, -0.7628, -0.4911],\n",
       "         ...,\n",
       "         [ 0.2087, -0.2230, -0.0372,  ...,  0.1041,  0.7136,  1.8701],\n",
       "         [ 0.0037,  0.4166, -1.3697,  ...,  0.3799,  1.1672,  0.9124],\n",
       "         [-0.0659, -0.4796,  0.5309,  ..., -0.6050, -0.4605, -0.5341]]],\n",
       "       grad_fn=<EmbeddingBackward0>)), image=ImageTokenData(tokens=tensor([[[-0.1049, -0.1489, -0.1169,  ...,  0.2318,  0.2277,  0.2562],\n",
       "         [-0.2601, -0.2411, -0.2474,  ...,  0.2021,  0.1989,  0.1925],\n",
       "         [-0.1489, -0.1873, -0.2092,  ...,  0.2413,  0.2357,  0.2329],\n",
       "         ...,\n",
       "         [ 0.0734,  0.0734,  0.0682,  ..., -0.2781, -0.2623, -0.2728],\n",
       "         [ 0.2887,  0.2654,  0.2136,  ..., -0.1990, -0.2096, -0.2201],\n",
       "         [-0.0534,  0.0305,  0.0305,  ..., -0.2716, -0.2683, -0.2486]]]), targets=tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), attention_mask=tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), embedding=tensor([[[ 0.2243,  0.1535,  0.0857,  ..., -0.1870, -0.0162, -0.0964],\n",
       "         [-0.0652, -0.0719, -0.0292,  ..., -0.2037, -0.0725, -0.1617],\n",
       "         [ 0.1381,  0.0451, -0.0297,  ..., -0.1500, -0.0641, -0.1657],\n",
       "         ...,\n",
       "         [ 0.0687,  0.0758,  0.0967,  ..., -0.6015, -0.6149, -0.5098],\n",
       "         [ 0.1730,  0.3239,  0.4082,  ..., -0.4999, -0.5908, -0.5058],\n",
       "         [-0.1170, -0.1532, -0.1406,  ..., -0.7078, -0.7351, -0.5654]]],\n",
       "       grad_fn=<AddBackward0>)), answer=TextTokenData(tokens=tensor([[21943, 50256, 50256,  ..., 50256, 50256, 50256]]), targets=tensor([[50256, 50256, 50256,  ..., 50256, 50256, 50256]]), attention_mask=tensor([[1, 0, 0,  ..., 0, 0, 0]]), embedding=tensor([[[-0.0024, -1.4722, -1.0243,  ..., -1.9218,  0.6434, -0.9012],\n",
       "         [ 1.2994, -0.2619,  0.0363,  ...,  0.3679,  0.8233, -2.0763],\n",
       "         [ 1.2994, -0.2619,  0.0363,  ...,  0.3679,  0.8233, -2.0763],\n",
       "         ...,\n",
       "         [ 1.2994, -0.2619,  0.0363,  ...,  0.3679,  0.8233, -2.0763],\n",
       "         [ 1.2994, -0.2619,  0.0363,  ...,  0.3679,  0.8233, -2.0763],\n",
       "         [ 1.2994, -0.2619,  0.0363,  ...,  0.3679,  0.8233, -2.0763]]],\n",
       "       grad_fn=<EmbeddingBackward0>)))"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = tokenizer.text_obs(\"What is the hair style of the blonde called?\")\n",
    "image = tokenizer.image(image_transform(vqa_dataset['train'][0]['image']))\n",
    "answer_text = \"foo\"\n",
    "answer = tokenizer.text_gen(answer_text)\n",
    "timestep = VQATimestep(question=question, image=image, answer=answer)\n",
    "timestep.embed(model.embedder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "301539cc-549c-4f07-b8dc-a054755caf44",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "question = tokenizer.text_obs(\"What is the hair style of the blonde called?\")\n",
    "image = tokenizer.image(image_transform(vqa_dataset['train'][0]['image']))\n",
    "answer_text = __text_tokenizer.bos_token\n",
    "answer = tokenizer.text_gen(answer_text)\n",
    "timestep = VQATimestep(question=question, image=image, answer=answer)\n",
    "next_word_token = None\n",
    "i = 0\n",
    "with torch.no_grad():\n",
    "    pr, ys, ms = model([timestep])\n",
    "    heat = 0.1\n",
    "    prh = pr / heat\n",
    "    sm = prh.softmax(dim=2)\n",
    "    last_index = ms.nonzero()[-1].cpu()[1]\n",
    "    next_word_probs = sm[0, last_index-1]\n",
    "    next_word_token = torch.multinomial(next_word_probs, num_samples=1)\n",
    "    next_word = __text_tokenizer.decode(next_word_token)\n",
    "    answer_text += next_word\n",
    "    answer = tokenizer.text_gen(answer_text, padding=False)\n",
    "    timestep = VQATimestep(question=question, image=image, answer=answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "9e20cfee-0ae1-4990-82d0-e8882ed0a6a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|endoftext|> park'"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf779b50-1c11-47e9-af0b-3a8598940652",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "75fcb31f-fea7-4a8c-ab7a-4f844dddac14",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs, ys, ms = timestep.embed(model.embedder).sequence(1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "c4498c5f-b18b-477b-9905-99c2ff6fc56b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1024, 768]), torch.Size([1024]), torch.Size([1024]))"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs.shape, ys.shape, ms.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "271ba462-626b-4970-8466-6b67aab0745f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[154]])"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = timestep.embed(model.embedder)\n",
    "ms.nonzero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "92d409a0-2edb-4620-b842-06c329b9f9b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 10, 768]), torch.Size([1, 144, 768]))"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb.question.embedding.shape, emb.image.embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "bcea1401-9c58-491c-9469-b357f0caa9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_vqa():\n",
    "    model.eval()\n",
    "    question = tokenizer.text_obs(\"What is the hair style of the blonde called?\")\n",
    "    image = tokenizer.image(image_transform(vqa_dataset['train'][0]['image']))\n",
    "    answer_text = __text_tokenizer.bos_token\n",
    "    answer = tokenizer.text_gen(answer_text)\n",
    "    timestep = VQATimestep(question=question, image=image, answer=answer)\n",
    "    next_word_token = None\n",
    "    i = 0\n",
    "    while i < 20 and next_word_token != __text_tokenizer.eos_token:\n",
    "        with torch.no_grad():\n",
    "            pr, ys, ms = model([timestep])\n",
    "            heat = 0.7\n",
    "            prh = pr / heat\n",
    "            sm = prh.softmax(dim=2)\n",
    "            last_index = ms.nonzero()[-1].cpu()[1]\n",
    "            next_word_probs = sm[0, last_index-1]\n",
    "            next_word_token = torch.multinomial(next_word_probs, num_samples=1)\n",
    "            next_word = __text_tokenizer.decode(next_word_token)\n",
    "            answer_text += next_word\n",
    "            answer = tokenizer.text_gen(answer_text, padding=False)\n",
    "            timestep = VQATimestep(question=question, image=image, answer=answer)\n",
    "        i += 1\n",
    "    return timestep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "2bbb29a9-8aaa-4fa5-bdc8-a56133cec03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = eval_vqa()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "c821e96c-8979-415b-af27-83796d37d191",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.answer.tokens[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "5603b93b-d38c-4993-91fc-13cb0c932d31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|endoftext|>agle sign southil signilgeapportotionaglegeWas andgeilalamelBoth'"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "__text_tokenizer.decode(result.answer.tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b11738a2-9909-4355-a24f-b5728a4b908a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 2, 50257])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "af91e8a1-e5ba-490a-aec1-0e13d278a3dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0],\n",
       "        [0, 1],\n",
       "        [0, 2],\n",
       "        [0, 3],\n",
       "        [0, 4]], device='cuda:0')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ms.nonzero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d041aebe-16d8-41bc-9170-aca57dba4e51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 4])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ms.nonzero()[-1].cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8d1ec768-5107-4489-ac74-95a9bb76d429",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(25)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens.text.tokens[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "61b0de11-c3c0-4168-ab9f-2b83b6d07ee5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 1024, 50257]), tensor([0, 4], device='cuda:0'))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm.shape, last_index[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e86351-6897-4885-a88f-513c595c4b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "    tokens = tokenize_text([text], max_length=SEQUENCE_LENGTH, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "    x = embed_text(tokens[\"input_ids\"]).to(device)\n",
    "    m = tokens[\"attention_mask\"].to(device)\n",
    "    length = m.sum().item()\n",
    "    o = model(inputs_embeds=x)\n",
    "    predicted = lm_head(o.last_hidden_state)\n",
    "    chosen = torch.multinomial(predicted.softmax(dim=2)[:, length-1], num_samples=1)\n",
    "    token = chosen[0]\n",
    "    text += _text_tokenizer.decode(chosen[0])\n",
    "    i += 1\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194e4ad7-95e4-4a27-a683-d221ad1ce0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "text = \"First Citizen:\"\n",
    "token = None\n",
    "i = 0\n",
    "with torch.no_grad():\n",
    "    while i < 20 and token != _text_tokenizer.eos_token_id:\n",
    "        tokens = tokenize_text([text], max_length=SEQUENCE_LENGTH, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "        x = embed_text(tokens[\"input_ids\"]).to(device)\n",
    "        m = tokens[\"attention_mask\"].to(device)\n",
    "        length = m.sum().item()\n",
    "        o = model(inputs_embeds=x)\n",
    "        predicted = lm_head(o.last_hidden_state)\n",
    "        chosen = torch.multinomial(predicted.softmax(dim=2)[:, length-1], num_samples=1)\n",
    "        token = chosen[0]\n",
    "        text += _text_tokenizer.decode(chosen[0])\n",
    "        i += 1\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "15942866-1fac-44c0-a0e3-8b0a1eafade1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '_text_tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[183], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m token \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 11\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m i \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m token \u001b[38;5;241m!=\u001b[39m \u001b[43m_text_tokenizer\u001b[49m\u001b[38;5;241m.\u001b[39meos_token:\n\u001b[1;32m     12\u001b[0m         x, y, m \u001b[38;5;241m=\u001b[39m sequence_vqa(tokenize_text, embed_text, tokenize_image, embed_image, batch)\n\u001b[1;32m     13\u001b[0m         x, y, m \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(device), y\u001b[38;5;241m.\u001b[39mto(device), m\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[0;31mNameError\u001b[0m: name '_text_tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "batch = {\n",
    "    \"question\": question,\n",
    "    \"image\": image,\n",
    "    \"answer\": [\"\"],\n",
    "}\n",
    "i = 0\n",
    "token = \"\"\n",
    "with torch.no_grad():\n",
    "    while i < 10 and token != _text_tokenizer.eos_token:\n",
    "        x, y, m = sequence_vqa(tokenize_text, embed_text, tokenize_image, embed_image, batch)\n",
    "        x, y, m = x.to(device), y.to(device), m.to(device)\n",
    "        o = model(inputs_embeds=x)\n",
    "        predicted = lm_head(o.last_hidden_state)\n",
    "        token = _text_tokenizer.decode(predicted.softmax(dim=2)[0].multinomial(num_samples=1).squeeze(1)[768+i])\n",
    "        token = _text_tokenizer.decode(predicted.argmax(dim=2).squeeze(0)[768+i])\n",
    "        # with temperature\n",
    "        heat = 0.1\n",
    "        heated = predicted / heat\n",
    "        token = _text_tokenizer.decode(heated.softmax(dim=2)[0].multinomial(num_samples=1).squeeze()[768+i])\n",
    "        batch[\"answer\"][0] += token\n",
    "        i += 1\n",
    "batch[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba529169-b327-4b7e-a262-5325f4d7a08f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
