{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "949b7cdb-1309-4a42-af31-70ddbbaa978f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "from dataclasses import dataclass, fields\n",
    "from functools import partial\n",
    "from itertools import cycle\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pdb\n",
    "import random\n",
    "import re\n",
    "import tempfile\n",
    "from einops import rearrange\n",
    "import datasets\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import minari\n",
    "from minigrid.core import constants as mgc\n",
    "import requests\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "import torchvision.transforms.v2 as transforms\n",
    "from torchvision.transforms.functional import pil_to_tensor\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import GPT2Tokenizer, GPT2Config, GPT2Model\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7a51868-93a6-4a29-b669-b7dca65d3985",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModalityData:\n",
    "    tokens: torch.Tensor\n",
    "    targets: torch.Tensor\n",
    "    attention_mask: torch.Tensor\n",
    "    embedding: torch.Tensor = torch.tensor([])\n",
    "\n",
    "    def combine(self, other):\n",
    "        \"\"\"Concats attributes of self to attributes of other.\n",
    "\n",
    "        Example:\n",
    "        You might have some text that's part of a control task.\n",
    "        Let's say the text is \"reach the exit\", the task's goal.\n",
    "        The control task episode has 4 observations, the previous 4 frames of video.\n",
    "        Combining the first 3 observations with the last observation would look like:\n",
    "          The shape of the text tokens of the first 3 observations: (3, 5)\n",
    "          The shape of the text tokens of the last observation:     (1, 5)\n",
    "        \"\"\"\n",
    "        return type(self)(\n",
    "            tokens=torch.concat([self.tokens, other.tokens]),\n",
    "            targets=torch.concat([self.targets, other.targets]),\n",
    "            attention_mask=torch.concat([self.attention_mask, other.attention_mask]),\n",
    "            embedding=torch.concat([self.embedding, other.embedding]),\n",
    "            \n",
    "        )\n",
    "\n",
    "    def embed(self, embedder):\n",
    "        return type(self)(\n",
    "            tokens=self.tokens,\n",
    "            targets=self.targets,\n",
    "            attention_mask=self.attention_mask,\n",
    "            embedding=self.embed_fn(embedder)(self.tokens),\n",
    "        ) \n",
    "\n",
    "    def to(self, device):\n",
    "        return type(self)(\n",
    "            tokens=self.tokens.to(device),\n",
    "            targets=self.targets.to(device),\n",
    "            attention_mask=self.attention_mask.to(device),\n",
    "            embedding=self.embedding.to(device),\n",
    "        )\n",
    "        \n",
    "    @property\n",
    "    def size(self):\n",
    "        \"\"\"The number of tokens this will consume of the context window\"\"\"\n",
    "        return self.tokens.size(0) * self.tokens.size(1)\n",
    "\n",
    "class TextData(ModalityData):\n",
    "    def embed_fn(self, embedder):\n",
    "        return embedder.text\n",
    "\n",
    "class ImageData(ModalityData):\n",
    "    def embed_fn(self, embedder):\n",
    "        return embedder.image\n",
    "        \n",
    "class DiscreteData(ModalityData):\n",
    "    def embed_fn(self, embedder):\n",
    "        return embedder.discrete\n",
    "\n",
    "# There is no such thing as ContinuousData.\n",
    "# Continuous just gets binned to discrete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7503a9c7-97ae-47fe-8d43-c92b19671e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://docs.python.org/3.7/library/dataclasses.html#dataclasses.dataclass\n",
    "# The order of the fields of subclasses matters!\n",
    "@dataclass\n",
    "class Observation:\n",
    "    def __getitem__(self, i):\n",
    "        return type(self)(**{\n",
    "            field.name: type(getattr(self, field.name))(\n",
    "                tokens=getattr(self, field.name).tokens[[i]],\n",
    "                targets=getattr(self, field.name).targets[[i]],\n",
    "                attention_mask=getattr(self, field.name).attention_mask[[i]],\n",
    "            )\n",
    "            for field in fields(self)\n",
    "        })\n",
    "\n",
    "    def combine(self, other):\n",
    "        return type(self)(**{\n",
    "            field.name: getattr(self, field.name).combine(getattr(other, field.name))\n",
    "            for field in fields(self)\n",
    "        })\n",
    "\n",
    "    @property\n",
    "    def size(self):\n",
    "        return sum(getattr(self, field.name).size for field in fields(self))\n",
    "\n",
    "    @property\n",
    "    def count(self):\n",
    "        # This is typically safe. But you might need to override it.\n",
    "        # For example, the count of a RL observation might be the number\n",
    "        # of actions, not the number of observations, because the last\n",
    "        # observation is the \"terminated\" observation that we don't care about.\n",
    "        return next(getattr(self, field.name) for field in fields(self)).tokens.size(0)\n",
    "\n",
    "    def embed(self, embedder):\n",
    "        return type(self)(**{\n",
    "            field.name: getattr(self, field.name).embed(embedder)\n",
    "            for field in fields(self)\n",
    "        })\n",
    "\n",
    "    def to(self, device):\n",
    "        return type(self)(**{\n",
    "            field.name: getattr(self, field.name).to(device)\n",
    "            for field in fields(self)\n",
    "        })\n",
    "\n",
    "    def sequence(self, sequence_length):\n",
    "        xs = torch.concat([getattr(self, field.name).embedding for field in fields(self)], dim=1)\n",
    "        ys = torch.concat([getattr(self, field.name).targets for field in fields(self)], dim=1)\n",
    "        ms = torch.concat([getattr(self, field.name).attention_mask for field in fields(self)], dim=1)\n",
    "        T, S, C = xs.shape\n",
    "        xs, ys, ms = xs.reshape(T*S, C), ys.reshape(T*S), ms.reshape(T*S)\n",
    "        padding_len = sequence_length - T*S\n",
    "        xs = F.pad(xs, (0, 0, 0, padding_len), value=0)\n",
    "        ys, ms = [F.pad(x, (0, padding_len), value=0) for x in [ys, ms]]\n",
    "        return xs, ys, ms\n",
    "\n",
    "@dataclass\n",
    "class GenericTextObservation(Observation):\n",
    "    text: TextData\n",
    "\n",
    "@dataclass\n",
    "class GenericVQAObservation(Observation):\n",
    "    question: TextData\n",
    "    image: ImageData\n",
    "    answer: TextData\n",
    "\n",
    "# There is no such thing as a GenericRoboticsObservation.\n",
    "# There's too much variety in robotics datasets.\n",
    "# But here's an example of what a specific one might look like.\n",
    "# https://minigrid.farama.org/environments/minigrid/FourRoomsEnv/\n",
    "@dataclass\n",
    "class FourRoomsObservation(Observation):\n",
    "    mission: TextData\n",
    "    direction: DiscreteData    \n",
    "    image: ImageData\n",
    "    action: DiscreteData\n",
    "\n",
    "    @property\n",
    "    def count(self):\n",
    "        return self.action.tokens.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff59177-d1b5-4fa5-8354-f59756841803",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19bee90e-e4f9-44a3-ac01-29f7cb30a64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def images_to_patches(images, patch_size=16):\n",
    "    return rearrange(images, 'b c (h s1) (w s2) -> b (h w) (c s1 s2)', s1=patch_size, s2=patch_size)\n",
    "\n",
    "def normalize_to_between_minus_one_plus_one(t: torch.Tensor):\n",
    "    min_val, max_val = t.min(), t.max()\n",
    "    if min_val == max_val:\n",
    "        return torch.zeros_like(t)\n",
    "    normalized = 2 * (t - min_val) / (max_val - min_val) - 1\n",
    "    return normalized\n",
    "\n",
    "def apply_along_dimension(func, dim, tensor):\n",
    "    tensor = tensor.transpose(0, dim)\n",
    "    shape = tensor.shape\n",
    "    tensor = tensor.reshape(shape[0], -1)\n",
    "    result = torch.stack([func(tensor[:, i]) for i in range(tensor.size(1))], dim=1)\n",
    "    result = result.reshape(shape).transpose(0, dim)\n",
    "    return result\n",
    "\n",
    "def mu_law_encode(x, M=256, mu=100):\n",
    "    M = torch.tensor(M, dtype=x.dtype)\n",
    "    mu = torch.tensor(mu, dtype=x.dtype)\n",
    "    x_mu = torch.sign(x) * torch.log(torch.abs(x) * mu + 1.0)\n",
    "    x_mu = x_mu / torch.log(M * mu + 1.0)\n",
    "    return x_mu\n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self, text_tokenizer):\n",
    "        # This is expected to be a partial with reasonable defaults around something like a HuggingFace Tokenizer.\n",
    "        self.text_tokenizer = text_tokenizer\n",
    "\n",
    "    @property\n",
    "    def bos_token(self):\n",
    "        return self.text_tokenizer.func.bos_token\n",
    "\n",
    "    @property\n",
    "    def eos_token(self):\n",
    "        return self.text_tokenizer.func.eos_token\n",
    "\n",
    "    # The difference between these two is just the defaults.\n",
    "    # Generative tokenizer has a ones attention mask.\n",
    "    # Observative tokenizer has a zeros attention mask.\n",
    "    def text(self, data, **kwargs):\n",
    "        tokenized =  self.text_tokenizer(data, **kwargs)\n",
    "        return TextData(**{\n",
    "            \"tokens\": tokenized[\"input_ids\"][:, :-1].to(torch.long),\n",
    "            \"targets\": tokenized[\"input_ids\"][:, 1:].to(torch.long),\n",
    "            \"attention_mask\": tokenized[\"attention_mask\"][:, :-1],\n",
    "        })\n",
    "\n",
    "    def text_obs(self, data, **kwargs):\n",
    "        text_data = self.text(data)\n",
    "        text_data.attention_mask = torch.zeros_like(text_data.attention_mask)\n",
    "        return text_data\n",
    "\n",
    "    def image(self, data):\n",
    "        if len(data.shape) == 3:\n",
    "          data = data.unsqueeze(0)\n",
    "        patches = images_to_patches(data, patch_size=16)\n",
    "        # Hardcoding as a reminder to do something smarter\n",
    "        SQUARE_ROOT_OF_PATCH_SIZE = 3.464\n",
    "        xs = (\n",
    "            apply_along_dimension(\n",
    "                normalize_to_between_minus_one_plus_one, 2, patches\n",
    "            )\n",
    "            / SQUARE_ROOT_OF_PATCH_SIZE\n",
    "        )\n",
    "        # We don't predict images, but we need ys\n",
    "        # becaues these image ys will be in our\n",
    "        # concatenated ys of text/image/action/etc...\n",
    "        ys = torch.zeros(xs.shape[:2]).to(torch.long)\n",
    "        ms = torch.zeros(xs.shape[:2])  # Same story as above.\n",
    "        return ImageData(tokens=xs, targets=ys, attention_mask=ms)\n",
    "\n",
    "    def discrete_obs(self, data):\n",
    "        if len(data.shape) == 0:\n",
    "            data = data.unsqueeze(0)\n",
    "        if len(data.shape) == 1:\n",
    "            data = data.unsqueeze(1)\n",
    "        xs = data[:, :-1] + self.text_tokenizer.func.vocab_size\n",
    "        ys = data[:, 1:] + self.text_tokenizer.func.vocab_size\n",
    "        ms = torch.zeros_like(ys)\n",
    "        return DiscreteData(tokens=xs, targets=ys, attention_mask=ms)\n",
    "\n",
    "    def discrete_act(self, data):\n",
    "        discrete_data = self.discrete_obs(data)\n",
    "        discrete_data.attention_mask = torch.ones_like(discrete_data.targets)\n",
    "        return discrete_data\n",
    "\n",
    "    def continuous_obs(self, data):\n",
    "        if len(data.shape) == 0:\n",
    "            data = data.unsqueeze(0)\n",
    "        if len(data.shape) == 1:\n",
    "            data = data.unsqueeze(1)\n",
    "        # hardcoding an assumption that boa/eoa surrounds data.\n",
    "        boa, data, eoa = data[:, [0]], data[:, 1:-1], data[:, [-1]]\n",
    "        encoded = torch.clip(mu_law_encode(data), -1, 1)\n",
    "        shifted = (encoded + 1) / 2  # Map [-1, 1] to [0, 1]\n",
    "        # 1022 because 1023 is boa/eoa\n",
    "        scaled = (shifted * 1022).long().clamp(0, 1022) + self.text_tokenizer.func.vocab_size\n",
    "        xs = torch.concat([boa, scaled, eoa], dim=1).to(torch.long)\n",
    "        ys = xs[:, 1:]\n",
    "        ms = torch.zeros_like(ys)\n",
    "        return DiscreteData(tokens=xs, targets=ys, attention_mask=ms)\n",
    "\n",
    "    def continuous_act(self, data):\n",
    "        continuous_data = self.continuous_obs(data)\n",
    "        continuous_data.attention_mask = torch.ones_like(continuous_data.targets)\n",
    "        return continuous_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3649c8cc-5946-4046-92c5-32c1f210ad65",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformDataset(Dataset):\n",
    "    def __init__(self, dataset, transform):\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.transform(self.dataset[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "948b6001-ceae-4124-b2f4-bf66a59100db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some FourRooms/Minigrid-specific stuff to turn\n",
    "# a 7x7x3 non-pixel observation into an pixel/image observation.\n",
    "lut = np.zeros((256, 3), dtype=np.uint8)\n",
    "for idx, color_name in mgc.IDX_TO_COLOR.items():\n",
    "    lut[idx] = mgc.COLORS[color_name]\n",
    "\n",
    "def minigrid_to_rgb(episode):\n",
    "    \"\"\"Convert discrete \"image\" observations into actual images.\n",
    "    I'm expecting this will improve our image modality while not losing\n",
    "    much. The downside is we can fit less in our context window. Note:\n",
    "    We might need to overlay the color/type image (index 1) with the\n",
    "    state image (index 2), if we really don't want to lose any info.\"\"\"\n",
    "    # Apply lookup to second channel\n",
    "    image = lut[episode.observations['image'][:, :, :, 1]]\n",
    "    # Convert to PyTorch tensor and permute\n",
    "    image = torch.from_numpy(image).permute(0, 3, 1, 2)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40a24ce2-dcb4-42e4-9291-931f80c30da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_transform = transforms.Compose([\n",
    "    transforms.ToDtype(torch.float32, scale=True),    \n",
    "    transforms.RandomResizedCrop((192, 192), (0.6, 1.0)),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f72599f-0d98-43da-8a4e-97d57366d875",
   "metadata": {},
   "outputs": [],
   "source": [
    "def four_rooms_tokenizer(tokenizer, episode, boa_token_id=1023, eoa_token_id=1023):\n",
    "    # There is always 1 more observation than there are actions,\n",
    "    # the \"terminated\" observation, which we don't care about.\n",
    "    # So take up to the :-1 of everything other than the actions.\n",
    "    image = image_transform(minigrid_to_rgb(episode)[:-1])\n",
    "    image = tokenizer.image(image)\n",
    "    mission = tokenizer.text_obs(episode.observations['mission'][:-1], padding=False)\n",
    "    direction = tokenizer.discrete_obs(torch.from_numpy(episode.observations['direction'])[:-1])\n",
    "    action = tokenizer.discrete_act(torch.stack(\n",
    "        [torch.stack([torch.tensor(boa_token_id), action, torch.tensor(eoa_token_id)]) \n",
    "        for action in torch.from_numpy(episode.actions)]\n",
    "    ))\n",
    "    return FourRoomsObservation(mission=mission, image=image, direction=direction, action=action)\n",
    "    \n",
    "def vqa_tokenizer(tokenizer, sample):\n",
    "    image = tokenizer.image(image_transform(pil_to_tensor(sample[\"image\"])))\n",
    "    answer = tokenizer.text(random.choice(sample[\"answers\"])[\"answer\"], padding=False)\n",
    "    question = tokenizer.text_obs(sample[\"question\"])\n",
    "    all_observations = GenericVQAObservation(question=question, image=image, answer=answer)\n",
    "    i = random.randint(0, all_observations.count - 1)\n",
    "    observation = all_observations[i]\n",
    "    i += 1\n",
    "    while i < all_observations.count and observation.size + observation[0].size < SEQUENCE_LENGTH:\n",
    "        observation = observation.combine(all_observations[i])\n",
    "        i += 1\n",
    "    return observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "38d670a2-e681-44f0-a108-ca3190c2e8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def minigrid_collate_fn(batch):\n",
    "    \"\"\"TODO: This isn't collation.\"\"\"\n",
    "    result = []\n",
    "    for sample in batch:\n",
    "        i = random.randint(0, sample.count - 1)\n",
    "        # Starting at that index, we'll continue adding observations to our context window until\n",
    "        # we run out of space.\n",
    "        step = sample[i]\n",
    "        i += 1\n",
    "        while i < sample.count and step.size + step[0].size < SEQUENCE_LENGTH:\n",
    "            step = step.combine(sample[i])\n",
    "            i += 1\n",
    "        result.append(step)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "27917b2f-aacf-4655-985a-5384d0f4bb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_tokenizer(tokenizer, text, bos_token='<|endoftext|>', eos_token='<|endoftext|>'):\n",
    "    return GenericTextObservation(text=tokenizer.text(bos_token + text + eos_token))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633d613d-ef9e-4add-9b04-0fe36dde97fd",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "312f9ba5-3195-4562-9f04-3ba50d4ba393",
   "metadata": {},
   "outputs": [],
   "source": [
    "def acquire_shakespeare_dataset():\n",
    "    temp_dir = tempfile.gettempdir()\n",
    "    shakespeare_filepath = Path(temp_dir)/\"shakespeare.txt\"\n",
    "    if not os.path.exists(shakespeare_filepath):\n",
    "        data_url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
    "        with open(shakespeare_filepath, 'w', encoding='utf-8') as f:\n",
    "            f.write(requests.get(data_url).text)\n",
    "    \n",
    "    with open(shakespeare_filepath, 'r', encoding='utf-8') as f:\n",
    "        data = f.read()\n",
    "\n",
    "    # Split the dataset into each character's lines.\n",
    "    # Continue taking lines until you have at least 250 words in the sample.\n",
    "    # Add that sample to the dataset.\n",
    "    characters_lines = re.split(r\"\\n\\s*\\n\", data.strip())\n",
    "    MIN_WORDS_PER_BATCH = 250\n",
    "    sample = [characters_lines[0]]\n",
    "    num_words_in_sample = len(characters_lines[0].split())\n",
    "    text_dataset = []\n",
    "    i = 1\n",
    "    while i < len(characters_lines):\n",
    "        if num_words_in_sample > MIN_WORDS_PER_BATCH:\n",
    "            text_dataset.append(\"\\n\\n\".join(sample))\n",
    "            num_words_in_sample -= len(sample[0].split())\n",
    "            sample = sample[1:]\n",
    "        sample += [characters_lines[i]]\n",
    "        num_words_in_sample += len(characters_lines[i].split())\n",
    "        i += 1\n",
    "\n",
    "    return text_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0bc2e24a-3114-4330-aaa3-e1f3b7063c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "minigrid_dataset = minari.load_dataset('D4RL/minigrid/fourrooms-v0', download=True)\n",
    "vqa_dataset = datasets.load_dataset(\"eihli/micro-ok-vqa\")\n",
    "shakespeare_dataset = acquire_shakespeare_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf258ea-2c58-46db-8089-98694c53fa3a",
   "metadata": {},
   "source": [
    "## Transform and Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bae1599c-4eec-453b-af2d-6cffe906c216",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQUENCE_LENGTH = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "98e0883f-5c10-40c1-b766-9a0ad9a9839f",
   "metadata": {},
   "outputs": [],
   "source": [
    "__text_tokenizer = GPT2Tokenizer.from_pretrained(\"openai-community/gpt2\", clean_up_tokenization_spaces=True)\n",
    "__text_tokenizer.pad_token = __text_tokenizer.eos_token\n",
    "_text_tokenizer = partial(\n",
    "    __text_tokenizer,\n",
    "    max_length=SEQUENCE_LENGTH,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4c9b01e0-e4db-469a-9f64-cc969e27b99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(_text_tokenizer)\n",
    "four_rooms_tokenize = partial(four_rooms_tokenizer, tokenizer)\n",
    "vqa_tokenize = partial(vqa_tokenizer, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bd9f8582-3d47-4dcd-9ea5-6fac980b61b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tokenize = partial(text_tokenizer, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "630c6912-5e40-4930-b294-be6d6ca55fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 453])\n",
      "torch.Size([6, 2]) torch.Size([6, 144, 768]) torch.Size([6, 144])\n"
     ]
    }
   ],
   "source": [
    "batch = [text_tokenize(x) for x in shakespeare_dataset[:3]]\n",
    "print(batch[0].text.tokens.shape)\n",
    "batch = minigrid_collate_fn(four_rooms_tokenize(minigrid_dataset[i]) for i in range(3))\n",
    "print(batch[0].action.tokens.shape, batch[0].image.tokens.shape, batch[0].image.attention_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c054f8d1-f4ca-404b-94b8-9244969ed53f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenericTextObservation(text=TextData(tokens=tensor([[50256, 31373,    11,   995]]), targets=tensor([[31373,    11,   995, 50256]]), attention_mask=tensor([[1, 1, 1, 1]]), embedding=tensor([])))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_tokenize('hello, world')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b03852e-8e6d-43a5-b948-9026bfc67173",
   "metadata": {},
   "source": [
    "## Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7f88bdb1-9ef9-47f4-b80a-09ca2241bcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "shakespeare_dataset_xf = TransformDataset(shakespeare_dataset, text_tokenize)\n",
    "minigrid_dataset_xf = TransformDataset(minigrid_dataset, four_rooms_tokenize)\n",
    "vqa_dataset_xf = TransformDataset(vqa_dataset['train'], vqa_tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca58132-e76b-4ad2-baf5-21df9e707a91",
   "metadata": {},
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f9e6367a-96f1-4799-ab03-ab87f728de2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "619eed52-4e50-4f40-afaf-8f7e86ea0efe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 453])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shakespeare_dataloader = DataLoader(shakespeare_dataset_xf, batch_size=BATCH_SIZE, collate_fn=lambda x: x)\n",
    "shakespeare_batch = next(iter(shakespeare_dataloader))\n",
    "shakespeare_batch[0].text.tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3ead6b39-d219-44ad-a336-6b9fe2315ad9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, torch.Size([6, 2]), torch.Size([6, 144, 768]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minigrid_dataloader = DataLoader(minigrid_dataset_xf, batch_size=BATCH_SIZE, collate_fn=minigrid_collate_fn)\n",
    "minigrid_batch = next(iter(minigrid_dataloader))\n",
    "len(minigrid_batch), minigrid_batch[0].mission.tokens.shape, minigrid_batch[0].image.tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "46748389-170e-4fd0-800d-0ab4d7e2fafe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 9]), torch.Size([1, 144, 768]), torch.Size([1, 2]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vqa_dataloader = DataLoader(vqa_dataset_xf, batch_size=BATCH_SIZE, collate_fn=lambda x: x)\n",
    "vqa_batch = next(iter(vqa_dataloader))\n",
    "vqa_batch[0].question.tokens.shape, vqa_batch[0].image.tokens.shape, vqa_batch[0].answer.targets.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52aebc21-96d0-452e-8b69-7173526b9160",
   "metadata": {},
   "source": [
    "# Embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "95e806b1-9d64-4bbf-8604-ce70793e2198",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# From section 2.2 of the Gato paper:\n",
    "#\n",
    "#    Tokens belonging to image patches for any time-step are embedded using a\n",
    "#    single ResNet (He et al., 2016a) block to obtain a vector per patch. For\n",
    "#    image patch token embeddings, we also add a learnable within-image position\n",
    "#    encoding vector.\n",
    "class ResNetV2Block(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, num_groups=24):\n",
    "        super(ResNetV2Block, self).__init__()\n",
    "        self.gn1 = nn.GroupNorm(1, in_channels)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size=3,\n",
    "            stride=stride,\n",
    "            padding=1,\n",
    "            bias=False,\n",
    "        )\n",
    "        self.gn2 = nn.GroupNorm(num_groups, out_channels)\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            out_channels, in_channels, kernel_size=3, stride=1, padding=1, bias=False\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, CHW = x.shape\n",
    "        # TODO: Remove these hardcoded values.\n",
    "        out = rearrange(x, 'b t (c h w) -> (b t) c h w', c=3, h=16)\n",
    "        out = self.gn1(out)\n",
    "        out = self.gelu(out)\n",
    "        out = self.conv1(out)\n",
    "        out = self.gn2(out)\n",
    "        out = self.gelu(out)\n",
    "        out = self.conv2(out)\n",
    "        return x + rearrange(out, '(b t) c h w -> b t (c h w)', b=B, t=T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "efc91d0f-dd03-450a-8207-9d5cb47e04c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Embedder:\n",
    "    discrete_embedding: Callable\n",
    "    image_embedding: Callable\n",
    "\n",
    "    def text(self, data):\n",
    "        return self.discrete_embedding(data)\n",
    "\n",
    "    def discrete(self, data):\n",
    "        return self.discrete_embedding(data)\n",
    "\n",
    "    def image(self, data):\n",
    "        return self.image_embedding(data)\n",
    "\n",
    "@dataclass\n",
    "class MiniGatoConfig:\n",
    "    embedding_dim: int\n",
    "    sequence_length: int\n",
    "    vocab_size: int \n",
    "    transformer_config: GPT2Config\n",
    "    transformer: GPT2Model\n",
    "\n",
    "def init_default_config() -> MiniGatoConfig:\n",
    "    transformer_config = GPT2Config()\n",
    "    return MiniGatoConfig(\n",
    "        embedding_dim=768,\n",
    "        sequence_length=1024,\n",
    "        vocab_size=__text_tokenizer.vocab_size,\n",
    "        transformer_config=transformer_config,\n",
    "        transformer=GPT2Model(transformer_config),\n",
    "    )\n",
    "\n",
    "default_config = init_default_config()\n",
    "\n",
    "class MiniGato(nn.Module):\n",
    "    def __init__(self, config: MiniGatoConfig=default_config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.sequence_length = self.config.sequence_length\n",
    "        # text + discrete + continuous (text and continuous _are_ discrete when you think about it)\n",
    "        discrete_embedding = nn.Embedding(self.config.vocab_size + 1024, self.config.embedding_dim)\n",
    "        image_embedding = ResNetV2Block(3, self.config.embedding_dim)\n",
    "        self.embedder = Embedder(discrete_embedding=discrete_embedding, image_embedding=image_embedding)\n",
    "        self.transformer = self.config.transformer\n",
    "        self.lm_head = nn.Linear(self.transformer.config.hidden_size, self.config.vocab_size + 1024)     \n",
    "\n",
    "    def forward(self, batch):\n",
    "        batch = [\n",
    "            sample.embed(self.embedder).sequence(self.sequence_length) for sample in batch\n",
    "        ]\n",
    "        xs, ys, ms = map(torch.stack, zip(*batch))\n",
    "        xs, ys, ms = [x.to(device) for x in [xs, ys, ms]]\n",
    "        out = self.transformer(inputs_embeds=xs)\n",
    "        predicted = self.lm_head(out.last_hidden_state)\n",
    "        return predicted, ys, ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b901d325-a96d-463d-88f6-31702432d715",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2e40fcf0-be16-4f91-89cf-17e8e1d4fa47",
   "metadata": {},
   "outputs": [],
   "source": [
    "minigrid_iterator = iter(minigrid_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bdf45b06-7b45-4379-90c7-ccd72f8e67e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "minigrid_batch = next(minigrid_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9e420165-26b3-4144-b11b-432223186e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infinite_dataloader(fn):\n",
    "    it = iter(fn())\n",
    "    while True:\n",
    "        try:\n",
    "            yield next(it)\n",
    "        except StopIteration:\n",
    "            it = iter(fn())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "101caa7d-1002-4dc0-851c-b771020928b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loss\n",
    "##\n",
    "## See section 2.3 of the Gato paper.\n",
    "##\n",
    "##   Let b index a training batch of sequences B. We define a masking function m\n",
    "##   such that m(b, l) = 1 if the token at index l is either from text or from\n",
    "##   the logged action of an agent, and 0 otherwise. The training loss for a\n",
    "##   batch B can then be written as...\n",
    "def cross_entropy(predicted, target, mask):\n",
    "    # See: https://youtu.be/kCc8FmEb1nY?list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ&t=1553\n",
    "    B, T, C = predicted.shape\n",
    "    predicted = predicted.view(B * T, C)\n",
    "    target = target.view(-1).to(torch.long)\n",
    "    losses = F.cross_entropy(predicted, target, reduction=\"none\")\n",
    "    losses = losses * mask.squeeze(-1).view(-1)\n",
    "    loss = losses.sum() / (mask.sum() + 1e-8)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "35d28ab7-f0d6-4a22-8065-ee253676c11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniGatoTrainer:\n",
    "    def __init__(self, model, optimizer, dataloaders, scheduler=None, lr=3e-4, num_iterations=10):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.dataloaders = dataloaders\n",
    "        self.scheduler = scheduler\n",
    "        self.dl_it = cycle(dataloaders)\n",
    "        self.losses = []\n",
    "        self.num_iterations = num_iterations\n",
    "        self.lr = lr\n",
    "\n",
    "    def train(self):\n",
    "        self.model.train()\n",
    "        for i in tqdm(range(self.num_iterations)):\n",
    "            dl = next(self.dl_it)\n",
    "            batch = next(dl)\n",
    "            optimizer.zero_grad()\n",
    "            predicted, targets, attention_mask = self.model(batch)\n",
    "            loss = cross_entropy(predicted, targets, attention_mask)\n",
    "            self.losses.append(loss.item())\n",
    "            loss.backward()\n",
    "            if self.scheduler:\n",
    "                self.scheduler.step()\n",
    "            self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "68a0a785-567a-42ca-a161-30deef63b579",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6c746035-3724-4d09-a2fd-3f49d1b4d295",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = [\n",
    "    infinite_dataloader(partial(DataLoader, minigrid_dataset_xf, batch_size=BATCH_SIZE, collate_fn=minigrid_collate_fn)),\n",
    "    infinite_dataloader(partial(DataLoader, shakespeare_dataset_xf, batch_size=BATCH_SIZE, collate_fn=lambda x: x)),\n",
    "    infinite_dataloader(partial(DataLoader, vqa_dataset_xf, batch_size=BATCH_SIZE, collate_fn=lambda x: x)),\n",
    "]\n",
    "dl_it = cycle(dataloaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3be9730a-e374-48a7-a674-875e966fba24",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = init_default_config()\n",
    "model = MiniGato(config).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988eb2e1-507a-4b56-b33e-bd26d78840ae",
   "metadata": {},
   "source": [
    "# Pre-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "743f54d1-3aeb-4926-9c04-516c713c3cf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|endoftext|>'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "__text_tokenizer.bos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "58426528-b3bd-493f-838b-8adc683c34ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenericTextObservation(text=TextData(tokens=tensor([[50256]]), targets=tensor([[50256]]), attention_mask=tensor([[1]]), embedding=tensor([])))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\n",
    "text_observation = text_tokenize(\"\")\n",
    "text_observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c33d772c-aad9-48f7-aee1-708e8a2056ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 768])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_embedding = text_observation.embed(model.embedder)\n",
    "text_embedding.text.embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "87e4a2a5-d0d6-485d-bc4c-e71d8ecfcbad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1024, 768]), torch.Size([1024]), tensor(1))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_sequence_tokens, text_sequence_targets, text_sequence_attention_mask = text_embedding.sequence(model.sequence_length)\n",
    "text_sequence_tokens.shape, text_sequence_targets.shape, text_sequence_attention_mask.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f25648ef-6701-4df2-8d27-548bb0417ed7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1024, 51281])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = [text_observation]\n",
    "predicted, targets, attention_mask = model(batch)\n",
    "predicted.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ff1ec0f5-c295-43d6-8a6c-b9419555c547",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([2.3962, 2.2500, 2.2314, 2.2211, 2.2108, 2.1482, 2.1481, 2.1359, 2.1135,\n",
       "        2.1098], device='cuda:0', grad_fn=<TopkBackward0>),\n",
       "indices=tensor([24577, 28412, 43163, 22805,   717, 36322, 40437, 35608, 13073, 39417],\n",
       "       device='cuda:0'))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted[0, text_observation.size].topk(k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e8f3b12e-b127-41ad-8e88-6f0517d281d0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'topk' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtopk\u001b[49m\u001b[38;5;241m.\u001b[39mindices\u001b[38;5;241m.\u001b[39mflatten()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'topk' is not defined"
     ]
    }
   ],
   "source": [
    "topk.indices.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c9394b-08ef-463b-bfcb-45376588b81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "topk = (predicted.exp() / predicted.exp().sum(dim=2, keepdims=True))[:, [text_observation.size]].topk(k=10, dim=2)\n",
    "print(f\"Top predicted tokens: {', '.join(__text_tokenizer.decode([x]) if x < __text_tokenizer.vocab_size else f\"<{str(x.item())}>\" for x in topk.indices.flatten())}\")\n",
    "print(f\"Token probabilities: {', '.join([f'{x:.4f}' for x in topk.values.flatten().tolist()])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39dca732-d20d-4ea4-83ff-09f5132aacb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7618f18e-0d30-4a54-840c-0a03f3c30990",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = minigrid_dataset[0]\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e0ff11-c49b-4d52-8df0-5a2d1414c657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's predict the action after the first 4 observations.\n",
    "fourrooms_observation = four_rooms_tokenize(sample)[:4]\n",
    "fourrooms_observation.action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9461ed-bd0d-46e5-964e-115f83714186",
   "metadata": {},
   "outputs": [],
   "source": [
    "fourrooms_observation.mission.tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db94e0fb-9584-49b3-a449-879d4eae1b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "fourrooms_embedding = fourrooms_observation.embed(model.embedder)\n",
    "fourrooms_embedding.action.embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0a7a7b-a91d-4d4a-be91-84e50c2244c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fourrooms_sequence_tokens, fourrooms_sequence_targets, fourrooms_sequence_attention_mask = fourrooms_embedding.sequence(model.sequence_length)\n",
    "fourrooms_sequence_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59d1336-c8f4-4ab2-9132-e21c068b2fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = [fourrooms_observation]\n",
    "predicted, targets, attention_mask = model(batch)\n",
    "predicted.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd55bfe-969d-4976-8e17-647e81d2f5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What's the index of the 4th action that we're trying to predict?\n",
    "fourrooms_observation.size, fourrooms_observation.action.tokens.size(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ad7c09-2807-4b3a-9f56-5d50e9545467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first token is the \"beginning of action\" token – a separator between the observations and actions.\n",
    "i = fourrooms_observation.size - fourrooms_observation.action.tokens.size(1)\n",
    "(model.embedder.discrete_embedding.weight.data @ fourrooms_sequence_tokens[i]).argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526b26c1-c752-4dc8-badc-e0a9885f4141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The next token is the first one we want to predict.\n",
    "(model.embedder.discrete_embedding.weight.data @ fourrooms_sequence_tokens[i+1]).argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456fdfc1-06a4-4a98-8247-e670c61840f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = [fourrooms_observation]\n",
    "predicted, targets, attention_mask = model(batch)\n",
    "predicted.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4a131d-2bea-4954-b3bf-041c5f79095c",
   "metadata": {},
   "outputs": [],
   "source": [
    "topk = (predicted.exp() / predicted.exp().sum(dim=2, keepdims=True))[:, [i]].topk(k=10, dim=2)\n",
    "print(f\"Top predicted tokens: {', '.join(f'{x}' for x in topk.indices.flatten())}\")\n",
    "print(f\"Token probabilities: {', '.join([f'{x:.4f}' for x in topk.values.flatten().tolist()])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914a5120-81c6-46ca-a3f3-db41561ba72e",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca0e9cf-e6fd-4e1f-bb71-14f0bc244eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_ITERATIONS = 100\n",
    "LR = 1e-3\n",
    "optimizer = torch.optim.AdamW(model.parameters())\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=NUM_ITERATIONS, eta_min=1e-5)\n",
    "trainer = MiniGatoTrainer(\n",
    "    model,\n",
    "    optimizer,\n",
    "    dataloaders,\n",
    "    num_iterations=NUM_ITERATIONS,\n",
    "    lr=LR,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac9f52f-6d9f-4fbe-85ac-be1338ff5839",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3ee547-0441-4697-8ef6-5c604f5bd496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = init_default_config()\n",
    "# model = MiniGato(config).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66cd5e7d-ac07-4ca6-afee-3124d1c491d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.losses[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acd56fc-67f0-4fb7-86ba-c3a71e501de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(trainer.losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3962376d-19da-4072-bea3-f69556e0ccbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 10\n",
    "data = torch.tensor(trainer.losses)\n",
    "moving_avg = torch.conv1d(\n",
    "    data.view(1, 1, data.size(0)), \n",
    "    torch.ones(1, 1, window_size) / window_size,\n",
    ").squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753ee6c9-94dc-422e-84c0-e8a5beeb49a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(moving_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5814617-ce4f-426b-ba13-049e7ffe536b",
   "metadata": {},
   "source": [
    "# Post-Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b499dfd-3c62-44e8-9224-10bce5abade0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = minigrid_dataset[0]\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb77588-2f24-4d1f-8a28-fadf3623c6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's predict the action after the first 4 observations.\n",
    "fourrooms_observation = four_rooms_tokenize(sample)[:4]\n",
    "fourrooms_observation.action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d5f8d5-e182-4712-bf12-d9063564881f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fourrooms_observation.mission.tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe851d87-a802-4063-b4da-1565b61178d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fourrooms_embedding = fourrooms_observation.embed(model.embedder)\n",
    "fourrooms_embedding.action.embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ba44a1-0905-42e8-8803-1e67f015cbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fourrooms_sequence_tokens, fourrooms_sequence_targets, fourrooms_sequence_attention_mask = fourrooms_embedding.sequence(model.sequence_length)\n",
    "fourrooms_sequence_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4bbb24-4600-46cd-a35d-e6e8e7f5c8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = [fourrooms_observation]\n",
    "predicted, targets, attention_mask = model(batch)\n",
    "predicted.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5912d274-2ee5-4b29-94b1-2ba40a943a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What's the index of the 4th action that we're trying to predict?\n",
    "fourrooms_observation.size, fourrooms_observation.action.tokens.size(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732aba8c-dfb7-43c7-ab45-998f4452f293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first token is the \"beginning of action\" token – a separator between the observations and actions.\n",
    "i = fourrooms_observation.size - fourrooms_observation.action.tokens.size(1)\n",
    "(model.embedder.discrete_embedding.weight.data @ fourrooms_sequence_tokens[i]).argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a909f9ab-11e3-4770-ac21-dd563885e413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The next token is the first one we want to predict.\n",
    "(model.embedder.discrete_embedding.weight.data @ fourrooms_sequence_tokens[i+1]).argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b456249b-679b-4b1e-8268-1a4f5b22b953",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = [fourrooms_observation]\n",
    "predicted, targets, attention_mask = model(batch)\n",
    "predicted.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4710e558-fb8e-479a-b2b3-6fe9c81abcb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "topk = (predicted.exp() / predicted.exp().sum(dim=2, keepdims=True))[:, [i]].topk(k=10, dim=2)\n",
    "print(f\"Top predicted tokens: {', '.join(f'{x}' for x in topk.indices.flatten())}\")\n",
    "print(f\"Token probabilities: {', '.join([f'{x:.4f}' for x in topk.values.flatten().tolist()])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe29e5e4-81fe-4120-af4e-2b1c0269bbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It _should_ be predicting 1, but it's predicting 2 (50259 - 50257).\n",
    "# 2 is such a common action that it can get a low loss by just predicting 2 for everything.\n",
    "# I would expect with more training, the probability for 1 will go up, \n",
    "# even if the `topk` still predicts 2 for a long time to come.\n",
    "[minigrid_dataset[i].actions for i in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa98fbbf-8642-40d5-9873-4a0b660c6cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = minigrid_dataset[100]\n",
    "fourrooms_observation = four_rooms_tokenize(sample)[:4]\n",
    "batch = [fourrooms_observation]\n",
    "predicted, targets, attention_mask = model(batch)\n",
    "topk = (predicted.exp() / predicted.exp().sum(dim=2, keepdims=True))[:, [i]].topk(k=10, dim=2)\n",
    "print(f\"Target token: {fourrooms_observation.action.tokens}\")\n",
    "print(f\"Top predicted tokens: {', '.join(f'{x}' for x in topk.indices.flatten())}\")\n",
    "print(f\"Token probabilities: {', '.join([f'{x:.4f}' for x in topk.values.flatten().tolist()])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c192ada-dfb9-4e91-a38e-9bc499bef604",
   "metadata": {},
   "source": [
    "```\n",
    "Target token: tensor([[51280, 50259],\n",
    "        [51280, 50259],\n",
    "        [51280, 50259],\n",
    "        [51280, 50257]])\n",
    "Top predicted tokens: 50259, 50257, 50258, 44558, 42175, 53, 2879, 3539, 321, 45\n",
    "Token probabilities: 0.8277, 0.0258, 0.0069, 0.0056, 0.0041, 0.0037, 0.0032, 0.0022, 0.0020, 0.0019\n",
    "```\n",
    "\n",
    "If you run a few more training iterations, you should see 0.0258 go up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912def86-6f79-42e9-9858-65a68e6c38a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b0cc6863-b6f7-44ff-b109-15114c56b8ab",
   "metadata": {},
   "source": [
    "# More Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1a1c7f23-ea4a-45c9-bc97-b0b6f924fa26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# minigrid_dataset = minari.load_dataset('D4RL/minigrid/fourrooms-v0', download=True)\n",
    "# vqa_dataset = datasets.load_dataset(\"eihli/micro-ok-vqa\")\n",
    "# shakespeare_dataset = acquire_shakespeare_dataset()\n",
    "pointmaze_dataset = minari.load_dataset('D4RL/pointmaze/open-v2', download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "be88e9dd-88c2-48ee-b5dd-6008677b148f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointMazeObservation(Observation):\n",
    "    observation: DiscreteData\n",
    "    goal: DiscreteData\n",
    "\n",
    "def pointmaze_tokenizer(tokenizer, episode, boa_token_id=1023, eoa_token_id=1023):\n",
    "    # There is always 1 more observation than there are actions,\n",
    "    # the \"terminated\" observation, which we don't care about.\n",
    "    # So take up to the :-1 of everything other than the actions.\n",
    "    mission = tokenizer.text_obs(episode.observations['mission'][:-1], padding=False)\n",
    "    observation = tokenizer.continuous_obs(episode.observations[\"observation\"])\n",
    "    goal = tokenizer.continuous_obs(episode.observations[\"desired_goal\"])\n",
    "    direction = tokenizer.discrete_obs(torch.from_numpy(episode.observations['direction'])[:-1])\n",
    "    action = tokenizer.continuous_act(torch.stack(\n",
    "        [torch.stack([torch.tensor(boa_token_id), action, torch.tensor(eoa_token_id)]) \n",
    "        for action in torch.from_numpy(episode.actions)]\n",
    "    ))\n",
    "    return PointMazeObservation(mission=mission, image=image, direction=direction, action=action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2a69aafc-b0ca-4ba5-af08-dcfca89a611f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-100,  -99,  -98,  -97,  -96])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.0119, 0.0316, 0.0861, 0.2341, 0.6364])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = torch.arange(-100, -95)\n",
    "#q = q-q.max()\n",
    "print(q)\n",
    "probs = q.exp() / q.exp().sum()\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fce509ca-bc78-473e-bd35-e051e2caf878",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.15781409, -1.16204705,  0.        ,  0.        ],\n",
       "       [-2.15543246, -1.15966541,  0.23816384,  0.23816384],\n",
       "       [-2.15067485, -1.15490781,  0.47576046,  0.47576046]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pointmaze_dataset[0].observations[\"observation\"][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b2e3f4ed-7364-4226-b697-a4597a24bd73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EpisodeData(id=0, total_steps=45, observations={achieved_goal: ndarray of shape (46, 2) and dtype float64, desired_goal: ndarray of shape (46, 2) and dtype float64, observation: ndarray of shape (46, 4) and dtype float64}, actions=ndarray of shape (45, 2) and dtype float32, rewards=ndarray of 45 floats, terminations=ndarray of 45 bools, truncations=ndarray of 45 bools, infos=dict with the following keys: ['goal', 'qpos', 'qvel', 'success'])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pointmaze_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "48605df4-75bb-427f-ad4c-e973913951ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.15781409, -1.16204705],\n",
       "       [-2.15543246, -1.15966541],\n",
       "       [-2.15067485, -1.15490781]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pointmaze_dataset[0].observations['achieved_goal'][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7acf9e55-601d-4715-9eac-2c048182391b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.22308949,  0.86017994],\n",
       "       [-2.22308949,  0.86017994],\n",
       "       [-2.22308949,  0.86017994]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pointmaze_dataset[0].observations['desired_goal'][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "26429547-d400-44f4-829f-22a68463da08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 1.        ],\n",
       "       [1.        , 1.        ],\n",
       "       [0.74168795, 1.        ]], dtype=float32)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pointmaze_dataset[0].actions[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9db8768-7650-4bb5-afb0-74e1640b6b6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7520f46-69f6-40d2-b3d4-6dcca0a6b0ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a5d15b-c34c-486a-b60c-69b2aae2a734",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862fecb5-c238-439b-8252-42f362b58358",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1515a4c2-6b35-489b-adb2-482eaf0615cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "714453a9-5116-4633-be17-7fe0536163c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vqa_batch = next(iter(DataLoader(vqa_dataset_xf, batch_size=BATCH_SIZE, collate_fn=lambda x: x, num_workers=4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "67122707-b677-4e8a-b0e9-c87d0563ef84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1024, 768])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = vqa_batch[0].embed(model.embedder)\n",
    "emb.question.embedding.shape, emb.image.embedding.shape, emb.answer.embedding.shape\n",
    "cc = torch.concat([emb.question.embedding, emb.image.embedding, emb.answer.embedding], dim=1)\n",
    "F.pad(cc, (0, 0, 0, 1024 - cc.size(1)), value=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "addb2b50-3530-4931-87ea-688515668def",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1024, 768]), torch.Size([1024]))"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vqa_batch[0].embed(model.embedder).sequence(1024)[0].shape, vqa_batch[0].embed(model.embedder).sequence(1024)[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "790bbdef-0ece-4e1a-a0dd-26d28a21d5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = iter(minigrid_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7b5c9d72-fc5f-4360-a39f-a47d6de39283",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "2a3742cf-5d46-404e-9b2a-17b7edc09110",
   "metadata": {},
   "outputs": [],
   "source": [
    "ep = batch\n",
    "em = [e.embed(model.embedder) for e in ep]\n",
    "sq = [e.sequence(model.sequence_length) for e in em]\n",
    "xs, ys, ms = map(torch.stack, zip(*sq))\n",
    "pr, ys, ms = model(ep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "9317da5e-6591-4e77-b390-273c5a305c00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DiscreteData(tokens=tensor([[51280, 50259],\n",
       "        [51280, 50258],\n",
       "        [51280, 50259],\n",
       "        [51280, 50259],\n",
       "        [51280, 50259],\n",
       "        [51280, 50259]]), targets=tensor([[50259, 51280],\n",
       "        [50258, 51280],\n",
       "        [50259, 51280],\n",
       "        [50259, 51280],\n",
       "        [50259, 51280],\n",
       "        [50259, 51280]]), attention_mask=tensor([[1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1]]), embedding=tensor([]))"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ep[0].action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "047b052c-1419-497f-92ed-865e02db0f1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(882,\n",
       " 6,\n",
       " tensor([], size=(6, 0), dtype=torch.int64),\n",
       " torch.Size([4, 1024, 50257]),\n",
       " torch.Size([4, 1024]))"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ep[0].size, ep[0].size // ep[0][0].size, ep[-1].action.targets, pr.shape, ys.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "e26cddd2-66d4-4f39-b1f8-9ddfb1823449",
   "metadata": {},
   "outputs": [],
   "source": [
    "heat = 0.01\n",
    "prh = pr / heat\n",
    "sm = prh.softmax(dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "946e2e9b-3f56-4704-a1e8-f653a22f765f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1024, 50257])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86b0239-383d-45a5-8466-28310303f819",
   "metadata": {},
   "source": [
    "Which indexes do you want? Not the 1023, because of padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "cef167e1-0158-4db4-9db8-adc1a8e0b6e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 1024, 50257]),\n",
       " torch.Size([4, 1024]),\n",
       " torch.Size([0, 1]),\n",
       " tensor([], device='cuda:0', size=(0, 1), dtype=torch.int64))"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr.shape, ms.shape, ms[0].nonzero().shape, ms[0].nonzero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "c6381f89-2bb7-42e3-8f89-77bdfb2df5dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([], device='cuda:0', dtype=torch.int32),\n",
       " tensor([], device='cuda:0', dtype=torch.int32),\n",
       " tensor([], device='cuda:0', dtype=torch.int32),\n",
       " tensor([], device='cuda:0', dtype=torch.int32)]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ys[batch_index][ms[batch_index].nonzero().flatten()].to(torch.int) for batch_index in range(len(batch))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "78896ef4-811c-4031-b828-bd6ea346f049",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([], device='cuda:0', dtype=torch.int32),\n",
       " tensor([], device='cuda:0', dtype=torch.int32),\n",
       " tensor([], device='cuda:0', dtype=torch.int32),\n",
       " tensor([], device='cuda:0', dtype=torch.int32)]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[pr[batch_index][ms[batch_index].nonzero().flatten()].argmax(dim=1).to(torch.int) for batch_index in range(len(batch))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "624c8593-d98d-4e74-87fc-c9581f1f4db3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([],\n",
       " tensor([-9.6881, -8.5413, -9.0677], device='cuda:0', grad_fn=<IndexBackward0>))"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ms[0].nonzero().flatten().tolist(), pr[0][0, [1, 2, 1023]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "c18f1d46-f2f2-418f-8095-26d6e83dda2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4399],\n",
       "        [4399],\n",
       "        [ 504],\n",
       "        [ 377]], device='cuda:0')"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sl = torch.multinomial(sm[:, 893, :], num_samples=1)\n",
    "sl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "d26dda6b-369f-4804-9484-0650618e3e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_text():\n",
    "    model.eval()\n",
    "    text = \"First Citizen:\"\n",
    "    next_word_token = None\n",
    "    i = 0\n",
    "    while i < 20 and next_word_token != __text_tokenizer.eos_token:\n",
    "        with torch.no_grad():\n",
    "            tokens = text_tokenize(text)\n",
    "            x = tokens.embed(model.embedder).to(device)\n",
    "            pr, ys, ms = model([tokens])\n",
    "            heat = 0.5\n",
    "            prh = pr / heat\n",
    "            sm = prh.softmax(dim=2)\n",
    "            last_index = ms.nonzero()[-1].cpu()[1]\n",
    "            next_word_probs = sm[0, last_index-1]\n",
    "            next_word_token = torch.multinomial(next_word_probs, num_samples=1)\n",
    "            next_word = __text_tokenizer.decode(next_word_token)\n",
    "            text += next_word\n",
    "        i += 1\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "e9ebf0e2-653e-46e6-ab13-3da255fd02ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen: were\n",
      ":\n",
      "\n",
      "\n",
      ",I\n",
      "USI:US\n",
      ":Now Cor, you\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(eval_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "6a7cd0c1-ddcd-4147-a8ec-391f12069386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Control targets:   []\n",
      "Control predicted: []\n"
     ]
    }
   ],
   "source": [
    "targets = [ys[batch_index][ms[batch_index].nonzero().flatten()].to(torch.int) for batch_index in range(len(batch))][0]\n",
    "predicted = [pr[batch_index][ms[batch_index].nonzero().flatten()].argmax(dim=1).to(torch.int) for batch_index in range(len(batch))][0]\n",
    "print(f\"Control targets:   {targets.tolist()}\\nControl predicted: {predicted.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "47af1818-be4d-4a83-a86d-bb6f324c484f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024, 50257])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm[0].shape["
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8670300c-b303-4d7c-83ff-9f7028f9a3c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "20da0246-8084-48ab-8f17-69b7c448a84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vqa_sample = next(iter(vqa_dataset[\"train\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "1b6d28f9-2803-468d-a63e-c5c84747aecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "vqa_batch = vqa_tokenize(vqa_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "dd54483f-1db6-4d93-86d0-6f1fe0b12f14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VQATimestep(question=TextTokenData(tokens=tensor([[ 2061,   318,   262, 42658,  2349,   286,   262, 32749,  1444,    30]]), targets=tensor([[ 2061,   318,   262, 42658,  2349,   286,   262, 32749,  1444,    30]]), attention_mask=tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), embedding=tensor([])), image=ImageTokenData(tokens=tensor([[[-0.2027, -0.2171, -0.2428,  ...,  0.2012,  0.2362,  0.2887],\n",
       "         [-0.1774, -0.1694, -0.1800,  ..., -0.0863, -0.0728, -0.0485],\n",
       "         [-0.1409, -0.1409, -0.1700,  ...,  0.0578,  0.0631,  0.0739],\n",
       "         ...,\n",
       "         [ 0.0972,  0.0924,  0.0732,  ...,  0.2398,  0.2446,  0.2495],\n",
       "         [ 0.1705,  0.1728,  0.1752,  ...,  0.2814,  0.2162, -0.0036],\n",
       "         [-0.0532, -0.0909, -0.1568,  ...,  0.2024,  0.1832,  0.1880]]]), targets=tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), attention_mask=tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), embedding=tensor([])), answer=TextTokenData(tokens=tensor([[  79, 1647]]), targets=tensor([[1647, 7894]]), attention_mask=tensor([[1, 1]]), embedding=tensor([])))"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vqa_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "6ef0608e-1851-43dd-99f9-1f2d782da6e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VQATimestep(question=TextTokenData(tokens=tensor([[ 2061,   318,   262,  4190,  3918,   286,   262, 21541,  1444,    30]]), targets=tensor([[ 2061,   318,   262,  4190,  3918,   286,   262, 21541,  1444,    30]]), attention_mask=tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), embedding=tensor([[[-0.5805, -0.0026, -2.2463,  ...,  0.1824,  0.5532, -0.5343],\n",
       "         [ 1.5129, -0.6796,  0.2971,  ..., -0.0411,  0.3358,  0.3959],\n",
       "         [ 0.9331,  1.9109, -0.0039,  ...,  0.5315, -0.7628, -0.4911],\n",
       "         ...,\n",
       "         [ 0.2087, -0.2230, -0.0372,  ...,  0.1041,  0.7136,  1.8701],\n",
       "         [ 0.0037,  0.4166, -1.3697,  ...,  0.3799,  1.1672,  0.9124],\n",
       "         [-0.0659, -0.4796,  0.5309,  ..., -0.6050, -0.4605, -0.5341]]],\n",
       "       grad_fn=<EmbeddingBackward0>)), image=ImageTokenData(tokens=tensor([[[-0.1049, -0.1489, -0.1169,  ...,  0.2318,  0.2277,  0.2562],\n",
       "         [-0.2601, -0.2411, -0.2474,  ...,  0.2021,  0.1989,  0.1925],\n",
       "         [-0.1489, -0.1873, -0.2092,  ...,  0.2413,  0.2357,  0.2329],\n",
       "         ...,\n",
       "         [ 0.0734,  0.0734,  0.0682,  ..., -0.2781, -0.2623, -0.2728],\n",
       "         [ 0.2887,  0.2654,  0.2136,  ..., -0.1990, -0.2096, -0.2201],\n",
       "         [-0.0534,  0.0305,  0.0305,  ..., -0.2716, -0.2683, -0.2486]]]), targets=tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), attention_mask=tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), embedding=tensor([[[ 0.2243,  0.1535,  0.0857,  ..., -0.1870, -0.0162, -0.0964],\n",
       "         [-0.0652, -0.0719, -0.0292,  ..., -0.2037, -0.0725, -0.1617],\n",
       "         [ 0.1381,  0.0451, -0.0297,  ..., -0.1500, -0.0641, -0.1657],\n",
       "         ...,\n",
       "         [ 0.0687,  0.0758,  0.0967,  ..., -0.6015, -0.6149, -0.5098],\n",
       "         [ 0.1730,  0.3239,  0.4082,  ..., -0.4999, -0.5908, -0.5058],\n",
       "         [-0.1170, -0.1532, -0.1406,  ..., -0.7078, -0.7351, -0.5654]]],\n",
       "       grad_fn=<AddBackward0>)), answer=TextTokenData(tokens=tensor([[21943, 50256, 50256,  ..., 50256, 50256, 50256]]), targets=tensor([[50256, 50256, 50256,  ..., 50256, 50256, 50256]]), attention_mask=tensor([[1, 0, 0,  ..., 0, 0, 0]]), embedding=tensor([[[-0.0024, -1.4722, -1.0243,  ..., -1.9218,  0.6434, -0.9012],\n",
       "         [ 1.2994, -0.2619,  0.0363,  ...,  0.3679,  0.8233, -2.0763],\n",
       "         [ 1.2994, -0.2619,  0.0363,  ...,  0.3679,  0.8233, -2.0763],\n",
       "         ...,\n",
       "         [ 1.2994, -0.2619,  0.0363,  ...,  0.3679,  0.8233, -2.0763],\n",
       "         [ 1.2994, -0.2619,  0.0363,  ...,  0.3679,  0.8233, -2.0763],\n",
       "         [ 1.2994, -0.2619,  0.0363,  ...,  0.3679,  0.8233, -2.0763]]],\n",
       "       grad_fn=<EmbeddingBackward0>)))"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = tokenizer.text_obs(\"What is the hair style of the blonde called?\")\n",
    "image = tokenizer.image(image_transform(vqa_dataset['train'][0]['image']))\n",
    "answer_text = \"foo\"\n",
    "answer = tokenizer.text_gen(answer_text)\n",
    "timestep = VQATimestep(question=question, image=image, answer=answer)\n",
    "timestep.embed(model.embedder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "301539cc-549c-4f07-b8dc-a054755caf44",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "question = tokenizer.text_obs(\"What is the hair style of the blonde called?\")\n",
    "image = tokenizer.image(image_transform(vqa_dataset['train'][0]['image']))\n",
    "answer_text = __text_tokenizer.bos_token\n",
    "answer = tokenizer.text_gen(answer_text)\n",
    "timestep = VQATimestep(question=question, image=image, answer=answer)\n",
    "next_word_token = None\n",
    "i = 0\n",
    "with torch.no_grad():\n",
    "    pr, ys, ms = model([timestep])\n",
    "    heat = 0.1\n",
    "    prh = pr / heat\n",
    "    sm = prh.softmax(dim=2)\n",
    "    last_index = ms.nonzero()[-1].cpu()[1]\n",
    "    next_word_probs = sm[0, last_index-1]\n",
    "    next_word_token = torch.multinomial(next_word_probs, num_samples=1)\n",
    "    next_word = __text_tokenizer.decode(next_word_token)\n",
    "    answer_text += next_word\n",
    "    answer = tokenizer.text_gen(answer_text, padding=False)\n",
    "    timestep = VQATimestep(question=question, image=image, answer=answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "9e20cfee-0ae1-4990-82d0-e8882ed0a6a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|endoftext|> park'"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf779b50-1c11-47e9-af0b-3a8598940652",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "75fcb31f-fea7-4a8c-ab7a-4f844dddac14",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs, ys, ms = timestep.embed(model.embedder).sequence(1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "c4498c5f-b18b-477b-9905-99c2ff6fc56b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1024, 768]), torch.Size([1024]), torch.Size([1024]))"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs.shape, ys.shape, ms.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "271ba462-626b-4970-8466-6b67aab0745f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[154]])"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = timestep.embed(model.embedder)\n",
    "ms.nonzero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "92d409a0-2edb-4620-b842-06c329b9f9b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 10, 768]), torch.Size([1, 144, 768]))"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb.question.embedding.shape, emb.image.embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "bcea1401-9c58-491c-9469-b357f0caa9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_vqa():\n",
    "    model.eval()\n",
    "    question = tokenizer.text_obs(\"What is the hair style of the blonde called?\")\n",
    "    image = tokenizer.image(image_transform(vqa_dataset['train'][0]['image']))\n",
    "    answer_text = __text_tokenizer.bos_token\n",
    "    answer = tokenizer.text_gen(answer_text)\n",
    "    timestep = VQATimestep(question=question, image=image, answer=answer)\n",
    "    next_word_token = None\n",
    "    i = 0\n",
    "    while i < 20 and next_word_token != __text_tokenizer.eos_token:\n",
    "        with torch.no_grad():\n",
    "            pr, ys, ms = model([timestep])\n",
    "            heat = 0.7\n",
    "            prh = pr / heat\n",
    "            sm = prh.softmax(dim=2)\n",
    "            last_index = ms.nonzero()[-1].cpu()[1]\n",
    "            next_word_probs = sm[0, last_index-1]\n",
    "            next_word_token = torch.multinomial(next_word_probs, num_samples=1)\n",
    "            next_word = __text_tokenizer.decode(next_word_token)\n",
    "            answer_text += next_word\n",
    "            answer = tokenizer.text_gen(answer_text, padding=False)\n",
    "            timestep = VQATimestep(question=question, image=image, answer=answer)\n",
    "        i += 1\n",
    "    return timestep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "2bbb29a9-8aaa-4fa5-bdc8-a56133cec03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = eval_vqa()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "c821e96c-8979-415b-af27-83796d37d191",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.answer.tokens[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "5603b93b-d38c-4993-91fc-13cb0c932d31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|endoftext|>agle sign southil signilgeapportotionaglegeWas andgeilalamelBoth'"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "__text_tokenizer.decode(result.answer.tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b11738a2-9909-4355-a24f-b5728a4b908a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 2, 50257])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "af91e8a1-e5ba-490a-aec1-0e13d278a3dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0],\n",
       "        [0, 1],\n",
       "        [0, 2],\n",
       "        [0, 3],\n",
       "        [0, 4]], device='cuda:0')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ms.nonzero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d041aebe-16d8-41bc-9170-aca57dba4e51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 4])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ms.nonzero()[-1].cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8d1ec768-5107-4489-ac74-95a9bb76d429",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(25)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens.text.tokens[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "61b0de11-c3c0-4168-ab9f-2b83b6d07ee5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 1024, 50257]), tensor([0, 4], device='cuda:0'))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm.shape, last_index[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e86351-6897-4885-a88f-513c595c4b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "    tokens = tokenize_text([text], max_length=SEQUENCE_LENGTH, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "    x = embed_text(tokens[\"input_ids\"]).to(device)\n",
    "    m = tokens[\"attention_mask\"].to(device)\n",
    "    length = m.sum().item()\n",
    "    o = model(inputs_embeds=x)\n",
    "    predicted = lm_head(o.last_hidden_state)\n",
    "    chosen = torch.multinomial(predicted.softmax(dim=2)[:, length-1], num_samples=1)\n",
    "    token = chosen[0]\n",
    "    text += _text_tokenizer.decode(chosen[0])\n",
    "    i += 1\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194e4ad7-95e4-4a27-a683-d221ad1ce0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "text = \"First Citizen:\"\n",
    "token = None\n",
    "i = 0\n",
    "with torch.no_grad():\n",
    "    while i < 20 and token != _text_tokenizer.eos_token_id:\n",
    "        tokens = tokenize_text([text], max_length=SEQUENCE_LENGTH, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "        x = embed_text(tokens[\"input_ids\"]).to(device)\n",
    "        m = tokens[\"attention_mask\"].to(device)\n",
    "        length = m.sum().item()\n",
    "        o = model(inputs_embeds=x)\n",
    "        predicted = lm_head(o.last_hidden_state)\n",
    "        chosen = torch.multinomial(predicted.softmax(dim=2)[:, length-1], num_samples=1)\n",
    "        token = chosen[0]\n",
    "        text += _text_tokenizer.decode(chosen[0])\n",
    "        i += 1\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "15942866-1fac-44c0-a0e3-8b0a1eafade1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '_text_tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[183], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m token \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 11\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m i \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m token \u001b[38;5;241m!=\u001b[39m \u001b[43m_text_tokenizer\u001b[49m\u001b[38;5;241m.\u001b[39meos_token:\n\u001b[1;32m     12\u001b[0m         x, y, m \u001b[38;5;241m=\u001b[39m sequence_vqa(tokenize_text, embed_text, tokenize_image, embed_image, batch)\n\u001b[1;32m     13\u001b[0m         x, y, m \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(device), y\u001b[38;5;241m.\u001b[39mto(device), m\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[0;31mNameError\u001b[0m: name '_text_tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "batch = {\n",
    "    \"question\": question,\n",
    "    \"image\": image,\n",
    "    \"answer\": [\"\"],\n",
    "}\n",
    "i = 0\n",
    "token = \"\"\n",
    "with torch.no_grad():\n",
    "    while i < 10 and token != _text_tokenizer.eos_token:\n",
    "        x, y, m = sequence_vqa(tokenize_text, embed_text, tokenize_image, embed_image, batch)\n",
    "        x, y, m = x.to(device), y.to(device), m.to(device)\n",
    "        o = model(inputs_embeds=x)\n",
    "        predicted = lm_head(o.last_hidden_state)\n",
    "        token = _text_tokenizer.decode(predicted.softmax(dim=2)[0].multinomial(num_samples=1).squeeze(1)[768+i])\n",
    "        token = _text_tokenizer.decode(predicted.argmax(dim=2).squeeze(0)[768+i])\n",
    "        # with temperature\n",
    "        heat = 0.1\n",
    "        heated = predicted / heat\n",
    "        token = _text_tokenizer.decode(heated.softmax(dim=2)[0].multinomial(num_samples=1).squeeze()[768+i])\n",
    "        batch[\"answer\"][0] += token\n",
    "        i += 1\n",
    "batch[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba529169-b327-4b7e-a262-5325f4d7a08f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
