{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f4d224e8-dfc7-4372-90af-521e8408d462",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Episode:\n",
    "    pass\n",
    "\n",
    "class TextEpisode(Episode):\n",
    "    def __init__(self, observations):\n",
    "        self.observations = observations\n",
    "\n",
    "class VQAEpisode(Episode):\n",
    "    pass\n",
    "\n",
    "class AgentEpisode(Episode):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "21eefb9d-46de-441d-9f2b-55e028d33798",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Step:\n",
    "    pass\n",
    "\n",
    "class TextEpisode(Episode):\n",
    "    def __init__(self, observations):\n",
    "        self.observations = observations\n",
    "\n",
    "class VQAEpisode(Episode):\n",
    "    pass\n",
    "\n",
    "class AgentEpisode(Episode):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "405b344d-9c53-497f-b7bf-ceb5583ff4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Observation:\n",
    "    def __init__(self, modalities):\n",
    "        self.modalities = modalities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0a84bebd-a02f-4bc8-961f-552e8df769e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "id": "5b253df3-3302-49f1-bc1e-107e75d13582",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from operator import attrgetter\n",
    "import torch.nn.functional as F\n",
    "\n",
    "@dataclass\n",
    "class Data:\n",
    "    tokens: torch.Tensor\n",
    "    targets: torch.Tensor\n",
    "    attention_mask: torch.Tensor\n",
    "\n",
    "    def stack(self, other, pad_token=torch.tensor(0)):\n",
    "        padded = min([self, other], key=lambda x: len(x.tokens))\n",
    "        other = max([self, other], key=lambda x: len(x.tokens))\n",
    "        pad_by = len(other.tokens) - len(padded.tokens)\n",
    "        padded = type(padded)(\n",
    "            tokens=F.pad(padded.tokens, (0, 0, 0, pad_by), value=pad_token),\n",
    "            targets=F.pad(padded.targets, (0, 0, 0, pad_by), value=pad_token),\n",
    "            attention_mask=F.pad(padded.attention_mask, (0, pad_by), value=pad_token),\n",
    "        )\n",
    "        return type(self)(\n",
    "            tokens=torch.stack([padded.tokens, other.tokens]),\n",
    "            targets=torch.stack([padded.targets, other.targets]),\n",
    "            attention_mask=torch.stack([padded.attention_mask, other.attention_mask])\n",
    "        )\n",
    "\n",
    "class TextData(Data):\n",
    "    pass\n",
    "\n",
    "class ImageData(Data):\n",
    "    pass\n",
    "\n",
    "class DiscreteData(Data):\n",
    "    pass\n",
    "\n",
    "class ContinuousData(Data): \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "id": "d7a820ba-922c-4ca1-8236-efda6be90f94",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Padding length should be less than or equal to two times the input dimension but got padding length 4 and input of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[626], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m a \u001b[38;5;241m=\u001b[39m TextData(tokens\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m4\u001b[39m), targets\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m4\u001b[39m), attention_mask\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mones((\u001b[38;5;241m4\u001b[39m,)))\n\u001b[1;32m      2\u001b[0m b \u001b[38;5;241m=\u001b[39m TextData(tokens\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m5\u001b[39m), targets\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m5\u001b[39m), attention_mask\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mones((\u001b[38;5;241m5\u001b[39m,)))\n\u001b[0;32m----> 3\u001b[0m c \u001b[38;5;241m=\u001b[39m \u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m c\u001b[38;5;241m.\u001b[39mtokens\u001b[38;5;241m.\u001b[39mshape, c\u001b[38;5;241m.\u001b[39mtargets\u001b[38;5;241m.\u001b[39mshape, c\n",
      "Cell \u001b[0;32mIn[625], line 16\u001b[0m, in \u001b[0;36mData.stack\u001b[0;34m(self, other, pad_token)\u001b[0m\n\u001b[1;32m     13\u001b[0m other \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m([\u001b[38;5;28mself\u001b[39m, other], key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mlen\u001b[39m(x\u001b[38;5;241m.\u001b[39mtokens))\n\u001b[1;32m     14\u001b[0m pad_by \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(other\u001b[38;5;241m.\u001b[39mtokens) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(padded\u001b[38;5;241m.\u001b[39mtokens)\n\u001b[1;32m     15\u001b[0m padded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(padded)(\n\u001b[0;32m---> 16\u001b[0m     tokens\u001b[38;5;241m=\u001b[39m\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpadded\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_by\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_token\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m     17\u001b[0m     targets\u001b[38;5;241m=\u001b[39mF\u001b[38;5;241m.\u001b[39mpad(padded\u001b[38;5;241m.\u001b[39mtargets, (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, pad_by), value\u001b[38;5;241m=\u001b[39mpad_token),\n\u001b[1;32m     18\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mF\u001b[38;5;241m.\u001b[39mpad(padded\u001b[38;5;241m.\u001b[39mattention_mask, (\u001b[38;5;241m0\u001b[39m, pad_by), value\u001b[38;5;241m=\u001b[39mpad_token),\n\u001b[1;32m     19\u001b[0m )\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)(\n\u001b[1;32m     21\u001b[0m     tokens\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mstack([padded\u001b[38;5;241m.\u001b[39mtokens, other\u001b[38;5;241m.\u001b[39mtokens]),\n\u001b[1;32m     22\u001b[0m     targets\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mstack([padded\u001b[38;5;241m.\u001b[39mtargets, other\u001b[38;5;241m.\u001b[39mtargets]),\n\u001b[1;32m     23\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mstack([padded\u001b[38;5;241m.\u001b[39mattention_mask, other\u001b[38;5;241m.\u001b[39mattention_mask])\n\u001b[1;32m     24\u001b[0m )\n",
      "File \u001b[0;32m~/.virtualenvs/neko/lib/python3.12/site-packages/torch/nn/functional.py:4552\u001b[0m, in \u001b[0;36mpad\u001b[0;34m(input, pad, mode, value)\u001b[0m\n\u001b[1;32m   4545\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreplicate\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m   4546\u001b[0m             \u001b[38;5;66;03m# Use slow decomp whose backward will be in terms of index_put.\u001b[39;00m\n\u001b[1;32m   4547\u001b[0m             \u001b[38;5;66;03m# importlib is required because the import cannot be top level\u001b[39;00m\n\u001b[1;32m   4548\u001b[0m             \u001b[38;5;66;03m# (cycle) and cannot be nested (TS doesn't support)\u001b[39;00m\n\u001b[1;32m   4549\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtorch._decomp.decompositions\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39m_replication_pad(\n\u001b[1;32m   4550\u001b[0m                 \u001b[38;5;28minput\u001b[39m, pad\n\u001b[1;32m   4551\u001b[0m             )\n\u001b[0;32m-> 4552\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Padding length should be less than or equal to two times the input dimension but got padding length 4 and input of dimension 1"
     ]
    }
   ],
   "source": [
    "a = TextData(tokens=torch.arange(4), targets=torch.arange(4), attention_mask=torch.ones((4,)))\n",
    "b = TextData(tokens=torch.arange(5), targets=torch.arange(5), attention_mask=torch.ones((5,)))\n",
    "c = a.stack(b)\n",
    "c.tokens.shape, c.targets.shape, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "id": "df0fa14b-8361-46dd-b729-15f2f04010f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQUENCE_LENGTH = 1024\n",
    "BATCH_SIZE = 4\n",
    "NUM_WORKERS = 4  # DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "id": "6e236eb6-fbdb-4009-9e6b-79ee685994c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "id": "ba709fef-7f78-496c-8574-caf639f26f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prefixing with _ to signify global.\n",
    "__text_tokenizer = GPT2Tokenizer.from_pretrained(\"openai-community/gpt2\", clean_up_tokenization_spaces=True)\n",
    "__text_tokenizer.pad_token = __text_tokenizer.eos_token\n",
    "_text_tokenizer = partial(\n",
    "    __text_tokenizer,\n",
    "    max_length=SEQUENCE_LENGTH+1,\n",
    "    truncation=True,\n",
    "    padding=\"max_length\",\n",
    "    return_tensors=\"pt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "id": "62c79503-2c19-4699-9452-f5e17a94da3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "BOS_TOK, EOS_TOK = __text_tokenizer.bos_token, __text_tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "id": "bb24dcf8-b7fe-4347-a9a5-d594c9b3e1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import re\n",
    "import tempfile\n",
    "import requests\n",
    "\n",
    "def acquire_shakespeare_dataset():\n",
    "    temp_dir = tempfile.gettempdir()\n",
    "    shakespeare_filepath = Path(temp_dir)/\"shakespeare.txt\"\n",
    "    if not os.path.exists(shakespeare_filepath):\n",
    "        data_url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
    "        with open(shakespeare_filepath, 'w', encoding='utf-8') as f:\n",
    "            f.write(requests.get(data_url).text)\n",
    "    \n",
    "    with open(shakespeare_filepath, 'r', encoding='utf-8') as f:\n",
    "        data = f.read()\n",
    "\n",
    "    # Split the dataset into each character's lines.\n",
    "    # Continue taking lines until you have at least 250 words in the sample.\n",
    "    # Add that sample to the dataset.\n",
    "    characters_lines = re.split(r\"\\n\\s*\\n\", data.strip())\n",
    "    characters_lines = [BOS_TOK + line + EOS_TOK for line in characters_lines]\n",
    "    MIN_WORDS_PER_BATCH = 250\n",
    "    sample = [characters_lines[0]]\n",
    "    num_words_in_sample = len(characters_lines[0].split())\n",
    "    text_dataset = []\n",
    "    i = 1\n",
    "    while i < len(characters_lines):\n",
    "        if num_words_in_sample > MIN_WORDS_PER_BATCH:\n",
    "            text_dataset.append(\"\".join(sample))\n",
    "            num_words_in_sample -= len(sample[0].split())\n",
    "            sample = sample[1:]\n",
    "        sample += [characters_lines[i]]\n",
    "        num_words_in_sample += len(characters_lines[i].split())\n",
    "        i += 1\n",
    "\n",
    "    return text_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "id": "d0655722-2a05-4db0-a89a-b799faf51fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = acquire_shakespeare_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "id": "f4f7155c-8c2f-486e-bb80-85357aef757b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import torch\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "id": "b21cec57-ef86-4aec-8ed0-78ac41295448",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformDataset(Dataset):\n",
    "    def __init__(self, dataset, transform):\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.transform(self.dataset[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "id": "75df2d64-76af-4400-a767-4c4dbd22ce0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from typing import Any\n",
    "\n",
    "class Sequencer(ABC):\n",
    "    @abstractmethod\n",
    "    def __call__(self, sample: Any) -> Episode:\n",
    "        \"\"\"Given a sample, tokenizes it and returns a dict of\n",
    "        tokens, targets, attention_mask, and modality.\"\"\"\n",
    "        pass\n",
    "\n",
    "class TextSequencer(Sequencer):\n",
    "    def __init__(self, tokenizer):   \n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def __call__(self, sample, **kwargs) -> Episode:\n",
    "        tokenized =  self.tokenizer(sample, **kwargs)\n",
    "        return TextData(**{\n",
    "            \"tokens\": tokenized[\"input_ids\"][:, :-1].squeeze(0),\n",
    "            \"targets\": tokenized[\"input_ids\"][:, 1:].squeeze(0),\n",
    "            \"attention_mask\": tokenized[\"attention_mask\"][:, :-1].squeeze(0),\n",
    "        })\n",
    "\n",
    "    def decode(self, *args, **kwargs):\n",
    "        return self.tokenizer.func.decode(*args, **kwargs)\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "        return self.tokenizer.func.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "id": "caec08e3-8be4-476e-b1ea-e56f4cfa9625",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_sequencer = TextSequencer(_text_tokenizer)\n",
    "text_dataset = TransformDataset(text_data, text_sequencer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "id": "b3d87a5c-2117-4ae4-9f01-5c36ce4c084e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextData(tokens=tensor([50256,  5962, 22307,  ..., 50256, 50256, 50256]), targets=tensor([ 5962, 22307,    25,  ..., 50256, 50256, 50256]), attention_mask=tensor([1, 1, 1,  ..., 0, 0, 0]))"
      ]
     },
     "execution_count": 591,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "id": "0af91594-977f-4541-8096-7263c9606a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms.v2 as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "id": "ec2c2673-59d0-47b8-8e4c-7f65331d05cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First things first, let's get the images resized, cropped, and normalized.\n",
    "image_transform = transforms.Compose([\n",
    "    # No particular reason to use `transforms.Compose` here since we're only doing one transform. But it's nice to know about.\n",
    "    transforms.RandomResizedCrop((192, 192), (0.5, 1.0)),\n",
    "    transforms.ToDtype(torch.float32, scale=True),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "id": "3f29c6c2-3e51-4301-9a1d-85af2536bde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "tta = itemgetter(\"tokens\", \"targets\", \"attention_mask\")\n",
    "\n",
    "class VQASequencer(Sequencer):\n",
    "    def __init__(self, text_tokenizer, image_tokenizer):   \n",
    "        self.text_tokenizer = text_tokenizer\n",
    "        self.image_tokenizer = image_tokenizer\n",
    "        \n",
    "    def __call__(self, sample, **kwargs) -> Episode:\n",
    "        question_tokens, question_targets, question_mask =  tta(self.text_tokenizer(sample[\"question\"]))\n",
    "        image_tokens, image_targets, image_mask =  tta(self.image_tokenizer(sample[\"image\"]))\n",
    "        answer_tokens, answer_targets, answer_mask =  tta(self.image_tokenizer(sample[\"answer\"]))\n",
    "        return {\n",
    "            \"tokens\": tokenized[\"input_ids\"][:, :-1].squeeze(0),\n",
    "            \"targets\": tokenized[\"input_ids\"][:, 1:].squeeze(0),\n",
    "            \"attention_mask\": tokenized[\"attention_mask\"][:, :-1].squeeze(0),\n",
    "        }\n",
    "\n",
    "    def decode(self, *args, **kwargs):\n",
    "        return self.tokenizer.func.decode(*args, **kwargs)\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "        return self.tokenizer.func.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "id": "562a85d0-fe84-4264-bd79-7e71c905e30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "id": "c87f489e-8891-4429-9be8-a9b8cda2d3ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.3616,  0.3014,  0.3458,  0.6947, -0.0158])"
      ]
     },
     "execution_count": 596,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "id": "eaab0d99-e459-4162-a20c-2e001f6145f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5])"
      ]
     },
     "execution_count": 597,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = FourRoomsStep(\n",
    "    mission=torch.randn(5),\n",
    "    image=torch.randn((3, 6, 6)),\n",
    "    direction=torch.randn(5),\n",
    "    separator=torch.randn(1),\n",
    "    action=torch.randn(5)\n",
    ")\n",
    "b = FourRoomsStep(\n",
    "    mission=torch.randn(5),\n",
    "    image=torch.randn((3, 6, 6)),\n",
    "    direction=torch.randn(5),\n",
    "    separator=torch.randn(1),\n",
    "    action=torch.randn(5)\n",
    ")\n",
    "a.stack(b).mission.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "id": "8f7f9b3a-239a-456b-895c-d506cfec3624",
   "metadata": {},
   "outputs": [],
   "source": [
    "__text_tokenizer = GPT2Tokenizer.from_pretrained(\"openai-community/gpt2\", clean_up_tokenization_spaces=True)\n",
    "__text_tokenizer.pad_token = __text_tokenizer.eos_token\n",
    "_text_tokenizer = partial(\n",
    "    __text_tokenizer,\n",
    "    max_length=SEQUENCE_LENGTH+1,\n",
    "    truncation=True,\n",
    "    padding=\"max_length\",\n",
    "    return_tensors=\"pt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "id": "3b7fadb9-4707-412b-8415-d5d893c75353",
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange\n",
    "def images_to_patches(images, patch_size=16):\n",
    "    return rearrange(images, 'b c (h s1) (w s2) -> b (h w) (c s1 s2)', s1=patch_size, s2=patch_size)\n",
    "def normalize_to_between_minus_one_plus_one(t: torch.Tensor):\n",
    "    min_val, max_val = t.min(), t.max()\n",
    "    if min_val == max_val:\n",
    "        return torch.zeros_like(t)\n",
    "    normalized = 2 * (t - min_val) / (max_val - min_val) - 1\n",
    "    return normalized\n",
    "# There's a small deviation in the NEKO codebase from the paper.\n",
    "# The paper normalizes _per patch_. The NEKO codebase currently normalizes _per image_.\n",
    "# https://github.com/eihli/NEKO/blob/master/gato/policy/embeddings.py#L38\n",
    "# This notebook normalizeds per patch. That's what this utility helps.\n",
    "def apply_along_dimension(func, dim, tensor):\n",
    "    tensor = tensor.transpose(0, dim)\n",
    "    shape = tensor.shape\n",
    "    tensor = tensor.reshape(shape[0], -1)\n",
    "    result = torch.stack([func(tensor[:, i]) for i in range(tensor.size(1))], dim=1)\n",
    "    result = result.reshape(shape).transpose(0, dim)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "id": "4b03033b-4ebf-4336-a947-be8c7919e466",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self, text_tokenizer):\n",
    "        self.text_tokenizer = text_tokenizer\n",
    "\n",
    "    def text(self, data, **kwargs):\n",
    "        tokenized =  self.text_tokenizer(data, **kwargs)\n",
    "        return TextData(**{\n",
    "            \"tokens\": tokenized[\"input_ids\"][:, :-1].squeeze(0),\n",
    "            \"targets\": tokenized[\"input_ids\"][:, 1:].squeeze(0),\n",
    "            \"attention_mask\": tokenized[\"attention_mask\"][:, :-1].squeeze(0),\n",
    "        })\n",
    "\n",
    "    def image(self, data):\n",
    "        patches = images_to_patches(data, patch_size=16)\n",
    "        # Hardcoding as a reminder to do something smarter\n",
    "        SQUARE_ROOT_OF_PATCH_SIZE = 3.464\n",
    "        xs = (\n",
    "            apply_along_dimension(\n",
    "                normalize_to_between_minus_one_plus_one, 2, patches\n",
    "            )\n",
    "            / SQUARE_ROOT_OF_PATCH_SIZE\n",
    "        )\n",
    "        # We don't predict images, but we need ys\n",
    "        # becaues these image ys will be in our \n",
    "        # concatenated ys of text/image/action/etc...\n",
    "        ys = torch.zeros(xs.shape[:2])\n",
    "        ms = torch.zeros(xs.shape[:2])  # Same story as above.\n",
    "        return ImageData(tokens=xs, targets=ys, attention_mask=ms)\n",
    "\n",
    "    def discrete(self, data):\n",
    "        # The Gato paper talks about offsetting their discrete tokens. But it\n",
    "        # seems rather arbitrary whether you offset it and use a single Embedding\n",
    "        # table or if you don't offset it and maintain two separate Embedding\n",
    "        # tables. I'm not going to offset it.\n",
    "        xs = data.to(torch.uint16)\n",
    "        ys = torch.zeros(xs.shape[:2])\n",
    "        ms = torch.zeros(xs.shape[:2])\n",
    "        return DiscreteData(tokens=xs, targets=ys, attention_mask=ms)\n",
    "\n",
    "    def continuous(self, data):\n",
    "        raise Exception('TODO: Tokenizer.continuous')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "id": "99a57558-06b9-4505-ac16-994dfdc757a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(_text_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "id": "9fc29a76-5998-47ae-9aa7-f690ab98690c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1024])"
      ]
     },
     "execution_count": 602,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.text([\"Hi\", \"There\"]).targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "id": "9b185f05-d89d-4b9d-b7c4-a58cb6350f0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 144, 768])"
      ]
     },
     "execution_count": 603,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.image(torch.randn((2, 3, 192, 192))).tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "id": "90af8b94-b297-4545-ae9d-6d7f4c81b0d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DiscreteData(tokens=tensor([1, 0, 4, 4, 3], dtype=torch.uint16), targets=tensor([0., 0., 0., 0., 0.]), attention_mask=tensor([0., 0., 0., 0., 0.]))"
      ]
     },
     "execution_count": 604,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.discrete(torch.randint(5, (5,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "id": "fb6329b6-7d11-46c0-a9df-64e4a3f8cb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's annoying â€“ our text sequencer is very generic and can be re-used across almost any text dataset.\n",
    "# Our VQA sequencer is less generic, but it could still probably be re-used across other question/answering dataset.\n",
    "# Our agent sequencer is very specific. It seems like every agent dataset is going to have its own datastructures.\n",
    "@dataclass\n",
    "class FourRoomsSample:\n",
    "    mission: TextData\n",
    "    image: ImageData\n",
    "    direction: DiscreteData\n",
    "    separator: TextData\n",
    "    action: DiscreteData\n",
    "\n",
    "    def stack(self, other):\n",
    "        return FourRoomsSample(\n",
    "            mission=self.mission.stack(other.mission),\n",
    "            image=self.image.stack(other.image),\n",
    "            direction=self.direction.stack(other.direction),\n",
    "            separator=self.separator.stack(other.separator),\n",
    "            action=self.action.stack(other.action),\n",
    "        )\n",
    "\n",
    "class FourRoomsSequencer(Sequencer):\n",
    "    def __init__(self, text_tokenizer, image_tokenizer, discrete_observation_tokenizer, discrete_action_tokenizer, image_transform):\n",
    "        self.text_tokenizer = text_tokenizer\n",
    "        self.image_tokenizer = image_tokenizer\n",
    "        # The only difference between an observation and an action tokenizer is that the observation tokenizer returns a zero mask\n",
    "        # because the Gato paper doesn't try to predict observations.\n",
    "        self.discrete_observation_tokenizer = discrete_observation_tokenizer\n",
    "        self.discrete_action_tokenizer = discrete_action_tokenizer\n",
    "        self.image_transform = image_transform\n",
    "\n",
    "    def __call__(self, episode_data, **kwargs) -> Episode:\n",
    "        \"\"\"What might a sample look like?\n",
    "\n",
    "        https://minari.farama.org/datasets/D4RL/minigrid/fourrooms-v0/\n",
    "\n",
    "        EpisodeData(\n",
    "            observations = {\n",
    "                direction: ndarray(20,),\n",
    "                image: ndarray(20, 7, 7, 3),\n",
    "                mission: ['reach the goal', 'reach the goal', ...]\n",
    "            },\n",
    "            actions = ndarray(19,)\n",
    "        )\n",
    "        \"\"\"\n",
    "        # We probably can't fit the entire episode in our context window, so\n",
    "        # pick a random spot to start from\n",
    "        i = random.randint(len(episode_data.actions))\n",
    "\n",
    "        # Starting at that index, we'll continue adding observations to our context window until\n",
    "        # we run out of space.\n",
    "        mission = self.text_tokenizer(episode_data.observations[\"mission\"][i])\n",
    "        image = self.image_tokenizer(self.image_transform(episode_data.observations[\"image\"][i]))\n",
    "        direction = self.discrete_observation_tokenizer(episode_data.observations[\"direction\"])\n",
    "        separator = TextData(tokens=torch.tensor([91]), targets=torch.tensor([0]), mask=torch.tensor([0]))\n",
    "        action = self.discrete_action_tokenizer(episode_data.actions[i])\n",
    "        step = FourRoomsSample(mission=mission, image=image, direction=direction, separator=separator, action=action)\n",
    "        steps = [step]\n",
    "\n",
    "        observation_length = len(mission.tokens) + len(image.tokens) + len(direction.tokens) + len(separator.tokens) + len(action.tokens)\n",
    "\n",
    "        current_sequence_length = observation_length\n",
    "        while i < len(episode_data.actions) and current_sequence_length + observation_length < SEQUENCE_LENGTH:\n",
    "            i += 1\n",
    "            mission = tta(self.text_tokenizer(episode_data.observations[\"mission\"][i]))\n",
    "            image = tta(self.image_tokenizer(self.image_transform(episode_data.observations[\"image\"][i])))\n",
    "            direction = tta(self.discrete_observation_tokenizer(episode_data.observations[\"direction\"]))\n",
    "            separator = TextData(tokens=torch.tensor([91]), targets=torch.tensor([0]), mask=torch.tensor([0]))\n",
    "            action = tta(self.discrete_action_tokenizer(episode_data.actions[i]))\n",
    "            step = step.stack(FourRoomsSample(mission=mission, image=image, direction=direction, separator=separator, action=action))\n",
    "            current_sequence_length += observation_length\n",
    "\n",
    "        return sample\n",
    "\n",
    "    def decode(self, *args, **kwargs):\n",
    "        return self.tokenizer.func.decode(*args, **kwargs)\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "        return self.tokenizer.func.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6403dc6d-b555-4c57-ae3f-6da1b825ba27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "id": "573db9e8-e61e-491e-a0c5-64fe8917b439",
   "metadata": {},
   "outputs": [],
   "source": [
    "import minari\n",
    "\n",
    "minigrid_dataset = minari.load_dataset('D4RL/minigrid/fourrooms-v0', download=True)\n",
    "env  = minigrid_dataset.recover_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "id": "f271a181-d8f1-4765-bb88-3de7df61af9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space: Dict('direction': Discrete(4), 'image': Box(0, 255, (7, 7, 3), uint8), 'mission': Text(1, 14, charset=                                                              ''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''(),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabcdeeeffghijklmnnoopqrrssttuvwxyzz{}))\n",
      "Action space: Discrete(7)\n",
      "Total episodes: 590\n",
      "Total steps: 10010\n"
     ]
    }
   ],
   "source": [
    "print(\"Observation space:\", minigrid_dataset.observation_space)\n",
    "print(\"Action space:\", minigrid_dataset.action_space)\n",
    "print(\"Total episodes:\", minigrid_dataset.total_episodes)\n",
    "print(\"Total steps:\", minigrid_dataset.total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "id": "2399f936-0eb0-4802-87d3-ffba9fde4867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPISODE ID'S SAMPLE 0: [31, 348, 9, 536, 400]\n",
      "EPISODE ID'S SAMPLE 1: [103, 265, 544, 204, 477]\n",
      "EPISODE ID'S SAMPLE 2: [302, 158, 14, 505, 522]\n",
      "EPISODE ID'S SAMPLE 3: [240, 125, 371, 87, 435]\n",
      "EPISODE ID'S SAMPLE 4: [468, 125, 305, 489, 469]\n"
     ]
    }
   ],
   "source": [
    "minigrid_dataset.set_seed(seed=123)\n",
    "\n",
    "for i in range(5):\n",
    "    # sample 5 episodes from the dataset\n",
    "    episodes = minigrid_dataset.sample_episodes(n_episodes=5)\n",
    "    # get id's from the sampled episodes\n",
    "    ids = list(map(lambda ep: ep.id, episodes))\n",
    "    print(f\"EPISODE ID'S SAMPLE {i}: {ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "id": "db508e92-3fe1-4ce6-969f-891bf16e1193",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EpisodeData(id=0, total_steps=19, observations={direction: ndarray of shape (20,) and dtype int64, image: ndarray of shape (20, 7, 7, 3) and dtype uint8, mission: ['reach the goal', 'reach the goal', 'reach the goal', 'reach the goal', 'reach the goal', 'reach the goal', 'reach the goal', 'reach the goal', 'reach the goal', 'reach the goal', 'reach the goal', 'reach the goal', 'reach the goal', 'reach the goal', 'reach the goal', 'reach the goal', 'reach the goal', 'reach the goal', 'reach the goal', 'reach the goal']}, actions=ndarray of shape (19,) and dtype int64, rewards=ndarray of 19 floats, terminations=ndarray of 19 bools, truncations=ndarray of 19 bools, infos=dict with the following keys: [])"
      ]
     },
     "execution_count": 609,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minigrid_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "id": "abe3e45a-6ce5-42f2-af05-9b63ebc6884f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First things first, let's get the images resized, cropped, and normalized.\n",
    "image_transform = transforms.Compose([\n",
    "    # No particular reason to use `transforms.Compose` here since we're only doing one transform. But it's nice to know about.\n",
    "    transforms.RandomResizedCrop((192, 192), (0.5, 1.0)),\n",
    "    transforms.ToDtype(torch.float32, scale=True),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "id": "8dd4020d-7d6c-47cf-a01f-5c0e05334d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from minigrid.core import constants as mgc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "id": "0bd96def-9d58-4b07-8deb-812deddaf43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "id": "33f24fcc-ab98-4fde-a496-25b053c8a085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lookup table\n",
    "lut = np.zeros((256, 3), dtype=np.uint8)\n",
    "for idx, color_name in mgc.IDX_TO_COLOR.items():\n",
    "    lut[idx] = mgc.COLORS[color_name]\n",
    "\n",
    "def minigrid_to_rgb(episode):\n",
    "    \"\"\"Convert discrete \"image\" observations into actual images.\n",
    "    I'm expecting this will improve our image modality while not losing\n",
    "    much. The downside is we can fit less in our context window. Note:\n",
    "    We might need to overlay the color/type image (index 1) with the \n",
    "    state image (index 2), if we really don't want to lose any info.\"\"\"\n",
    "    # Apply lookup to second channel\n",
    "    image = lut[episode.observations['image'][:, :, :, 1]]\n",
    "    # Convert to PyTorch tensor and permute\n",
    "    image = torch.from_numpy(image).permute(0, 3, 1, 2)\n",
    "    return image\n",
    "\n",
    "image_transform = transforms.Compose([\n",
    "    # No particular reason to use `transforms.Compose` here since we're only doing one transform. But it's nice to know about.\n",
    "    transforms.RandomResizedCrop((192, 192), (0.5, 1.0)),\n",
    "    transforms.ToDtype(torch.float32, scale=True),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])    \n",
    "])\n",
    "\n",
    "def minigrid_tokenizer(tokenizer, episode):\n",
    "    image = image_transform(minigrid_to_rgb(episode))\n",
    "    image = tokenizer.image(image)\n",
    "    mission = tokenizer.text(episode.observations['mission'])\n",
    "    direction = tokenizer.discrete(torch.from_numpy(episode.observations['direction']))\n",
    "    separator = TextData(tokens=torch.tensor([91]), targets=torch.tensor([0]), attention_mask=torch.tensor([0]))\n",
    "    action = tokenizer.discrete(torch.from_numpy(episode.actions))\n",
    "    return FourRoomsSample(mission=mission, image=image, direction=direction, separator=separator, action=action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "id": "e4d3af24-d254-4347-beed-2d9f56b390d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EpisodeData(id=0, total_steps=19, observations={direction: ndarray of shape (20,) and dtype int64, image: ndarray of shape (20, 7, 7, 3) and dtype uint8, mission: ['reach the goal', 'reach the goal', 'reach the goal', 'reach the goal', 'reach the goal', 'reach the goal', 'reach the goal', 'reach the goal', 'reach the goal', 'reach the goal', 'reach the goal', 'reach the goal', 'reach the goal', 'reach the goal', 'reach the goal', 'reach the goal', 'reach the goal', 'reach the goal', 'reach the goal', 'reach the goal']}, actions=ndarray of shape (19,) and dtype int64, rewards=ndarray of 19 floats, terminations=ndarray of 19 bools, truncations=ndarray of 19 bools, infos=dict with the following keys: [])"
      ]
     },
     "execution_count": 614,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs = minigrid_dataset[0]\n",
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "id": "6fa1d7db-b40a-4cfe-9ed6-1605a7bc03e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "minigrid_tokenize = partial(minigrid_tokenizer, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "id": "b1013911-126e-416e-90cd-cfc42284ebe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "minigrid_dataset_xf = TransformDataset(minigrid_dataset, minigrid_tokenize)\n",
    "minigrid_dataset_xf[0];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "id": "dbdf3d52-6d88-4c0d-85b6-3d627efe35b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "id": "c85d8a23-335f-4cd2-a8c9-270175818362",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "id": "9b2f1394-809d-4851-9cde-1182a9c46d76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Field(name='tokens',type=<class 'torch.Tensor'>,default=<dataclasses._MISSING_TYPE object at 0x7e069826d3d0>,default_factory=<dataclasses._MISSING_TYPE object at 0x7e069826d3d0>,init=True,repr=True,hash=None,compare=True,metadata=mappingproxy({}),kw_only=False,_field_type=_FIELD),\n",
       " Field(name='targets',type=<class 'torch.Tensor'>,default=<dataclasses._MISSING_TYPE object at 0x7e069826d3d0>,default_factory=<dataclasses._MISSING_TYPE object at 0x7e069826d3d0>,init=True,repr=True,hash=None,compare=True,metadata=mappingproxy({}),kw_only=False,_field_type=_FIELD),\n",
       " Field(name='attention_mask',type=<class 'torch.Tensor'>,default=<dataclasses._MISSING_TYPE object at 0x7e069826d3d0>,default_factory=<dataclasses._MISSING_TYPE object at 0x7e069826d3d0>,init=True,repr=True,hash=None,compare=True,metadata=mappingproxy({}),kw_only=False,_field_type=_FIELD))"
      ]
     },
     "execution_count": 631,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fields(minigrid_dataset_xf[0].mission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "id": "43361709-a508-47bc-a47e-679931d57e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def minigrid_collate_fn(batch):\n",
    "    result = batch[0]\n",
    "    for episode in batch[1:]:\n",
    "        result = result.stack(episode)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "id": "450615aa-b966-422d-ba62-b38ec3ebbeb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "minigrid_dataloader = DataLoader(minigrid_dataset_xf, collate_fn=minigrid_collate_fn, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20bd28b-3a6e-40f3-92ca-97cfa80cb640",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "id": "93bad7ad-8291-4e46-9615-9aeb9506410b",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [2, 20, 1051] at entry 0 and [29, 1024] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[622], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m minigrid_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mminigrid_dataloader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.virtualenvs/neko/lib/python3.12/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.virtualenvs/neko/lib/python3.12/site-packages/torch/utils/data/dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.virtualenvs/neko/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[618], line 4\u001b[0m, in \u001b[0;36mminigrid_collate_fn\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m      2\u001b[0m result \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m episode \u001b[38;5;129;01min\u001b[39;00m batch[\u001b[38;5;241m1\u001b[39m:]:\n\u001b[0;32m----> 4\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepisode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "Cell \u001b[0;32mIn[605], line 14\u001b[0m, in \u001b[0;36mFourRoomsSample.stack\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstack\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m FourRoomsSample(\n\u001b[0;32m---> 14\u001b[0m         mission\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmission\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmission\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m     15\u001b[0m         image\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage\u001b[38;5;241m.\u001b[39mstack(other\u001b[38;5;241m.\u001b[39mimage),\n\u001b[1;32m     16\u001b[0m         direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdirection\u001b[38;5;241m.\u001b[39mstack(other\u001b[38;5;241m.\u001b[39mdirection),\n\u001b[1;32m     17\u001b[0m         separator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseparator\u001b[38;5;241m.\u001b[39mstack(other\u001b[38;5;241m.\u001b[39mseparator),\n\u001b[1;32m     18\u001b[0m         action\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction\u001b[38;5;241m.\u001b[39mstack(other\u001b[38;5;241m.\u001b[39maction),\n\u001b[1;32m     19\u001b[0m     )\n",
      "Cell \u001b[0;32mIn[576], line 21\u001b[0m, in \u001b[0;36mData.stack\u001b[0;34m(self, other, pad_token)\u001b[0m\n\u001b[1;32m     14\u001b[0m pad_by \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(other\u001b[38;5;241m.\u001b[39mtokens) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(padded\u001b[38;5;241m.\u001b[39mtokens)\n\u001b[1;32m     15\u001b[0m padded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(padded)(\n\u001b[1;32m     16\u001b[0m     tokens\u001b[38;5;241m=\u001b[39mF\u001b[38;5;241m.\u001b[39mpad(padded\u001b[38;5;241m.\u001b[39mtokens, (\u001b[38;5;241m0\u001b[39m, pad_by), value\u001b[38;5;241m=\u001b[39mpad_token),\n\u001b[1;32m     17\u001b[0m     targets\u001b[38;5;241m=\u001b[39mF\u001b[38;5;241m.\u001b[39mpad(padded\u001b[38;5;241m.\u001b[39mtargets, (\u001b[38;5;241m0\u001b[39m, pad_by), value\u001b[38;5;241m=\u001b[39mpad_token),\n\u001b[1;32m     18\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mF\u001b[38;5;241m.\u001b[39mpad(padded\u001b[38;5;241m.\u001b[39mattention_mask, (\u001b[38;5;241m0\u001b[39m, pad_by), value\u001b[38;5;241m=\u001b[39mpad_token),\n\u001b[1;32m     19\u001b[0m )\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)(\n\u001b[0;32m---> 21\u001b[0m     tokens\u001b[38;5;241m=\u001b[39m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpadded\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokens\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m     22\u001b[0m     targets\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mstack([padded\u001b[38;5;241m.\u001b[39mtargets, other\u001b[38;5;241m.\u001b[39mtargets]),\n\u001b[1;32m     23\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mstack([padded\u001b[38;5;241m.\u001b[39mattention_mask, other\u001b[38;5;241m.\u001b[39mattention_mask])\n\u001b[1;32m     24\u001b[0m )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [2, 20, 1051] at entry 0 and [29, 1024] at entry 1"
     ]
    }
   ],
   "source": [
    "minigrid_batch = next(iter(minigrid_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4edbbb3-b64d-4ba3-a683-1f41d53fcb89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c8c33e-3e75-467d-bbe5-685670c81c44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c107d4-47ed-43b0-9e25-022609e8e1ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e258fcb-ae2c-4341-97df-fe66b46392cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c778682-11f6-408d-bf4e-7a95151c3fe5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3561ab5-5e37-4a12-8aca-411fcb2eb7b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313b3480-5db7-4374-8daf-0052de20e85f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
