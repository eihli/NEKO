{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c497ef5-9126-48b6-b285-17df0293ed03",
   "metadata": {},
   "source": [
    "# Control\n",
    "\n",
    "I'm using this notebook to:\n",
    "\n",
    "1. Explore what it takes to add a new control dataset.\n",
    "2. Make notes that we can use for discussion and knowledge-share.\n",
    "\n",
    "My plan, for now, is to copy into this notebook the bare minimum lines needed to run a training/evaluation step using a ControlTask.\n",
    "\n",
    "Every time I hit a road block, I'll make note of the issue, try my best to get around it (not necessarily in the correct way), and rinse/repeat.\n",
    "\n",
    "If you're scanning for places where I have questions, look for formatting ***? like this ?***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8db616d4-a211-4ba0-b506-eabb353815fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you edit code while this notebook is running,\n",
    "# you might need to reload the module for the changes\n",
    "# to take effect. You could also resolve the issue\n",
    "# by restarting the kernel. But reloading the module\n",
    "# is quicker and less destructive. I'm adding the \n",
    "# reload at the top so you can just run this cell\n",
    "# after making any breaking edits to the code.\n",
    "import importlib\n",
    "import gato.tasks.control_task\n",
    "importlib.reload(gato.tasks.control_task)\n",
    "\n",
    "import minari\n",
    "import gymnasium as gym\n",
    "from gato.tasks.control_task import ControlTask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818a1f01-4a17-4b4d-8dae-37895acd0f18",
   "metadata": {},
   "source": [
    "## BabyAI Minari dataset\n",
    "\n",
    "I grabbed a random dataset from the ones created with [the bot.py script in Santiago's baby-ai-dataset repo](https://github.com/snat-s/baby-ai-dataset/blob/master/scripts/bot.py#L64)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9eb40a3-7ba7-45fc-901c-5c3650467ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'BabyAI-GoToOpen-v0'\n",
    "dataset = minari.load_dataset(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "48e2d91e-6129-4b31-9948-c23816ecc559",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<minari.dataset.minari_dataset.MinariDataset at 0x7fdb736327a0>,\n",
       " 'BabyAI-GoToOpen-v0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset, dataset.spec.env_spec.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8bb39ed5-1830-43b9-bf39-76b3930c072a",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(dataset.spec.env_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1786d1ed-44f4-412c-83fc-3535c2455ef7",
   "metadata": {},
   "source": [
    "# train.py\n",
    "\n",
    "## Training arguments\n",
    "\n",
    "I'm using [Namespace](https://docs.python.org/3/library/argparse.html#argparse.Namespace) to hack up an `args` object so that I don't have to go through the hassle of `parser = ArgumentParser; parser.add_argument(...); args = parser.parse(['foo', 'bar', 'baz', ...])`.\n",
    "\n",
    "Browsing the code, the only `args` attribute that I see `ControlTask` access is `args.patch_size`, so that's the only one I'm bothering to add."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c6510562-6b7e-4598-b5fa-464d18b6b975",
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6a94b4b9-fc85-40bc-ba7e-bdd0690a5af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(patch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "dcdf66fd-3f1a-440a-893a-60492878b545",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_len = 512\n",
    "training_prompg_len_proportion=0.5\n",
    "share_prompt_episodes=True\n",
    "top_5_prompting=None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06ba044-04d3-46c6-8714-48ec277b4c67",
   "metadata": {},
   "source": [
    "# ControlTask\n",
    "\n",
    "`train.py` creates a `ControlTask`.\n",
    "\n",
    "## Supported observation spaces\n",
    "\n",
    "The first error I encounter is that the BabyAI environment has a Dict observation space.\n",
    "\n",
    "***? What would it take to support a Dict (or any other) observation space ?***\n",
    "\n",
    "Looking at how this is used, I see it conditionally:\n",
    "\n",
    "- If it's a Box that has a shape of length 2 or 3\n",
    "    - Adds image transforms\n",
    "- Otherwise\n",
    "    - Sets `obs_str` to `'continuous_obs'`, which eventually makes its way into `input_dict`, which eventually makes its way to `predict_control`, which doesn't get checked in `predict_control` but eventually makes its way to `tokenize_input_dicts`, and _that's_ where it gets checked.\n",
    "        - It tokenizes the batch with the `continuous_obs_tokenizer`. \n",
    "\n",
    "(Is this a safe condition? Might there be non-image con)\n",
    "\n",
    "### tokens_per_space\n",
    "\n",
    "This is another place where the type of the observation space is checked.\n",
    "\n",
    "Box: `space.shape[0]`\n",
    "Discrete: `1`\n",
    "Dict: ?\n",
    "\n",
    "***? What should this? How is it used? ?***\n",
    "\n",
    "`tokens_per_space` gets assigned to `action_tokens` [here](https://github.com/eihli/NEKO/blob/b66b48b88117307a442c43a7f4d8701706670144/gato/tasks/control_task.py#L74) and then used to calculate `tokens_per_timestamp` and is eventually used to create/manipulate the shape of the `input_dict` in [ControlTask.evaluate](https://github.com/eihli/NEKO/blob/b66b48b88117307a442c43a7f4d8701706670144/gato/tasks/control_task.py#L138).\n",
    "\n",
    "### Hacking my way past errors.\n",
    "\n",
    "I hard coded some arbitrary values to get past the assertions.\n",
    "\n",
    "For example, I added an `isinstance(space, Dict): return 1` to `tokens_per_space` [here](https://github.com/eihli/NEKO/blob/b66b48b88117307a442c43a7f4d8701706670144/gato/tasks/control_task.py#L23).\n",
    "\n",
    "I'm sure it's going to blow up due to a dict size mismatch or something. I just want to get to that point so I can see that error and maybe understand what the value _should_ be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f7f4423d-1c28-4f32-9192-21218f90cb15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict('direction': Discrete(4), 'image': Box(0, 255, (7, 7, 3), uint8), 'mission': MissionSpace(<function BabyAIMissionSpace._gen_mission at 0x7fdb727b09d0>, None))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9d28676e-6225-44f0-ae05-ef16596903b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "control = ControlTask('playground_env_name', env, dataset, context_len, args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707ce9cf-c93d-4a7b-879b-4ba8e713c3a9",
   "metadata": {},
   "source": [
    "# Back to train.py now that we have a ControlTask\n",
    "\n",
    "Now we're going to need a lot more args."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4f37f081-43eb-4070-90b6-7417b44a2145",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gato.policy.gato_policy import GatoPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d99217f7-1f7d-43f9-b6c0-feda8569e7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.device = 'cpu'\n",
    "args.embed_dim = 768\n",
    "args.layers = 8\n",
    "args.heads = 24\n",
    "args.dropout = 0.1\n",
    "args.mu = 100\n",
    "args.M = 256\n",
    "args.resid_mid_channels = 128\n",
    "args.continuous_tokens = 1024\n",
    "args.discrete_tokens = 1024\n",
    "args.sequence_length = 1024\n",
    "args.disable_patch_pos_encoding = False\n",
    "args.disable_inner_pos_encoding = False\n",
    "args.activation_fn = 'gelu'\n",
    "args.pretrained_lm = None\n",
    "args.flash = False\n",
    "args.tokenizer_model_name = 'gpt2'\n",
    "args.pad_seq = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "2b4e33b6-020e-47ac-be9b-f80b837b7dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GatoPolicy(\n",
    "    device=args.device,\n",
    "    embed_dim=args.embed_dim,\n",
    "    layers=args.layers,\n",
    "    heads=args.heads,\n",
    "    dropout=args.dropout,\n",
    "    mu=args.mu,\n",
    "    M=args.M,\n",
    "    patch_size=args.patch_size,\n",
    "    resid_mid_channels=args.resid_mid_channels,\n",
    "    continuous_tokens=args.continuous_tokens,\n",
    "    discrete_tokens=args.discrete_tokens,\n",
    "    context_len=args.sequence_length,\n",
    "    use_patch_pos_encoding=not args.disable_patch_pos_encoding,\n",
    "    use_pos_encoding=not args.disable_inner_pos_encoding,\n",
    "    activation_fn=args.activation_fn,\n",
    "    pretrained_lm=args.pretrained_lm,\n",
    "    flash=args.flash,\n",
    "    tokenizer_model_name=args.tokenizer_model_name,\n",
    "    pad_seq=args.pad_seq,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "9c8c4bce-b9d1-4f74-9ed6-da0b8332670f",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.embed_dim = model.embed_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2346e1de-55fe-4d1d-b37b-f64ded900e8b",
   "metadata": {},
   "source": [
    "# Trainer\n",
    "\n",
    "***? What if we wanted to skip the trainer? Could I just run `evaluate` on the ControlTask myself ?***\n",
    "\n",
    "`trainer.train` runs `train_iteration`.\n",
    "\n",
    "`train_iteration` runs `model.train()` (where `model` is an `nn.Module` ([docs](https://pytorch.org/docs/stable/generated/torch.nn.Module.html)) and we don't extend its `train` method).\n",
    "\n",
    "`GatoPolicy.forward` gets something called a \"final_representation\", which is the result of passing some token embeddings and a mask to the GPT2Model.\n",
    "\n",
    "I'm getting a bit lost at this point. I'd love to have someone explain what's going on around this part of the code.\n",
    "\n",
    "`forward` calls `tokenize_input_dicts(inputs)` [here](https://github.com/eihli/NEKO/blob/b66b48b88117307a442c43a7f4d8701706670144/gato/policy/gato_policy.py#L156). If your tracking `inputs`, which is probably an important variable to track`, then this line is probably important.\n",
    "\n",
    "Continuing anyways...\n",
    "\n",
    "We eventually call `predict_tokens` and return the `logits` (and conditionally the `loss`). [predict_token](https://github.com/eihli/NEKO/blob/b9facb61e7d48bf5f9fef9f4ec73b85b531e4aaf/gato/policy/gato_policy.py#L123) is an `nn.Linear`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "1990fd98-76a5-4641-a1c4-1b85e965e98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gato.training.trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74339dc-85f7-4a9e-85f1-3e016f010643",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    accelerator,\n",
    "    tasks,\n",
    "    exp_name,\n",
    "    args\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4820ee44-49d7-4911-bee5-d6b5aaf3f8b7",
   "metadata": {},
   "source": [
    "# Pause...\n",
    "\n",
    "I just realized it would probably benefit me to explore what an existing _working_ dataset looks like. I'm going to take a break from this notebook to go do that. Maybe I'll do it below. Maybe I'll do it in a new notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d28794-9429-4926-9cf1-604be5f33bd0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
